meta:
  name: Persistent Logging System with Per-Item Detail Files and Summary Progress
    Log
  description: 'Implement a two-tier logging system in auto-pipeline.py: a logs/ directory
    containing one detailed log file per backlog item (named to match the item slug)
    capturing all console output for that item''s full lifecycle, plus a top-level
    logs/pipeline.log capturing summary events only (item started, completed, warnings,
    errors). All existing log() and verbose_log() calls route through an extended
    facade so no call-site changes are needed. Log files persist across restarts,
    appending new runs with timestamped session headers.'
  plan_doc: docs/plans/2026-02-18-1-persistent-logging-system-with-per-item-detail-files-and-summary-progress-log-design.md
  created: '2026-02-18'
  max_attempts_default: 3
  validation:
    enabled: true
    run_after:
    - coder
    validators:
    - validator
    max_validation_attempts: 1
sections:
- id: phase-1
  name: Phase 1 - Core Logging Infrastructure
  status: in_progress
  tasks:
  - id: '1.1'
    name: Add logging constants and module-level state to auto-pipeline.py
    agent: coder
    status: completed
    description: "Add the constants and module-level state needed for the two-tier\
      \ logging system\nto scripts/auto-pipeline.py.\n\nReference: docs/plans/2026-02-18-1-persistent-logging-system-with-per-item-detail-files-and-summary-progress-log-design.md\n\
      \nSteps:\n1. Read scripts/auto-pipeline.py. Find the \"# ─── Configuration ─...\"\
      \ block\n   (around line 50). Note the existing constants like DEFECT_DIR, PLANS_DIR\
      \ etc.\n\n2. After the existing constants block (after STOP_SEMAPHORE_PATH and\
      \ before\n   HOT_RELOAD_WATCHED_FILES), add:\n     LOGS_DIR = \"logs\"\n   \
      \  SUMMARY_LOG_FILENAME = \"pipeline.log\"\n\n3. Find the \"# Global state\"\
      \ block (around line 138). After the existing\n   global state variables (_PIPELINE_PID,\
      \ VERBOSE, CLAUDE_CMD, etc.), add:\n     _item_log_file: Optional[\"TextIO\"\
      ] = None  # Detail log for current item\n\n   Note: \"TextIO\" needs to be imported.\
      \ Add to the typing import:\n     from typing import Optional, TextIO\n   (Replace\
      \ the existing \"from typing import Optional\" line.)\n\n4. Verify syntax:\n\
      \   python3 -c \"import py_compile; py_compile.compile('scripts/auto-pipeline.py',\
      \ doraise=True); print('syntax OK')\"\n\n5. Verify the constants exist:\n  \
      \ python3 -c \"\n   import importlib.util\n   spec = importlib.util.spec_from_file_location('ap',\
      \ 'scripts/auto-pipeline.py')\n   mod = importlib.util.module_from_spec(spec)\n\
      \   spec.loader.exec_module(mod)\n   assert hasattr(mod, 'LOGS_DIR'), 'Missing\
      \ LOGS_DIR'\n   assert mod.LOGS_DIR == 'logs', f'Wrong LOGS_DIR: {mod.LOGS_DIR}'\n\
      \   assert hasattr(mod, 'SUMMARY_LOG_FILENAME'), 'Missing SUMMARY_LOG_FILENAME'\n\
      \   assert mod.SUMMARY_LOG_FILENAME == 'pipeline.log'\n   print('Constants verified\
      \ OK')\n   \"\n\nFiles: scripts/auto-pipeline.py\n"
    attempts: 1
    last_attempt: '2026-02-18T08:53:14.582438'
    model_used: sonnet
    completed_at: '2026-02-18T08:54:59.522330'
    result_message: Added LOGS_DIR and SUMMARY_LOG_FILENAME constants, _item_log_file
      global state, and extended typing import with TextIO in auto-pipeline.py
  - id: '1.2'
    name: Implement _open_item_log(), _close_item_log(), and _log_summary() helpers
    agent: coder
    status: completed
    depends_on:
    - '1.1'
    description: "Add three helper functions to scripts/auto-pipeline.py for the two-tier\
      \ logging\nsystem. These functions manage the per-item detail log file and write\
      \ to the\nsummary log.\n\nReference: docs/plans/2026-02-18-1-persistent-logging-system-with-per-item-detail-files-and-summary-progress-log-design.md\n\
      \nSteps:\n1. Read scripts/auto-pipeline.py. Find the \"# ─── Logging ─...\"\
      \ section\n   (around line 174) where log() and verbose_log() are defined.\n\
      \n2. Immediately BEFORE the log() function definition, add the three new helpers:\n\
      \n   def _open_item_log(slug: str, item_name: str, item_type: str) -> None:\n\
      \       \"\"\"Open a per-item detail log file in logs/<slug>.log (append mode).\n\
      \n       Creates the logs/ directory if it does not exist. Writes a session-start\n\
      \       header so multiple pipeline runs are clearly separated in the log file.\n\
      \n       Args:\n           slug: Backlog item slug used as the log filename\
      \ (e.g. '1-feature-slug').\n           item_name: Human-readable item name for\
      \ the session header.\n           item_type: Item type string ('defect' or 'feature').\n\
      \       \"\"\"\n       global _item_log_file\n       os.makedirs(LOGS_DIR, exist_ok=True)\n\
      \       log_path = os.path.join(LOGS_DIR, f\"{slug}.log\")\n       _item_log_file\
      \ = open(log_path, \"a\", encoding=\"utf-8\", buffering=1)\n       ts = datetime.now().strftime(\"\
      %Y-%m-%d %H:%M:%S\")\n       header = (\n           \"=\" * 80 + \"\\n\"\n \
      \          f\"SESSION START  {ts}  PID={_PIPELINE_PID}\\n\"\n           f\"\
      Item: {slug}\\n\"\n           f\"Name: {item_name}\\n\"\n           f\"Type:\
      \ {item_type}\\n\"\n           + \"=\" * 80 + \"\\n\"\n       )\n       _item_log_file.write(header)\n\
      \       _item_log_file.flush()\n\n\n   def _close_item_log(result: str) -> None:\n\
      \       \"\"\"Write a session-end footer and close the current item detail log.\n\
      \n       Safe to call even if no log is open (no-op in that case).\n\n     \
      \  Args:\n           result: Short result string written into the footer (e.g.\
      \ 'success',\n                   'failed', 'verification-exhausted').\n    \
      \   \"\"\"\n       global _item_log_file\n       if _item_log_file is None:\n\
      \           return\n       ts = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"\
      )\n       footer = (\n           \"=\" * 80 + \"\\n\"\n           f\"SESSION\
      \ END  {ts}  result={result}\\n\"\n           + \"=\" * 80 + \"\\n\"\n     \
      \  )\n       _item_log_file.write(footer)\n       _item_log_file.flush()\n \
      \      _item_log_file.close()\n       _item_log_file = None\n\n\n   def _log_summary(level:\
      \ str, event: str, slug: str, detail: str = \"\") -> None:\n       \"\"\"Append\
      \ a single structured line to logs/pipeline.log.\n\n       Creates logs/ if\
      \ it does not exist. Each line has the format:\n         YYYY-MM-DD HH:MM:SS\
      \ [LEVEL]  EVENT  slug  detail\n\n       Args:\n           level: One of 'INFO',\
      \ 'WARN', 'ERROR'.\n           event: Event keyword (e.g. 'STARTED', 'COMPLETED',\
      \ 'FAILED').\n           slug: Backlog item slug.\n           detail: Optional\
      \ extra detail appended after the slug.\n       \"\"\"\n       os.makedirs(LOGS_DIR,\
      \ exist_ok=True)\n       summary_path = os.path.join(LOGS_DIR, SUMMARY_LOG_FILENAME)\n\
      \       ts = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n       parts =\
      \ [ts, f\"[{level}]\", event, slug]\n       if detail:\n           parts.append(detail)\n\
      \       line = \"  \".join(parts) + \"\\n\"\n       with open(summary_path,\
      \ \"a\", encoding=\"utf-8\") as f:\n           f.write(line)\n\n3. Verify syntax:\n\
      \   python3 -c \"import py_compile; py_compile.compile('scripts/auto-pipeline.py',\
      \ doraise=True); print('syntax OK')\"\n\n4. Verify the functions are importable:\n\
      \   python3 -c \"\n   import importlib.util\n   spec = importlib.util.spec_from_file_location('ap',\
      \ 'scripts/auto-pipeline.py')\n   mod = importlib.util.module_from_spec(spec)\n\
      \   spec.loader.exec_module(mod)\n   assert callable(mod._open_item_log), 'Missing\
      \ _open_item_log'\n   assert callable(mod._close_item_log), 'Missing _close_item_log'\n\
      \   assert callable(mod._log_summary), 'Missing _log_summary'\n   print('Helper\
      \ functions verified OK')\n   \"\n\nFiles: scripts/auto-pipeline.py\n"
    attempts: 1
    last_attempt: '2026-02-18T08:55:02.139226'
    model_used: sonnet
    completed_at: '2026-02-18T08:57:11.211567'
    result_message: Added _open_item_log(), _close_item_log(), and _log_summary()
      helpers to scripts/auto-pipeline.py immediately before the log() function. Syntax
      and importability verified OK.
  - id: '1.3'
    name: Extend log() and verbose_log() to tee output to item detail log
    agent: coder
    status: in_progress
    depends_on:
    - '1.2'
    description: "Extend the existing log() and verbose_log() functions in scripts/auto-pipeline.py\n\
      to tee their output to the currently-open item detail log file (_item_log_file).\n\
      \nReference: docs/plans/2026-02-18-1-persistent-logging-system-with-per-item-detail-files-and-summary-progress-log-design.md\n\
      \nSteps:\n1. Read scripts/auto-pipeline.py. Find log() (around line 180) and\n\
      \   verbose_log() (around line 186). Their current implementations are:\n\n\
      \     def log(message: str) -> None:\n         \"\"\"Print a timestamped log\
      \ message with PID for process tracking.\"\"\"\n         timestamp = datetime.now().strftime(\"\
      %H:%M:%S\")\n         print(f\"[{timestamp}] [AUTO-PIPELINE:{_PIPELINE_PID}]\
      \ {message}\", flush=True)\n\n     def verbose_log(message: str) -> None:\n\
      \         \"\"\"Print a verbose log message if verbose mode is enabled.\"\"\"\
      \n         if VERBOSE:\n             timestamp = datetime.now().strftime(\"\
      %H:%M:%S.%f\")[:-3]\n             print(f\"[{timestamp}] [VERBOSE:{_PIPELINE_PID}]\
      \ {message}\", flush=True)\n\n2. Replace log() with:\n\n     def log(message:\
      \ str) -> None:\n         \"\"\"Print a timestamped log message with PID for\
      \ process tracking.\n\n         Also writes to the currently-open item detail\
      \ log file (if any).\n         \"\"\"\n         timestamp = datetime.now().strftime(\"\
      %H:%M:%S\")\n         line = f\"[{timestamp}] [AUTO-PIPELINE:{_PIPELINE_PID}]\
      \ {message}\"\n         print(line, flush=True)\n         if _item_log_file\
      \ is not None:\n             _item_log_file.write(line + \"\\n\")\n        \
      \     _item_log_file.flush()\n\n3. Replace verbose_log() with:\n\n     def verbose_log(message:\
      \ str) -> None:\n         \"\"\"Print a verbose log message if verbose mode\
      \ is enabled.\n\n         Also writes to the currently-open item detail log\
      \ file (if any).\n         \"\"\"\n         if VERBOSE:\n             timestamp\
      \ = datetime.now().strftime(\"%H:%M:%S.%f\")[:-3]\n             line = f\"[{timestamp}]\
      \ [VERBOSE:{_PIPELINE_PID}] {message}\"\n             print(line, flush=True)\n\
      \             if _item_log_file is not None:\n                 _item_log_file.write(line\
      \ + \"\\n\")\n                 _item_log_file.flush()\n\n4. Verify syntax:\n\
      \   python3 -c \"import py_compile; py_compile.compile('scripts/auto-pipeline.py',\
      \ doraise=True); print('syntax OK')\"\n\nFiles: scripts/auto-pipeline.py\n"
    attempts: 1
    last_attempt: '2026-02-18T08:57:13.645332'
- id: phase-2
  name: Phase 2 - Wire Logging into process_item()
  status: pending
  tasks:
  - id: '2.1'
    name: Open/close item log and emit summary events in process_item()
    agent: coder
    status: pending
    depends_on:
    - '1.3'
    description: "Wire the logging lifecycle into process_item() and the _archive_and_report()\n\
      helper in scripts/auto-pipeline.py so that:\n  - The item detail log is opened\
      \ before processing starts.\n  - Summary events are written to pipeline.log\
      \ at key lifecycle points.\n  - The item detail log is closed (with appropriate\
      \ result) when processing ends.\n\nReference: docs/plans/2026-02-18-1-persistent-logging-system-with-per-item-detail-files-and-summary-progress-log-design.md\n\
      \nSteps:\n1. Read scripts/auto-pipeline.py. Find process_item() (around line\
      \ 1680).\n   Note the structure:\n     - Creates SlackNotifier, logs header,\
      \ calls slack.send_status\n     - Fast path: last_verification_passed -> _archive_and_report\n\
      \     - Main cycle loop\n     - Various return points (True for success, False\
      \ for failure)\n\n2. At the very start of process_item(), after the SlackNotifier\
      \ is created\n   and before any log() calls, add:\n     _open_item_log(item.slug,\
      \ item.display_name, item.item_type)\n     _log_summary(\"INFO\", \"STARTED\"\
      , item.slug, f\"type={item.item_type}\")\n\n3. Find every return statement in\
      \ process_item() and in _archive_and_report().\n   Wrap each return path to\
      \ call _close_item_log() with an appropriate result\n   string before returning:\n\
      \     - Success returns: _close_item_log(\"success\")\n     - Failure returns\
      \ (plan creation failed, orchestrator failed, etc.):\n       _close_item_log(\"\
      failed\")\n     - Verification exhausted: _close_item_log(\"verification-exhausted\"\
      )\n     - Stop requested: _close_item_log(\"stopped\")\n\n   Also add _log_summary()\
      \ calls at key points:\n     - After archive completes: _log_summary(\"INFO\"\
      , \"COMPLETED\", item.slug, ...)\n     - After orchestrator failure: _log_summary(\"\
      ERROR\", \"FAILED\", item.slug, \"phase=orchestrator\")\n     - After plan creation\
      \ failure: _log_summary(\"ERROR\", \"FAILED\", item.slug, \"phase=plan-creation\"\
      )\n     - After verification exhausted: _log_summary(\"WARN\", \"VERIFICATION_EXHAUSTED\"\
      , item.slug, ...)\n\n   IMPORTANT: Use a try/finally pattern to guarantee _close_item_log()\
      \ is\n   always called even if an unexpected exception occurs:\n\n     def process_item(item,\
      \ dry_run=False, session_tracker=None) -> bool:\n         _open_item_log(item.slug,\
      \ item.display_name, item.item_type)\n         _log_summary(\"INFO\", \"STARTED\"\
      , item.slug, f\"type={item.item_type}\")\n         try:\n             return\
      \ _process_item_inner(item, dry_run, session_tracker)\n         except Exception\
      \ as e:\n             log(f\"UNEXPECTED ERROR in process_item: {e}\")\n    \
      \         _log_summary(\"ERROR\", \"CRASHED\", item.slug, str(e))\n        \
      \     return False\n         finally:\n             _close_item_log(\"done\"\
      )\n\n   Alternatively, if the refactoring to _process_item_inner is too invasive,\n\
      \   wrap each return individually with _close_item_log() and _log_summary().\n\
      \   Use whichever approach causes fewer changes to the existing code structure.\n\
      \n4. Verify syntax:\n   python3 -c \"import py_compile; py_compile.compile('scripts/auto-pipeline.py',\
      \ doraise=True); print('syntax OK')\"\n\n5. Verify process_item can still be\
      \ imported:\n   python3 -c \"\n   import importlib.util\n   spec = importlib.util.spec_from_file_location('ap',\
      \ 'scripts/auto-pipeline.py')\n   mod = importlib.util.module_from_spec(spec)\n\
      \   spec.loader.exec_module(mod)\n   assert callable(mod.process_item), 'process_item\
      \ not found'\n   print('process_item verified OK')\n   \"\n\nFiles: scripts/auto-pipeline.py\n"
- id: phase-3
  name: Phase 3 - Gitignore and Cleanup
  status: pending
  tasks:
  - id: '3.1'
    name: Add logs/ to .gitignore
    agent: coder
    status: pending
    depends_on:
    - '2.1'
    description: "Add a logs/ entry to .gitignore so runtime log files are not committed\
      \ to git.\n\nSteps:\n1. Read .gitignore (if it exists). If it does not exist,\
      \ create it.\n\n2. Check if \"logs/\" is already present. If not, append:\n\
      \     # Runtime pipeline logs (generated at runtime, not committed)\n     logs/\n\
      \n3. Verify .gitignore contains the entry:\n   python3 -c \"\n   content = open('.gitignore').read()\n\
      \   assert 'logs/' in content, 'logs/ not in .gitignore'\n   print('.gitignore\
      \ verified OK')\n   \"\n\nFiles: .gitignore\n"
- id: phase-4
  name: Phase 4 - Unit Tests
  status: pending
  tasks:
  - id: '4.1'
    name: Add unit tests for logging helpers and tee behavior
    agent: coder
    status: pending
    depends_on:
    - '3.1'
    description: "Add unit tests for the new logging infrastructure to tests/test_auto_pipeline.py.\n\
      \nReference: docs/plans/2026-02-18-1-persistent-logging-system-with-per-item-detail-files-and-summary-progress-log-design.md\n\
      \nSteps:\n1. Read tests/test_auto_pipeline.py to understand the test structure,\
      \ imports,\n   and the importlib pattern used to load auto-pipeline.py.\n\n\
      2. At the top of the file, add these new names to the imports from mod:\n  \
      \   _open_item_log = mod._open_item_log\n     _close_item_log = mod._close_item_log\n\
      \     _log_summary = mod._log_summary\n     LOGS_DIR = mod.LOGS_DIR\n     SUMMARY_LOG_FILENAME\
      \ = mod.SUMMARY_LOG_FILENAME\n\n3. Add the following test cases after the existing\
      \ tests:\n\n   a. test_logs_dir_constant():\n      Import the module. Assert\
      \ mod.LOGS_DIR == 'logs'.\n      Assert mod.SUMMARY_LOG_FILENAME == 'pipeline.log'.\n\
      \n   b. test_open_item_log_creates_file(tmp_path, monkeypatch):\n      Monkeypatch\
      \ mod.LOGS_DIR to str(tmp_path / 'logs').\n      Monkeypatch mod._PIPELINE_PID\
      \ to 99999.\n      Call mod._open_item_log('test-slug', 'Test Feature', 'feature').\n\
      \      Assert (tmp_path / 'logs' / 'test-slug.log').exists().\n      content\
      \ = (tmp_path / 'logs' / 'test-slug.log').read_text()\n      Assert 'SESSION\
      \ START' in content.\n      Assert 'test-slug' in content.\n      Assert 'feature'\
      \ in content.\n      Call mod._close_item_log('success').\n      Assert mod._item_log_file\
      \ is None.\n\n   c. test_close_item_log_writes_footer(tmp_path, monkeypatch):\n\
      \      Monkeypatch mod.LOGS_DIR to str(tmp_path / 'logs').\n      Call mod._open_item_log('slug2',\
      \ 'Item', 'defect').\n      Call mod._close_item_log('failed').\n      content\
      \ = (tmp_path / 'logs' / 'slug2.log').read_text()\n      Assert 'SESSION END'\
      \ in content.\n      Assert 'failed' in content.\n      Assert mod._item_log_file\
      \ is None.\n\n   d. test_close_item_log_noop_when_not_open():\n      Ensure\
      \ mod._item_log_file is None (it should be after other tests).\n      Call mod._close_item_log('result')\
      \ — should not raise.\n\n   e. test_log_tees_to_item_log_file(tmp_path, monkeypatch):\n\
      \      Monkeypatch mod.LOGS_DIR to str(tmp_path / 'logs').\n      Call mod._open_item_log('tee-slug',\
      \ 'Tee Test', 'feature').\n      Call mod.log('hello from log').\n      content\
      \ = (tmp_path / 'logs' / 'tee-slug.log').read_text()\n      Assert 'hello from\
      \ log' in content.\n      Call mod._close_item_log('success').\n\n   f. test_log_summary_creates_pipeline_log(tmp_path,\
      \ monkeypatch):\n      Monkeypatch mod.LOGS_DIR to str(tmp_path / 'logs').\n\
      \      Call mod._log_summary('INFO', 'STARTED', 'my-slug', 'type=feature').\n\
      \      summary_path = tmp_path / 'logs' / 'pipeline.log'\n      Assert summary_path.exists().\n\
      \      content = summary_path.read_text()\n      Assert '[INFO]' in content.\n\
      \      Assert 'STARTED' in content.\n      Assert 'my-slug' in content.\n  \
      \    Assert 'type=feature' in content.\n\n   g. test_log_summary_appends(tmp_path,\
      \ monkeypatch):\n      Monkeypatch mod.LOGS_DIR to str(tmp_path / 'logs').\n\
      \      Call mod._log_summary('INFO', 'STARTED', 'slug-a').\n      Call mod._log_summary('INFO',\
      \ 'COMPLETED', 'slug-a').\n      content = (tmp_path / 'logs' / 'pipeline.log').read_text()\n\
      \      lines = [l for l in content.splitlines() if l.strip()]\n      Assert\
      \ len(lines) >= 2.\n      Assert any('STARTED' in l for l in lines).\n     \
      \ Assert any('COMPLETED' in l for l in lines).\n\n   h. test_open_item_log_appends_on_second_run(tmp_path,\
      \ monkeypatch):\n      Monkeypatch mod.LOGS_DIR to str(tmp_path / 'logs').\n\
      \      # First run\n      mod._open_item_log('append-slug', 'Item', 'feature')\n\
      \      mod.log('first run message')\n      mod._close_item_log('success')\n\
      \      # Second run\n      mod._open_item_log('append-slug', 'Item', 'feature')\n\
      \      mod.log('second run message')\n      mod._close_item_log('success')\n\
      \      content = (tmp_path / 'logs' / 'append-slug.log').read_text()\n     \
      \ Assert 'first run message' in content.\n      Assert 'second run message'\
      \ in content.\n      # Two session headers\n      Assert content.count('SESSION\
      \ START') == 2.\n\n4. Run tests:\n   ~/.pyenv/versions/3.11.*/bin/python -m\
      \ pytest tests/test_auto_pipeline.py -v\n   Fix any failures before marking\
      \ this task complete.\n\nFiles: tests/test_auto_pipeline.py\n"
- id: phase-5
  name: Phase 5 - Verification
  status: pending
  tasks:
  - id: '5.1'
    name: Final verification - syntax, tests, and dry-run
    agent: code-reviewer
    status: pending
    depends_on:
    - '4.1'
    description: "Run all verification checks to confirm the persistent logging feature\
      \ is\ncorrect and all tests pass.\n\nSteps:\n1. Check Python syntax for both\
      \ scripts:\n   python3 -c \"import py_compile; py_compile.compile('scripts/auto-pipeline.py',\
      \ doraise=True); py_compile.compile('scripts/plan-orchestrator.py', doraise=True)\"\
      \n\n2. Run the full test suite:\n   ~/.pyenv/versions/3.11.*/bin/python -m pytest\
      \ tests/ 2>/dev/null || echo 'No test suite configured'\n\n3. Verify the new\
      \ constants exist:\n   python3 -c \"\n   import importlib.util\n   spec = importlib.util.spec_from_file_location('ap',\
      \ 'scripts/auto-pipeline.py')\n   mod = importlib.util.module_from_spec(spec)\n\
      \   spec.loader.exec_module(mod)\n   assert hasattr(mod, 'LOGS_DIR'), 'Missing\
      \ LOGS_DIR'\n   assert mod.LOGS_DIR == 'logs'\n   assert hasattr(mod, 'SUMMARY_LOG_FILENAME'),\
      \ 'Missing SUMMARY_LOG_FILENAME'\n   assert callable(mod._open_item_log), 'Missing\
      \ _open_item_log'\n   assert callable(mod._close_item_log), 'Missing _close_item_log'\n\
      \   assert callable(mod._log_summary), 'Missing _log_summary'\n   print('All\
      \ logging constants and helpers verified OK')\n   \"\n\n4. Verify log() tees\
      \ to file (functional smoke test):\n   python3 -c \"\n   import importlib.util,\
      \ tempfile, os\n   spec = importlib.util.spec_from_file_location('ap', 'scripts/auto-pipeline.py')\n\
      \   mod = importlib.util.module_from_spec(spec)\n   spec.loader.exec_module(mod)\n\
      \   with tempfile.TemporaryDirectory() as td:\n       mod.LOGS_DIR = td\n  \
      \     mod._open_item_log('smoke-test', 'Smoke Test Item', 'feature')\n     \
      \  mod.log('smoke test line')\n       mod._close_item_log('success')\n     \
      \  log_path = os.path.join(td, 'smoke-test.log')\n       assert os.path.exists(log_path),\
      \ 'Log file not created'\n       content = open(log_path).read()\n       assert\
      \ 'smoke test line' in content, 'Log line not in file'\n       assert 'SESSION\
      \ START' in content, 'Missing session header'\n       assert 'SESSION END' in\
      \ content, 'Missing session footer'\n       print('Smoke test passed OK')\n\
      \   \"\n\n5. Verify summary log smoke test:\n   python3 -c \"\n   import importlib.util,\
      \ tempfile, os\n   spec = importlib.util.spec_from_file_location('ap', 'scripts/auto-pipeline.py')\n\
      \   mod = importlib.util.module_from_spec(spec)\n   spec.loader.exec_module(mod)\n\
      \   with tempfile.TemporaryDirectory() as td:\n       mod.LOGS_DIR = td\n  \
      \     mod._log_summary('INFO', 'STARTED', 'test-item', 'type=feature')\n   \
      \    mod._log_summary('INFO', 'COMPLETED', 'test-item', 'duration=10s')\n  \
      \     summary = os.path.join(td, 'pipeline.log')\n       assert os.path.exists(summary),\
      \ 'Summary log not created'\n       content = open(summary).read()\n       assert\
      \ 'STARTED' in content\n       assert 'COMPLETED' in content\n       assert\
      \ 'test-item' in content\n       print('Summary log smoke test passed OK')\n\
      \   \"\n\n6. Verify .gitignore has logs/ entry:\n   python3 -c \"\n   content\
      \ = open('.gitignore').read()\n   assert 'logs/' in content, 'logs/ missing\
      \ from .gitignore'\n   print('.gitignore verified OK')\n   \"\n\n7. Run orchestrator\
      \ dry-run to confirm no startup errors:\n   python3 scripts/plan-orchestrator.py\
      \ --plan .claude/plans/sample-plan.yaml --dry-run\n\nIf any check fails, report\
      \ the specific failure with details.\n\nFiles: scripts/auto-pipeline.py, scripts/plan-orchestrator.py,\
      \ .gitignore, tests/test_auto_pipeline.py\n"
