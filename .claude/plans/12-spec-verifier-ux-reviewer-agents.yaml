meta:
  name: Spec Verifier and UX Reviewer Agents
  description: 'Create two missing validator agents: spec-verifier (checks code against
    functional specifications) and ux-reviewer (evaluates UI quality and usability).
    Both are read-only validators producing PASS/WARN/FAIL verdicts. Add SPEC_VERIFIER_KEYWORDS
    and UX_REVIEWER_KEYWORDS to plan-orchestrator.py for automatic agent inference.
    Write integration tests. Both agents integrate with the existing validation pipeline
    as additional validators.

    '
  plan_doc: docs/plans/2026-02-16-12-spec-verifier-ux-reviewer-agents-design.md
  created: '2026-02-16'
  max_attempts_default: 3
  validation:
    enabled: true
    run_after:
    - coder
    validators:
    - validator
    max_validation_attempts: 1
sections:
- id: phase-1
  name: Phase 1 - Agent Definitions
  status: completed
  tasks:
  - id: '1.1'
    name: Create spec-verifier agent definition
    agent: coder
    status: completed
    description: "Create the spec-verifier agent definition file.\nReference: docs/plans/2026-02-16-12-spec-verifier-ux-reviewer-agents-design.md\n\
      Steps:\n1. Create .claude/agents/spec-verifier.md with YAML frontmatter and\
      \ agent body.\nThe frontmatter must be:\n  ---\n  name: spec-verifier\n  description:\
      \ >\n    Functional specification verifier. Use after UI changes to validate\
      \ that\n    component placement, data display, and user workflow match the spec.\n\
      \    Read-only: does not modify code.\n  tools:\n    - Read\n    - Grep\n  \
      \  - Glob\n  model: sonnet\n  ---\n\nThe body must describe:\na. Role: You are\
      \ a functional specification verifier. Your job is to\n   independently verify\
      \ that code changes match the project functional\n   specifications. You do\
      \ NOT fix issues - you only inspect, compare, and\n   report findings.\n\nb.\
      \ Before Verifying section: Read the functional spec for every page\n   affected\
      \ by the changes. Read the relevant domain checklists from\n   .claude/checklists/\
      \ (especially crud-operations.md for CRUD features).\n   Read the implemented\
      \ source files.\n\nc. Verification Checklist section with these checks:\n  \
      \ - For each UI component added or moved:\n     - Does it appear on the correct\
      \ page per the spec?\n     - Is it in the correct section/position within the\
      \ page?\n     - Does it match the workflow described in the spec?\n   - For\
      \ each data display:\n     - Is real data shown when available?\n     - When\
      \ data is unavailable, does it show an appropriate empty state?\n     - Is there\
      \ any fallback to data from a different source? (FAIL if yes)\n   - For CRUD\
      \ operations (reference .claude/checklists/crud-operations.md):\n     - Do create\
      \ and edit use the same form/modal?\n     - Is data properly saved and reloaded?\n\
      \     - Can the user cancel without saving?\n     - Are mandatory field validations\
      \ present?\n   - For removed components: was removal explicitly requested?\n\
      \nd. Output Format section: Must produce standard verdict format:\n   **Verdict:\
      \ PASS** or **Verdict: WARN** or **Verdict: FAIL**\n   Followed by **Findings:**\
      \ with - [PASS|WARN|FAIL] items and file:line refs.\n   Followed by **Evidence:**\
      \ with spec references and code references.\n\ne. Verdict Rules:\n   PASS: All\
      \ spec requirements are implemented correctly.\n   WARN: Minor deviations found\
      \ (positioning, styling) but core functionality matches.\n   FAIL: Components\
      \ on wrong page, data from wrong source, missing required\n   functionality,\
      \ or removed components without explicit request.\n\nf. Constraints: Read-only\
      \ agent. Only use Read, Grep, and Glob to inspect\n   the codebase. Do not modify\
      \ source files.\n\ng. Output Protocol: Write .claude/plans/task-status.json\
      \ when done\n   (same format as other validators).\n\n2. Verify the file loads\
      \ correctly:\n   python3 -c \"\n   import importlib.util\n   spec = importlib.util.spec_from_file_location('po',\
      \ 'scripts/plan-orchestrator.py')\n   mod = importlib.util.module_from_spec(spec)\n\
      \   spec.loader.exec_module(mod)\n   agent = mod.load_agent_definition('spec-verifier')\n\
      \   assert agent is not None, 'spec-verifier agent not found'\n   assert agent['name']\
      \ == 'spec-verifier', f'Wrong name: {agent[\\\"name\\\"]}'\n   assert agent['model']\
      \ == 'sonnet', f'Wrong model: {agent[\\\"model\\\"]}'\n   assert 'Read' in agent['tools'],\
      \ 'Missing Read tool'\n   assert 'Grep' in agent['tools'], 'Missing Grep tool'\n\
      \   assert 'Glob' in agent['tools'], 'Missing Glob tool'\n   assert 'Bash' not\
      \ in agent['tools'], 'spec-verifier should not have Bash'\n   print(f'spec-verifier\
      \ agent loaded: {agent[\\\"name\\\"]}, model={agent[\\\"model\\\"]}')\n   \"\
      \n\nFiles: .claude/agents/spec-verifier.md\n"
    attempts: 1
    last_attempt: '2026-02-16T19:02:02.274089'
    model_used: sonnet
    completed_at: '2026-02-16T19:04:08.191634'
    result_message: Created spec-verifier agent definition with YAML frontmatter,
      verification checklist, and output protocol. Agent loads correctly and verified.
  - id: '1.2'
    name: Create ux-reviewer agent definition
    agent: coder
    parallel_group: agent-definitions
    status: completed
    description: "Create the ux-reviewer agent definition file.\nReference: docs/plans/2026-02-16-12-spec-verifier-ux-reviewer-agents-design.md\n\
      Steps:\n1. Create .claude/agents/ux-reviewer.md with YAML frontmatter and agent\
      \ body.\nThe frontmatter must be:\n  ---\n  name: ux-reviewer\n  description:\
      \ >\n    UX/UI quality reviewer. Use after UI changes to evaluate design quality,\n\
      \    accessibility, and usability. Read-only: does not modify code.\n    Distinct\
      \ from ux-designer which generates designs.\n  tools:\n    - Read\n    - Grep\n\
      \    - Glob\n  model: sonnet\n  ---\n\nThe body must describe:\na. Role: You\
      \ are a UX/UI quality reviewer. Your job is to independently\n   evaluate implemented\
      \ UI code for design quality, accessibility, and\n   usability. You do NOT fix\
      \ issues or generate designs - you only inspect\n   and report findings.\n\n\
      b. Before Reviewing section: Read the implemented source files for all\n   UI\
      \ components modified. Read existing UI components to understand the\n   project\
      \ design system and component library. Read the design document\n   referenced\
      \ in the task (if any).\n\nc. Review Checklist section with these checks:\n\
      \   - Responsive layout: components work on mobile and desktop viewports\n \
      \  - Accessibility: ARIA labels on interactive elements, keyboard\n     navigation,\
      \ sufficient color contrast, screen reader compatibility\n   - State coverage:\
      \ loading states, error states, empty states all handled\n   - Visual hierarchy:\
      \ information density is appropriate, primary actions\n     are visually prominent\n\
      \   - Interaction patterns: hover states, focus indicators, click targets\n\
      \     are large enough for touch\n   - Consistency: uses project component library,\
      \ follows existing patterns\n   - Feedback: uses centralized notification/toast\
      \ system, not inline ad-hoc divs\n\nd. Output Format section: Must produce standard\
      \ verdict format:\n   **Verdict: PASS** or **Verdict: WARN** or **Verdict: FAIL**\n\
      \   Followed by **Findings:** with - [PASS|WARN|FAIL] items and file:line refs.\n\
      \   Followed by **Evidence:** with code references.\n   Followed by **Quality\
      \ Score:** with scores for:\n   - Clarity: X/10\n   - Consistency: X/10\n  \
      \ - Accessibility: X/10\n   - Implementation Feasibility: X/10\n\ne. Verdict\
      \ Rules:\n   PASS: All usability checks pass. Quality scores average 7 or above.\n\
      \   WARN: Minor issues found (missing hover states, suboptimal layout) but\n\
      \   no accessibility violations. Quality scores average 5-7.\n   FAIL: Accessibility\
      \ violations, missing loading/error/empty states, or\n   components that do\
      \ not work on mobile. Quality scores average below 5.\n\nf. Constraints: Read-only\
      \ agent. Only use Read, Grep, and Glob to inspect\n   the codebase. Do not modify\
      \ source files. You are distinct from\n   ux-designer which generates designs\
      \ in Phase 0.\n\ng. Output Protocol: Write .claude/plans/task-status.json when\
      \ done\n   (same format as other validators).\n\n2. Verify the file loads correctly:\n\
      \   python3 -c \"\n   import importlib.util\n   spec = importlib.util.spec_from_file_location('po',\
      \ 'scripts/plan-orchestrator.py')\n   mod = importlib.util.module_from_spec(spec)\n\
      \   spec.loader.exec_module(mod)\n   agent = mod.load_agent_definition('ux-reviewer')\n\
      \   assert agent is not None, 'ux-reviewer agent not found'\n   assert agent['name']\
      \ == 'ux-reviewer', f'Wrong name: {agent[\\\"name\\\"]}'\n   assert agent['model']\
      \ == 'sonnet', f'Wrong model: {agent[\\\"model\\\"]}'\n   assert 'Read' in agent['tools'],\
      \ 'Missing Read tool'\n   assert 'Bash' not in agent['tools'], 'ux-reviewer\
      \ should not have Bash'\n   print(f'ux-reviewer agent loaded: {agent[\\\"name\\\
      \"]}, model={agent[\\\"model\\\"]}')\n   \"\n\nFiles: .claude/agents/ux-reviewer.md\n"
    attempts: 1
    last_attempt: '2026-02-16T19:04:10.271189'
    model_used: sonnet
    completed_at: '2026-02-16T19:06:16.087641'
    result_message: Created ux-reviewer agent definition with comprehensive UX/accessibility
      checklist and quality scoring system
- id: phase-2
  name: Phase 2 - Orchestrator Integration
  status: completed
  tasks:
  - id: '2.1'
    name: Add SPEC_VERIFIER_KEYWORDS and UX_REVIEWER_KEYWORDS to infer_agent_for_task
    agent: coder
    status: completed
    depends_on:
    - '1.1'
    - '1.2'
    description: "In scripts/plan-orchestrator.py, add two new keyword constants for\
      \ automatic agent inference so tasks with spec/ux keywords auto-select the correct\
      \ agent.\nReference: docs/plans/2026-02-16-12-spec-verifier-ux-reviewer-agents-design.md\n\
      Steps:\n1. Read scripts/plan-orchestrator.py and find the existing keyword\n\
      \   constants: REVIEWER_KEYWORDS, PLANNER_KEYWORDS, QA_AUDITOR_KEYWORDS,\n \
      \  DESIGNER_KEYWORDS (around lines 144-169).\n\n2. Add two new constants after\
      \ QA_AUDITOR_KEYWORDS:\n\n   # Keywords indicating a spec verification task.\n\
      \   # When infer_agent_for_task() matches any of these, it selects \"spec-verifier\"\
      .\n   SPEC_VERIFIER_KEYWORDS = [\n       \"spec verifier\", \"spec verification\"\
      , \"functional spec\",\n       \"spec-verifier\", \"spec compliance\"\n   ]\n\
      \n   # Keywords indicating a UX review task.\n   # When infer_agent_for_task()\
      \ matches any of these, it selects \"ux-reviewer\".\n   UX_REVIEWER_KEYWORDS\
      \ = [\n       \"ux review\", \"ux-reviewer\", \"usability review\",\n      \
      \ \"accessibility review\", \"ui quality\"\n   ]\n\n3. In the infer_agent_for_task()\
      \ function, add two new checks.\n   These must go AFTER the QA_AUDITOR_KEYWORDS\
      \ check and BEFORE the\n   DESIGNER_KEYWORDS check. The spec-verifier and ux-reviewer\
      \ keywords\n   are multi-word phrases so they should be checked before single-word\n\
      \   designer keywords to avoid false positives:\n\n   for keyword in SPEC_VERIFIER_KEYWORDS:\n\
      \       if keyword in text:\n           return \"spec-verifier\"\n\n   for keyword\
      \ in UX_REVIEWER_KEYWORDS:\n       if keyword in text:\n           return \"\
      ux-reviewer\"\n\n4. Update the docstring of infer_agent_for_task() to include\
      \ the new\n   steps in the priority order list:\n   1. REVIEWER_KEYWORDS ->\
      \ \"code-reviewer\"\n   2. PLANNER_KEYWORDS -> \"planner\"\n   3. QA_AUDITOR_KEYWORDS\
      \ -> \"qa-auditor\"\n   4. SPEC_VERIFIER_KEYWORDS -> \"spec-verifier\"\n   5.\
      \ UX_REVIEWER_KEYWORDS -> \"ux-reviewer\"\n   6. DESIGNER_KEYWORDS -> \"systems-designer\"\
      \n   7. Default -> \"coder\"\n\n5. Verify syntax:\n   python3 -c \"import py_compile;\
      \ py_compile.compile('scripts/plan-orchestrator.py', doraise=True); print('syntax\
      \ OK')\"\n\n6. Verify the inference works:\n   python3 -c \"\n   import importlib.util\n\
      \   spec = importlib.util.spec_from_file_location('po', 'scripts/plan-orchestrator.py')\n\
      \   mod = importlib.util.module_from_spec(spec)\n   spec.loader.exec_module(mod)\n\
      \   task1 = {'name': 'Spec verification', 'description': 'Run spec verifier\
      \ on the feature'}\n   result1 = mod.infer_agent_for_task(task1)\n   assert\
      \ result1 == 'spec-verifier', f'Expected spec-verifier, got {result1}'\n   task2\
      \ = {'name': 'UX review', 'description': 'Run ux review on the component'}\n\
      \   result2 = mod.infer_agent_for_task(task2)\n   assert result2 == 'ux-reviewer',\
      \ f'Expected ux-reviewer, got {result2}'\n   print(f'Inference tests passed:\
      \ {result1}, {result2}')\n   \"\n\nFiles: scripts/plan-orchestrator.py\n"
    attempts: 1
    last_attempt: '2026-02-16T19:06:18.166185'
    model_used: sonnet
    completed_at: '2026-02-16T19:09:12.960476'
    result_message: Added SPEC_VERIFIER_KEYWORDS and UX_REVIEWER_KEYWORDS to infer_agent_for_task
      with proper priority ordering to avoid false positives
- id: phase-3
  name: Phase 3 - Unit Tests
  status: pending
  tasks:
  - id: '3.1'
    name: Write unit tests for spec-verifier and ux-reviewer integration
    agent: coder
    status: pending
    depends_on:
    - '2.1'
    description: "Create tests/test_spec_verifier_ux_reviewer.py with unit tests verifying\
      \ that both agent definitions load correctly, keyword inference works, and both\
      \ agents integrate with ValidationConfig.\nReference: docs/plans/2026-02-16-12-spec-verifier-ux-reviewer-agents-design.md\n\
      Steps:\n1. Create tests/test_spec_verifier_ux_reviewer.py with these test cases:\n\
      \n   a. test_spec_verifier_agent_loads: Load the spec-verifier agent using\n\
      \      load_agent_definition(\"spec-verifier\"). Assert it returns non-None.\n\
      \      Assert name == \"spec-verifier\", model == \"sonnet\". Assert \"Read\"\
      ,\n      \"Grep\", \"Glob\" are in tools. Assert \"Bash\" is NOT in tools.\n\
      \n   b. test_spec_verifier_agent_body_has_sections: Load the agent and check\n\
      \      that the body contains key sections: \"Verification Checklist\",\n  \
      \    \"Output Format\", \"Verdict\", \"Constraints\".\n\n   c. test_spec_verifier_references_crud_checklist:\
      \ Load the agent and\n      verify the body references \".claude/checklists/crud-operations.md\"\
      \n      to confirm it reuses existing domain knowledge.\n\n   d. test_ux_reviewer_agent_loads:\
      \ Load the ux-reviewer agent using\n      load_agent_definition(\"ux-reviewer\"\
      ). Assert it returns non-None.\n      Assert name == \"ux-reviewer\", model\
      \ == \"sonnet\". Assert \"Read\",\n      \"Grep\", \"Glob\" are in tools. Assert\
      \ \"Bash\" is NOT in tools.\n\n   e. test_ux_reviewer_agent_body_has_sections:\
      \ Load the agent and check\n      that the body contains key sections: \"Review\
      \ Checklist\",\n      \"Output Format\", \"Verdict\", \"Quality Score\", \"\
      Constraints\".\n\n   f. test_ux_reviewer_distinct_from_ux_designer: Load both\
      \ ux-reviewer\n      and ux-designer agents. Assert they have different names.\
      \ Assert\n      ux-reviewer model is \"sonnet\" and ux-designer model is \"\
      opus\".\n\n   g. test_infer_agent_for_spec_verification_task: Call infer_agent_for_task\n\
      \      with a task containing \"spec verification\" in description. Assert\n\
      \      returns \"spec-verifier\".\n\n   h. test_infer_agent_for_functional_spec_task:\
      \ Call infer_agent_for_task\n      with a task containing \"functional spec\"\
      \ in description. Assert\n      returns \"spec-verifier\".\n\n   i. test_infer_agent_for_ux_review_task:\
      \ Call infer_agent_for_task with\n      a task containing \"ux review\" in description.\
      \ Assert returns\n      \"ux-reviewer\".\n\n   j. test_infer_agent_for_accessibility_review_task:\
      \ Call\n      infer_agent_for_task with a task containing \"accessibility review\"\
      .\n      Assert returns \"ux-reviewer\".\n\n   k. test_infer_agent_non_spec_ux_task:\
      \ Call infer_agent_for_task with\n      a task containing \"implement the feature\"\
      . Assert does NOT return\n      \"spec-verifier\" or \"ux-reviewer\" (should\
      \ return \"coder\").\n\n   l. test_validators_list_accepts_new_agents: Create\
      \ a ValidationConfig\n      with validators=[\"code-reviewer\", \"spec-verifier\"\
      , \"ux-reviewer\"].\n      Assert \"spec-verifier\" and \"ux-reviewer\" are\
      \ in validators.\n\n2. Import using importlib (same pattern as test_qa_auditor_integration.py):\n\
      \   import importlib.util\n   spec = importlib.util.spec_from_file_location(\n\
      \       \"plan_orchestrator\", \"scripts/plan-orchestrator.py\")\n   mod = importlib.util.module_from_spec(spec)\n\
      \   spec.loader.exec_module(mod)\n   load_agent_definition = mod.load_agent_definition\n\
      \   infer_agent_for_task = mod.infer_agent_for_task\n   ValidationConfig = mod.ValidationConfig\n\
      \n3. Run: python3 -m pytest tests/test_spec_verifier_ux_reviewer.py -v\n   Fix\
      \ any failures.\n\nFiles: tests/test_spec_verifier_ux_reviewer.py\n"
- id: phase-4
  name: Phase 4 - Verification
  status: pending
  tasks:
  - id: '4.1'
    name: Verify syntax, tests, and dry-run
    agent: code-reviewer
    status: pending
    depends_on:
    - '3.1'
    description: "Run verification checks to confirm the spec-verifier and ux-reviewer\
      \ agents feature works correctly.\nSteps:\n1. Check Python syntax for both scripts:\n\
      \   python3 -c \"import py_compile; py_compile.compile('scripts/auto-pipeline.py',\
      \ doraise=True); py_compile.compile('scripts/plan-orchestrator.py', doraise=True)\"\
      \n\n2. Run unit tests:\n   python3 -m pytest tests/ 2>/dev/null || echo 'No\
      \ test suite configured'\n\n3. Verify both new agents load with correct metadata:\n\
      \   python3 -c \"\n   import importlib.util\n   spec = importlib.util.spec_from_file_location('po',\
      \ 'scripts/plan-orchestrator.py')\n   mod = importlib.util.module_from_spec(spec)\n\
      \   spec.loader.exec_module(mod)\n   for name in ['spec-verifier', 'ux-reviewer']:\n\
      \       agent = mod.load_agent_definition(name)\n       assert agent is not\
      \ None, f'{name} not found'\n       assert agent['name'] == name, f'Wrong name\
      \ for {name}'\n       assert agent['model'] == 'sonnet', f'Wrong model for {name}'\n\
      \       assert 'Bash' not in agent['tools'], f'{name} should not have Bash'\n\
      \       print(f'{name} agent verified: model={agent[\\\"model\\\"]}, tools={agent[\\\
      \"tools\\\"]}')\n   print('Both agents verified')\n   \"\n\n4. Verify SPEC_VERIFIER_KEYWORDS\
      \ and UX_REVIEWER_KEYWORDS exist:\n   python3 -c \"\n   import importlib.util\n\
      \   spec = importlib.util.spec_from_file_location('po', 'scripts/plan-orchestrator.py')\n\
      \   mod = importlib.util.module_from_spec(spec)\n   spec.loader.exec_module(mod)\n\
      \   assert hasattr(mod, 'SPEC_VERIFIER_KEYWORDS'), 'SPEC_VERIFIER_KEYWORDS not\
      \ found'\n   assert len(mod.SPEC_VERIFIER_KEYWORDS) >= 4, f'Too few spec keywords:\
      \ {len(mod.SPEC_VERIFIER_KEYWORDS)}'\n   assert hasattr(mod, 'UX_REVIEWER_KEYWORDS'),\
      \ 'UX_REVIEWER_KEYWORDS not found'\n   assert len(mod.UX_REVIEWER_KEYWORDS)\
      \ >= 4, f'Too few ux keywords: {len(mod.UX_REVIEWER_KEYWORDS)}'\n   print(f'SPEC_VERIFIER_KEYWORDS:\
      \ {mod.SPEC_VERIFIER_KEYWORDS}')\n   print(f'UX_REVIEWER_KEYWORDS: {mod.UX_REVIEWER_KEYWORDS}')\n\
      \   \"\n\n5. Verify infer_agent_for_task returns correct agents:\n   python3\
      \ -c \"\n   import importlib.util\n   spec = importlib.util.spec_from_file_location('po',\
      \ 'scripts/plan-orchestrator.py')\n   mod = importlib.util.module_from_spec(spec)\n\
      \   spec.loader.exec_module(mod)\n   tests = [\n       ({'name': 'Spec verification',\
      \ 'description': 'Run spec verifier'}, 'spec-verifier'),\n       ({'name': 'UX\
      \ review', 'description': 'Run ux review'}, 'ux-reviewer'),\n       ({'name':\
      \ 'Implement feature', 'description': 'Build the feature'}, 'coder'),\n   ]\n\
      \   for task, expected in tests:\n       result = mod.infer_agent_for_task(task)\n\
      \       assert result == expected, f'Expected {expected}, got {result} for {task[\\\
      \"name\\\"]}'\n       print(f'PASS: {task[\\\"name\\\"]} -> {result}')\n   print('All\
      \ inference tests passed')\n   \"\n\n6. Run orchestrator dry-run to verify no\
      \ startup errors:\n   python3 scripts/plan-orchestrator.py --plan .claude/plans/sample-plan.yaml\
      \ --dry-run\n\nIf any check fails, report the failure with specific details.\n\
      Files: scripts/plan-orchestrator.py, .claude/agents/spec-verifier.md,\n    \
      \   .claude/agents/ux-reviewer.md, tests/test_spec_verifier_ux_reviewer.py\n"
