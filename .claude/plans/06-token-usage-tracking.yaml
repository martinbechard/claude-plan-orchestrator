meta:
  name: Token Usage Tracking and Cost Reporting
  description: 'Track token consumption across orchestrator tasks and auto-pipeline
    work items, producing per-task and aggregate usage reports with cost, cache hit
    rate, and token breakdowns.

    '
  plan_doc: docs/plans/2026-02-13-06-token-usage-tracking-design.md
  created: '2026-02-13'
  max_attempts_default: 3
sections:
- id: phase-1
  name: Phase 1 - TaskUsage Dataclass and Parsing
  status: pending
  tasks:
  - id: '1.1'
    name: Add TaskUsage dataclass and extend TaskResult
    agent: coder
    status: pending
    description: "In scripts/plan-orchestrator.py, add a TaskUsage dataclass and extend\
      \ TaskResult.\n\n1. After the TaskResult dataclass (line 278), add:\n\n    @dataclass\n\
      \    class TaskUsage:\n        \"\"\"Token usage metrics from a single Claude\
      \ CLI task execution.\"\"\"\n        input_tokens: int = 0\n        output_tokens:\
      \ int = 0\n        cache_read_tokens: int = 0\n        cache_creation_tokens:\
      \ int = 0\n        total_cost_usd: float = 0.0\n        num_turns: int = 0\n\
      \        duration_api_ms: int = 0\n\n2. Add an optional usage field to the TaskResult\
      \ dataclass:\n\n    usage: Optional[TaskUsage] = None\n\n3. Verify with: python3\
      \ -c \"import py_compile; py_compile.compile('scripts/plan-orchestrator.py',\
      \ doraise=True); print('OK')\"\n\nReference: docs/plans/2026-02-13-06-token-usage-tracking-design.md\n\
      Reference: scripts/plan-orchestrator.py lines 269-278 for TaskResult location\n"
    attempts: 3
    last_attempt: '2026-02-13T22:40:55.537833'
    last_error: "Claude exited with code 1: error: unknown option '---\nname: coder\n\
      description: \"Implementation specialist for coding tasks. Follows CODING-RULES.md,\n\
      \  validates against design docs, and commits frequently.\"\ntools:\n  - Read\n\
      \  - Edit\n  - Write\n  - Bash\n  - Grep\n  - Glob\nmodel: sonnet\n---\n\n#\
      \ Coder Agent\n\n## Role\n\nYou are an implementation specialist. Your job is\
      \ to write high-quality code that\nfollows the project's coding standards and\
      \ design specifications. You receive tasks\nfrom the plan orchestrator and execute\
      \ them preci"
  - id: '1.2'
    name: Add parse_usage_from_result helper function
    agent: coder
    status: pending
    description: "In scripts/plan-orchestrator.py, add a helper function after the\
      \ TaskUsage dataclass.\n\nFunction: parse_usage_from_result(result_data: dict)\
      \ -> TaskUsage\n  - Takes the result dict from Claude CLI JSON output\n  - Extracts\
      \ usage fields using safe .get() with defaults:\n    - input_tokens from result_data.get(\"\
      usage\", {}).get(\"input_tokens\", 0)\n    - output_tokens from result_data.get(\"\
      usage\", {}).get(\"output_tokens\", 0)\n    - cache_read_tokens from result_data.get(\"\
      usage\", {}).get(\"cache_read_input_tokens\", 0)\n    - cache_creation_tokens\
      \ from result_data.get(\"usage\", {}).get(\"cache_creation_input_tokens\", 0)\n\
      \    - total_cost_usd from result_data.get(\"total_cost_usd\", 0.0)\n    - num_turns\
      \ from result_data.get(\"num_turns\", 0)\n    - duration_api_ms from result_data.get(\"\
      duration_api_ms\", 0)\n  - Returns a TaskUsage instance\n  - Add a docstring\
      \ explaining it parses Claude CLI result JSON\n\nVerify: python3 -c \"import\
      \ py_compile; py_compile.compile('scripts/plan-orchestrator.py', doraise=True);\
      \ print('OK')\"\n\nReference: docs/plans/2026-02-13-06-token-usage-tracking-design.md\n"
    depends_on:
    - '1.1'
- id: phase-2
  name: Phase 2 - CLI Output Capture
  status: pending
  tasks:
  - id: '2.1'
    name: Modify stream_json_output to capture result event
    agent: coder
    status: pending
    description: "In scripts/plan-orchestrator.py, modify stream_json_output() (line\
      \ 1392)\nto capture the result event data.\n\nCurrent signature:\n    def stream_json_output(pipe,\
      \ collector: OutputCollector) -> None\n\nNew signature:\n    def stream_json_output(pipe,\
      \ collector: OutputCollector,\n                          result_holder: dict)\
      \ -> None\n\nChanges:\n1. Add a result_holder parameter (a mutable dict passed\
      \ by reference)\n2. In the event_type == \"result\" block (line 1433), after\
      \ the existing\n   print statement, add:\n\n    result_holder[\"result_event\"\
      ] = event\n\n   This captures the full result event dict containing total_cost_usd,\n\
      \   usage, num_turns, duration_ms, duration_api_ms etc.\n\n3. Find the call\
      \ site in run_claude_task() (line 1490-1492) where\n   stream_json_output is\
      \ called via threading.Thread:\n\n    stdout_thread = threading.Thread(\n  \
      \      target=stream_json_output,\n        args=(process.stdout, stdout_collector)\n\
      \    )\n\n   Change to:\n\n    result_holder = {}\n    stdout_thread = threading.Thread(\n\
      \        target=stream_json_output,\n        args=(process.stdout, stdout_collector,\
      \ result_holder)\n    )\n\n   Move result_holder declaration BEFORE the if VERBOSE\
      \ block so it is\n   accessible later regardless of mode. Initialize at function\
      \ start:\n\n    result_holder: dict = {}\n\nVerify: python3 -c \"import py_compile;\
      \ py_compile.compile('scripts/plan-orchestrator.py', doraise=True); print('OK')\"\
      \n\nReference: scripts/plan-orchestrator.py lines 1392-1442, 1488-1498\n"
    depends_on:
    - '1.2'
  - id: '2.2'
    name: Add --output-format json in non-verbose mode
    agent: coder
    status: pending
    description: "In scripts/plan-orchestrator.py, modify run_claude_task() to always\
      \ use\nstructured output.\n\nCurrently (lines 1459-1461):\n    if VERBOSE:\n\
      \        cmd.extend([\"--output-format\", \"stream-json\", \"--verbose\"])\n\
      \nChange to:\n    if VERBOSE:\n        cmd.extend([\"--output-format\", \"stream-json\"\
      , \"--verbose\"])\n    else:\n        cmd.extend([\"--output-format\", \"json\"\
      ])\n\nThis ensures we always get structured JSON output from the Claude CLI,\n\
      which is necessary to extract usage data in non-verbose mode.\n\nIn non-verbose\
      \ mode, after the process completes and threads are joined\n(after line 1548\
      \ where stdout_thread.join() happens), add JSON parsing:\n\n    # Parse JSON\
      \ result for usage data (non-verbose mode)\n    if not VERBOSE:\n        raw_output\
      \ = stdout_collector.get_output().strip()\n        if raw_output:\n        \
      \    try:\n                parsed_result = json.loads(raw_output)\n        \
      \        result_holder[\"result_event\"] = parsed_result\n            except\
      \ (json.JSONDecodeError, ValueError):\n                verbose_log(\"Failed\
      \ to parse JSON output for usage\", \"WARN\")\n\nThis way both verbose and non-verbose\
      \ paths populate result_holder.\n\nIMPORTANT: The progress dots display (lines\
      \ 1507-1519) reads bytes from\nstdout_collector. Since --output-format json\
      \ writes a single JSON blob at\nthe end, the progress dots will still work (they\
      \ just show activity based\non bytes received). The user-facing display is unchanged.\n\
      \nVerify: python3 -c \"import py_compile; py_compile.compile('scripts/plan-orchestrator.py',\
      \ doraise=True); print('OK')\"\n\nReference: scripts/plan-orchestrator.py lines\
      \ 1459-1461, 1544-1548\n"
    depends_on:
    - '2.1'
  - id: '2.3'
    name: Extract TaskUsage in run_claude_task return paths
    agent: coder
    status: pending
    description: "In scripts/plan-orchestrator.py, modify all TaskResult return statements\n\
      in run_claude_task() to include parsed usage data.\n\nAfter the threads are\
      \ joined and JSON is parsed (work from task 2.1 and 2.2),\nadd usage extraction\
      \ before the return-code checking block:\n\n    # Extract usage data from result\
      \ event\n    task_usage = None\n    result_event = result_holder.get(\"result_event\"\
      )\n    if result_event:\n        task_usage = parse_usage_from_result(result_event)\n\
      \nThen add usage=task_usage to every TaskResult(...) constructor call in\nrun_claude_task()\
      \ that returns after a real execution (not dry_run, not\nexception handlers).\
      \ Specifically:\n\n1. Rate-limited return (line ~1586): add usage=task_usage\n\
      2. Error return (line ~1595): add usage=task_usage\n3. Status completed return\
      \ (line ~1614): add usage=task_usage\n4. Status failed return (line ~1622):\
      \ add usage=task_usage\n5. No status file return (line ~1631): add usage=task_usage\n\
      \nDo NOT add usage to:\n- The dry_run return (line 1448) - no execution happened\n\
      - The TimeoutExpired return (line ~1639) - no result available\n- The generic\
      \ Exception return (line ~1646) - no result available\n\nVerify: python3 -c\
      \ \"import py_compile; py_compile.compile('scripts/plan-orchestrator.py', doraise=True);\
      \ print('OK')\"\n\nReference: scripts/plan-orchestrator.py lines 1580-1650\n"
    depends_on:
    - '2.2'
- id: phase-3
  name: Phase 3 - Plan Usage Tracker and Reporting
  status: pending
  tasks:
  - id: '3.1'
    name: Add PlanUsageTracker class
    agent: coder
    status: pending
    description: "In scripts/plan-orchestrator.py, add a PlanUsageTracker class after\n\
      the CircuitBreaker class (around line 330).\n\nclass PlanUsageTracker:\n   \
      \ \"\"\"Accumulates token usage across all tasks in a plan run.\"\"\"\n\n  \
      \  def __init__(self):\n        self.task_usages: dict[str, TaskUsage] = {}\n\
      \n    def record(self, task_id: str, usage: TaskUsage) -> None:\n        \"\"\
      \"Record usage for a completed task.\"\"\"\n        self.task_usages[task_id]\
      \ = usage\n\n    def get_total_usage(self) -> TaskUsage:\n        \"\"\"Aggregate\
      \ usage across all recorded tasks.\"\"\"\n        total = TaskUsage()\n    \
      \    for u in self.task_usages.values():\n            total.input_tokens +=\
      \ u.input_tokens\n            total.output_tokens += u.output_tokens\n     \
      \       total.cache_read_tokens += u.cache_read_tokens\n            total.cache_creation_tokens\
      \ += u.cache_creation_tokens\n            total.total_cost_usd += u.total_cost_usd\n\
      \            total.num_turns += u.num_turns\n            total.duration_api_ms\
      \ += u.duration_api_ms\n        return total\n\n    def get_cache_hit_rate(self)\
      \ -> float:\n        \"\"\"Calculate cache hit rate across all tasks.\"\"\"\n\
      \        total = self.get_total_usage()\n        denominator = total.cache_read_tokens\
      \ + total.input_tokens\n        if denominator == 0:\n            return 0.0\n\
      \        return total.cache_read_tokens / denominator\n\n    def get_section_usage(self,\
      \ plan: dict, section_id: str) -> TaskUsage:\n        \"\"\"Aggregate usage\
      \ for tasks in a specific section.\"\"\"\n        section_usage = TaskUsage()\n\
      \        for section in plan.get(\"sections\", []):\n            if section.get(\"\
      id\") != section_id:\n                continue\n            for task in section.get(\"\
      tasks\", []):\n                task_id = task.get(\"id\", \"\")\n          \
      \      if task_id in self.task_usages:\n                    u = self.task_usages[task_id]\n\
      \                    section_usage.input_tokens += u.input_tokens\n        \
      \            section_usage.output_tokens += u.output_tokens\n              \
      \      section_usage.cache_read_tokens += u.cache_read_tokens\n            \
      \        section_usage.cache_creation_tokens += u.cache_creation_tokens\n  \
      \                  section_usage.total_cost_usd += u.total_cost_usd\n      \
      \              section_usage.num_turns += u.num_turns\n                    section_usage.duration_api_ms\
      \ += u.duration_api_ms\n        return section_usage\n\n    def format_task_summary(self,\
      \ task_id: str) -> str:\n        \"\"\"Format a single-line summary for a task.\"\
      \"\"\n        usage = self.task_usages.get(task_id)\n        if not usage:\n\
      \            return \"\"\n        total = self.get_total_usage()\n        cache_denom\
      \ = usage.cache_read_tokens + usage.input_tokens\n        cache_rate = (usage.cache_read_tokens\
      \ / cache_denom * 100) if cache_denom > 0 else 0\n        return (\n       \
      \     f\"[Usage] Task {task_id}: ${usage.total_cost_usd:.4f} | \"\n        \
      \    f\"{usage.input_tokens:,} in / {usage.output_tokens:,} out / \"\n     \
      \       f\"{usage.cache_read_tokens:,} cached ({cache_rate:.0f}% cache hit)\
      \ | \"\n            f\"Running: ${total.total_cost_usd:.4f}\"\n        )\n\n\
      \    def format_final_summary(self, plan: dict) -> str:\n        \"\"\"Format\
      \ the complete usage summary for plan completion.\"\"\"\n        total = self.get_total_usage()\n\
      \        cache_rate = self.get_cache_hit_rate() * 100\n        lines = [\n \
      \           \"\\n=== Usage Summary ===\",\n            f\"Total cost: ${total.total_cost_usd:.4f}\"\
      ,\n            f\"Total tokens: {total.input_tokens:,} input / {total.output_tokens:,}\
      \ output\",\n            f\"Cache: {total.cache_read_tokens:,} read / {total.cache_creation_tokens:,}\
      \ created ({cache_rate:.0f}% hit rate)\",\n            f\"API time: {total.duration_api_ms\
      \ / 1000:.1f}s across {total.num_turns} turns\",\n            \"Per-section\
      \ breakdown:\",\n        ]\n        for section in plan.get(\"sections\", []):\n\
      \            sid = section.get(\"id\", \"\")\n            su = self.get_section_usage(plan,\
      \ sid)\n            task_count = len([\n                t for t in section.get(\"\
      tasks\", [])\n                if t.get(\"id\", \"\") in self.task_usages\n \
      \           ])\n            if task_count > 0:\n                lines.append(\n\
      \                    f\"  {section.get('name', sid)}: \"\n                 \
      \   f\"${su.total_cost_usd:.4f} ({task_count} tasks)\"\n                )\n\
      \        return \"\\n\".join(lines)\n\nVerify: python3 -c \"import py_compile;\
      \ py_compile.compile('scripts/plan-orchestrator.py', doraise=True); print('OK')\"\
      \n\nReference: docs/plans/2026-02-13-06-token-usage-tracking-design.md\n"
    depends_on:
    - '1.2'
  - id: '3.2'
    name: Integrate PlanUsageTracker into main execution loop
    agent: coder
    status: pending
    description: "In scripts/plan-orchestrator.py, integrate PlanUsageTracker into\n\
      the run_orchestrator() function.\n\n1. In the run_orchestrator() function (starts\
      \ around line 1700), create a\n   PlanUsageTracker instance near the circuit_breaker\
      \ initialization:\n\n    usage_tracker = PlanUsageTracker()\n\n2. In the SEQUENTIAL\
      \ EXECUTION block (line ~1952), after task_result is\n   obtained (line 2049)\
      \ and before the result printout (line 2052), add:\n\n    # Record usage\n \
      \   if task_result.usage:\n        usage_tracker.record(task_id, task_result.usage)\n\
      \        print(usage_tracker.format_task_summary(task_id))\n\n3. In the PARALLEL\
      \ EXECUTION block (line ~1800), after results are\n   collected from futures\
      \ (line 1830-1833), add usage recording:\n\n    # Record usage from parallel\
      \ tasks\n    for tid, tres in results.items():\n        if tres and tres.usage:\n\
      \            usage_tracker.record(tid, tres.usage)\n            print(f\"  {usage_tracker.format_task_summary(tid)}\"\
      )\n\n4. At the \"All tasks completed\" point (line 1962), before the smoke test\n\
      \   section, add:\n\n    # Print usage summary\n    print(usage_tracker.format_final_summary(plan))\n\
      \n5. Also print usage summary at the final Summary section (line 2143):\n\n\
      \    print(f\"\\n=== Summary ===\")\n    print(f\"Tasks completed: {tasks_completed}\"\
      )\n    print(f\"Tasks failed: {tasks_failed}\")\n    if usage_tracker.task_usages:\n\
      \        print(usage_tracker.format_final_summary(plan))\n\nVerify: python3\
      \ -c \"import py_compile; py_compile.compile('scripts/plan-orchestrator.py',\
      \ doraise=True); print('OK')\"\n\nReference: scripts/plan-orchestrator.py lines\
      \ 1700-2146\n"
    depends_on:
    - '2.3'
    - '3.1'
  - id: '3.3'
    name: Add usage report file generation
    agent: coder
    status: pending
    description: "In scripts/plan-orchestrator.py, add a method to PlanUsageTracker\n\
      and call it when the plan completes.\n\n1. Add a method to PlanUsageTracker:\n\
      \n    def write_usage_report(self, plan: dict, plan_path: str) -> Optional[str]:\n\
      \        \"\"\"Write a JSON usage report alongside the plan logs.\n        Returns\
      \ the report file path, or None if no usage data.\"\"\"\n        if not self.task_usages:\n\
      \            return None\n\n        meta = plan.get(\"meta\", {})\n        plan_name\
      \ = meta.get(\"name\", \"unknown\")\n        # Sanitize plan name for filename\n\
      \        safe_name = re.sub(r'[^a-zA-Z0-9_-]', '-', plan_name.lower()).strip('-')\n\
      \n        total = self.get_total_usage()\n        cache_rate = self.get_cache_hit_rate()\n\
      \n        report = {\n            \"plan_name\": plan_name,\n            \"\
      plan_path\": plan_path,\n            \"completed_at\": datetime.now().isoformat(),\n\
      \            \"total\": {\n                \"cost_usd\": total.total_cost_usd,\n\
      \                \"input_tokens\": total.input_tokens,\n                \"output_tokens\"\
      : total.output_tokens,\n                \"cache_read_tokens\": total.cache_read_tokens,\n\
      \                \"cache_creation_tokens\": total.cache_creation_tokens,\n \
      \               \"cache_hit_rate\": round(cache_rate, 4),\n                \"\
      num_turns\": total.num_turns,\n                \"duration_api_ms\": total.duration_api_ms,\n\
      \            },\n            \"sections\": [],\n            \"tasks\": [],\n\
      \        }\n\n        for section in plan.get(\"sections\", []):\n         \
      \   sid = section.get(\"id\", \"\")\n            su = self.get_section_usage(plan,\
      \ sid)\n            task_count = len([\n                t for t in section.get(\"\
      tasks\", [])\n                if t.get(\"id\", \"\") in self.task_usages\n \
      \           ])\n            if task_count > 0:\n                report[\"sections\"\
      ].append({\n                    \"id\": sid,\n                    \"name\":\
      \ section.get(\"name\", sid),\n                    \"cost_usd\": su.total_cost_usd,\n\
      \                    \"input_tokens\": su.input_tokens,\n                  \
      \  \"output_tokens\": su.output_tokens,\n                    \"task_count\"\
      : task_count,\n                })\n\n        for task_id, usage in self.task_usages.items():\n\
      \            # Find task name from plan\n            task_name = task_id\n \
      \           for section in plan.get(\"sections\", []):\n                for\
      \ task in section.get(\"tasks\", []):\n                    if task.get(\"id\"\
      ) == task_id:\n                        task_name = task.get(\"name\", task_id)\n\
      \                        break\n            report[\"tasks\"].append({\n   \
      \             \"id\": task_id,\n                \"name\": task_name,\n     \
      \           \"cost_usd\": usage.total_cost_usd,\n                \"input_tokens\"\
      : usage.input_tokens,\n                \"output_tokens\": usage.output_tokens,\n\
      \                \"cache_read_tokens\": usage.cache_read_tokens,\n         \
      \       \"cache_creation_tokens\": usage.cache_creation_tokens,\n          \
      \      \"num_turns\": usage.num_turns,\n                \"duration_api_ms\"\
      : usage.duration_api_ms,\n            })\n\n        log_dir = Path(\".claude/plans/logs\"\
      )\n        log_dir.mkdir(parents=True, exist_ok=True)\n        report_path =\
      \ log_dir / f\"{safe_name}-usage-report.json\"\n        with open(report_path,\
      \ \"w\") as f:\n            json.dump(report, f, indent=2)\n        return str(report_path)\n\
      \n2. Call write_usage_report() at the plan completion point (line ~1962),\n\
      \   right after printing the usage summary:\n\n    report_path = usage_tracker.write_usage_report(plan,\
      \ plan_path)\n    if report_path:\n        print(f\"[Usage report saved to:\
      \ {report_path}]\")\n\nVerify: python3 -c \"import py_compile; py_compile.compile('scripts/plan-orchestrator.py',\
      \ doraise=True); print('OK')\"\n\nReference: docs/plans/2026-02-13-06-token-usage-tracking-design.md\n"
    depends_on:
    - '3.2'
- id: phase-4
  name: Phase 4 - Log File Enhancement
  status: pending
  tasks:
  - id: '4.1'
    name: Add usage data to per-task log files
    agent: coder
    status: pending
    description: "In scripts/plan-orchestrator.py, enhance the log file writing section\n\
      in run_claude_task() (lines 1559-1575) to include usage data.\n\nAfter the existing\
      \ log header lines (Return code, Stdout lines, Stderr lines),\nadd usage data\
      \ if available:\n\n    # After line: f.write(f\"Stderr lines: {stderr_collector.line_count}\\\
      n\")\n    # Add:\n    if task_usage:\n        f.write(f\"Cost: ${task_usage.total_cost_usd:.4f}\\\
      n\")\n        f.write(f\"Tokens: {task_usage.input_tokens} input / \"\n    \
      \          f\"{task_usage.output_tokens} output / \"\n              f\"{task_usage.cache_read_tokens}\
      \ cache_read / \"\n              f\"{task_usage.cache_creation_tokens} cache_create\\\
      n\")\n        f.write(f\"Turns: {task_usage.num_turns}\\n\")\n        f.write(f\"\
      API time: {task_usage.duration_api_ms}ms\\n\")\n\nNote: task_usage is computed\
      \ from result_holder before this code block runs\n(as implemented in task 2.3).\
      \ The variable is already in scope.\n\nVerify: python3 -c \"import py_compile;\
      \ py_compile.compile('scripts/plan-orchestrator.py', doraise=True); print('OK')\"\
      \n\nReference: scripts/plan-orchestrator.py lines 1559-1575\n"
    depends_on:
    - '2.3'
- id: phase-5
  name: Phase 5 - Auto-Pipeline Integration
  status: pending
  tasks:
  - id: '5.1'
    name: Add usage report reading to auto-pipeline
    agent: coder
    status: pending
    description: "In scripts/auto-pipeline.py, add the ability to read and aggregate\n\
      usage reports from orchestrator runs.\n\n1. Add a function after execute_plan()\
      \ (line ~1013):\n\n    def read_usage_report(plan_path: str) -> Optional[dict]:\n\
      \        \"\"\"Read the usage report JSON generated by the orchestrator.\n \
      \       Returns the parsed report dict, or None if not found.\"\"\"\n      \
      \  try:\n            with open(plan_path, \"r\") as f:\n                plan\
      \ = yaml.safe_load(f)\n            plan_name = plan.get(\"meta\", {}).get(\"\
      name\", \"unknown\")\n            safe_name = re.sub(r'[^a-zA-Z0-9_-]', '-',\
      \ plan_name.lower()).strip('-')\n            report_path = Path(\".claude/plans/logs\"\
      ) / f\"{safe_name}-usage-report.json\"\n            if report_path.exists():\n\
      \                with open(report_path, \"r\") as f:\n                    return\
      \ json.load(f)\n        except (IOError, json.JSONDecodeError, yaml.YAMLError)\
      \ as e:\n            log(f\"Failed to read usage report: {e}\")\n        return\
      \ None\n\n2. In the execute_plan() function, after the success check (line ~1006),\n\
      \   read and log the usage report:\n\n    # After: log(f\"Orchestrator completed:\
      \ {completed} tasks\")\n    # Add:\n    usage_report = read_usage_report(plan_path)\n\
      \    if usage_report:\n        total = usage_report.get(\"total\", {})\n   \
      \     log(f\"  Cost: ${total.get('cost_usd', 0):.4f}\")\n        log(f\"  Tokens:\
      \ {total.get('input_tokens', 0):,} in / {total.get('output_tokens', 0):,} out\"\
      )\n        log(f\"  Cache hit rate: {total.get('cache_hit_rate', 0) * 100:.0f}%\"\
      )\n\n3. Make sure 're' is imported at the top of auto-pipeline.py (check if\
      \ it\n   already is).\n\nVerify: python3 -c \"import py_compile; py_compile.compile('scripts/auto-pipeline.py',\
      \ doraise=True); print('OK')\"\n\nReference: scripts/auto-pipeline.py lines\
      \ 965-1013\n"
    depends_on:
    - '3.3'
  - id: '5.2'
    name: Add session-level usage aggregation to auto-pipeline
    agent: coder
    status: pending
    description: "In scripts/auto-pipeline.py, add session-level usage tracking.\n\
      \n1. Add a module-level list to track session usage. Near the other globals\n\
      \   (around line 85), add:\n\n    SESSION_USAGE_REPORTS: list[dict] = []\n\n\
      2. In execute_plan(), after reading the usage report (from task 5.1),\n   append\
      \ it to session tracking:\n\n    if usage_report:\n        SESSION_USAGE_REPORTS.append(usage_report)\n\
      \n3. Add a function to write the session summary:\n\n    def write_session_usage_summary()\
      \ -> Optional[str]:\n        \"\"\"Write a session-level usage summary aggregating\
      \ all plan executions.\"\"\"\n        if not SESSION_USAGE_REPORTS:\n      \
      \      return None\n\n        total_cost = sum(\n            r.get(\"total\"\
      , {}).get(\"cost_usd\", 0)\n            for r in SESSION_USAGE_REPORTS\n   \
      \     )\n        total_input = sum(\n            r.get(\"total\", {}).get(\"\
      input_tokens\", 0)\n            for r in SESSION_USAGE_REPORTS\n        )\n\
      \        total_output = sum(\n            r.get(\"total\", {}).get(\"output_tokens\"\
      , 0)\n            for r in SESSION_USAGE_REPORTS\n        )\n\n        summary\
      \ = {\n            \"session_timestamp\": datetime.now().isoformat(),\n    \
      \        \"plans_executed\": len(SESSION_USAGE_REPORTS),\n            \"total_cost_usd\"\
      : total_cost,\n            \"total_input_tokens\": total_input,\n          \
      \  \"total_output_tokens\": total_output,\n            \"plans\": [\n      \
      \          {\n                    \"plan_name\": r.get(\"plan_name\", \"unknown\"\
      ),\n                    \"cost_usd\": r.get(\"total\", {}).get(\"cost_usd\"\
      , 0),\n                }\n                for r in SESSION_USAGE_REPORTS\n \
      \           ],\n        }\n\n        log_dir = Path(\".claude/plans/logs\")\n\
      \        log_dir.mkdir(parents=True, exist_ok=True)\n        report_path = log_dir\
      \ / f\"pipeline-session-{datetime.now().strftime('%Y%m%d-%H%M%S')}.json\"\n\
      \        with open(report_path, \"w\") as f:\n            json.dump(summary,\
      \ f, indent=2)\n\n        log(f\"\\n=== Pipeline Session Usage ===\")\n    \
      \    log(f\"Plans executed: {len(SESSION_USAGE_REPORTS)}\")\n        log(f\"\
      Total cost: ${total_cost:.4f}\")\n        log(f\"Total tokens: {total_input:,}\
      \ input / {total_output:,} output\")\n        return str(report_path)\n\n4.\
      \ Call write_session_usage_summary() at pipeline exit points.\n   Find the main\
      \ pipeline loop exit (search for the main loop that calls\n   process_item or\
      \ similar). Add the call before the final exit/return.\n   Common pattern: look\
      \ for KeyboardInterrupt handler or the main() function\n   exit path.\n\nVerify:\
      \ python3 -c \"import py_compile; py_compile.compile('scripts/auto-pipeline.py',\
      \ doraise=True); print('OK')\"\n\nReference: scripts/auto-pipeline.py\nReference:\
      \ docs/plans/2026-02-13-06-token-usage-tracking-design.md\n"
    depends_on:
    - '5.1'
- id: phase-6
  name: Phase 6 - Unit Tests
  status: pending
  tasks:
  - id: '6.1'
    name: Create unit tests for TaskUsage and parse_usage_from_result
    agent: coder
    status: pending
    description: "Create tests/test_token_usage.py with unit tests.\n\nThe test file\
      \ should test:\n\n1. TaskUsage dataclass defaults - all fields default to 0/0.0\n\
      \n2. parse_usage_from_result with complete data:\n   Input:\n     {\"total_cost_usd\"\
      : 0.0234, \"num_turns\": 5, \"duration_api_ms\": 12345,\n      \"usage\": {\"\
      input_tokens\": 1000, \"output_tokens\": 500,\n               \"cache_read_input_tokens\"\
      : 800, \"cache_creation_input_tokens\": 200}}\n   Expected: TaskUsage with those\
      \ values\n\n3. parse_usage_from_result with empty dict:\n   Input: {}\n   Expected:\
      \ TaskUsage with all zeros\n\n4. parse_usage_from_result with partial data:\n\
      \   Input: {\"total_cost_usd\": 0.01}\n   Expected: TaskUsage(total_cost_usd=0.01,\
      \ rest zeros)\n\n5. PlanUsageTracker.record and get_total_usage:\n   Record\
      \ two tasks, verify totals are summed correctly\n\n6. PlanUsageTracker.get_cache_hit_rate:\n\
      \   - With data: cache_read=800, input=200 -> 0.8\n   - With no data: should\
      \ return 0.0\n\n7. PlanUsageTracker.format_task_summary:\n   Record a task,\
      \ verify the formatted string contains cost and token counts\n\n8. PlanUsageTracker.format_final_summary:\n\
      \   Create a mock plan dict with sections and tasks, record usage,\n   verify\
      \ the summary contains section breakdowns\n\nImport approach: Since plan-orchestrator.py\
      \ is a script (not a package),\nuse this import pattern at the top of the test\
      \ file:\n\n    import sys\n    import os\n    sys.path.insert(0, os.path.join(os.path.dirname(__file__),\
      \ '..', 'scripts'))\n    # Import by loading the module\n    import importlib.util\n\
      \    spec = importlib.util.spec_from_file_location(\n        \"plan_orchestrator\"\
      ,\n        os.path.join(os.path.dirname(__file__), '..', 'scripts', 'plan-orchestrator.py')\n\
      \    )\n    # Note: plan-orchestrator.py has top-level code that runs on import.\n\
      \    # We may need to mock or handle that. If import fails, test the\n    #\
      \ functions by extracting them or by using subprocess.\n\nALTERNATIVE if direct\
      \ import is difficult due to top-level code:\nCreate standalone test functions\
      \ that replicate the logic and test it.\nFor example, copy the parse_usage_from_result\
      \ logic into the test and\nverify it works correctly. Then test PlanUsageTracker\
      \ by instantiating it\ndirectly (it has no external dependencies).\n\nRun: python\
      \ -m pytest tests/test_token_usage.py -v\nFix any failures before marking complete.\n\
      \nReference: docs/plans/2026-02-13-06-token-usage-tracking-design.md\n"
    depends_on:
    - '3.1'
    - '1.2'
  - id: '6.2'
    name: Create integration test for usage report generation
    agent: coder
    status: pending
    description: "Add integration tests to tests/test_token_usage.py for usage report\n\
      generation.\n\nAdd these test cases:\n\n1. PlanUsageTracker.write_usage_report\
      \ with data:\n   - Create a mock plan dict with meta.name, sections, and tasks\n\
      \   - Record usage for several tasks\n   - Call write_usage_report(plan, \"\
      test-plan.yaml\")\n   - Verify the JSON file was created in .claude/plans/logs/\n\
      \   - Read and parse the JSON, verify structure:\n     - plan_name matches\n\
      \     - total.cost_usd is correct sum\n     - sections array has correct per-section\
      \ breakdown\n     - tasks array has correct per-task data\n   - Clean up the\
      \ generated file after test (use tmp_path fixture or\n     manually delete)\n\
      \n2. PlanUsageTracker.write_usage_report with no data:\n   - Create tracker\
      \ with no recordings\n   - Call write_usage_report\n   - Verify it returns None\
      \ and no file is created\n\n3. Test the report filename sanitization:\n   -\
      \ Use a plan name with special characters like \"My Plan (v2) - test!\"\n  \
      \ - Verify the generated filename uses only safe characters\n\nUse tmp_path\
      \ or tempfile to avoid polluting the real logs directory.\nYou may need to monkeypatch\
      \ Path or use os.chdir to a temp directory.\n\nRun: python -m pytest tests/test_token_usage.py\
      \ -v\nFix any failures before marking complete.\n\nReference: docs/plans/2026-02-13-06-token-usage-tracking-design.md\n"
    depends_on:
    - '3.3'
    - '6.1'
- id: phase-7
  name: Phase 7 - Verification
  status: pending
  tasks:
  - id: '7.1'
    name: Run full verification suite
    agent: code-reviewer
    status: pending
    description: "Run all verification checks to confirm the token usage tracking\n\
      feature works correctly.\n\n1. Syntax check both scripts:\n   python3 -c \"\
      import py_compile; py_compile.compile('scripts/plan-orchestrator.py', doraise=True);\
      \ print('plan-orchestrator.py: OK')\"\n   python3 -c \"import py_compile; py_compile.compile('scripts/auto-pipeline.py',\
      \ doraise=True); print('auto-pipeline.py: OK')\"\n\n2. Run unit tests:\n   python\
      \ -m pytest tests/test_token_usage.py -v\n\n3. Run orchestrator dry-run to verify\
      \ no regressions:\n   python scripts/plan-orchestrator.py --plan .claude/plans/sample-plan.yaml\
      \ --dry-run\n\n4. Verify TaskUsage dataclass exists and has correct fields:\n\
      \   python3 -c \"\n   import ast, sys\n   with open('scripts/plan-orchestrator.py')\
      \ as f:\n       source = f.read()\n   assert 'class TaskUsage' in source, 'TaskUsage\
      \ class not found'\n   assert 'input_tokens' in source, 'input_tokens field\
      \ not found'\n   assert 'output_tokens' in source, 'output_tokens field not\
      \ found'\n   assert 'cache_read_tokens' in source, 'cache_read_tokens field\
      \ not found'\n   assert 'total_cost_usd' in source, 'total_cost_usd field not\
      \ found'\n   assert 'parse_usage_from_result' in source, 'parse function not\
      \ found'\n   assert 'class PlanUsageTracker' in source, 'PlanUsageTracker not\
      \ found'\n   assert 'write_usage_report' in source, 'write_usage_report not\
      \ found'\n   print('All structural checks passed')\n   \"\n\n5. Verify --output-format\
      \ json is added for non-verbose mode:\n   python3 -c \"\n   with open('scripts/plan-orchestrator.py')\
      \ as f:\n       source = f.read()\n   assert '--output-format\", \"json' in\
      \ source or \\\\\\n          '--output-format\\\", \\\\\"json' in source, \\\
      \\\\n          'Non-verbose --output-format json not found'\n   print('Output\
      \ format check passed')\n   \"\n\n6. Verify auto-pipeline reads usage reports:\n\
      \   python3 -c \"\n   with open('scripts/auto-pipeline.py') as f:\n       source\
      \ = f.read()\n   assert 'read_usage_report' in source, 'read_usage_report not\
      \ found'\n   assert 'SESSION_USAGE_REPORTS' in source, 'Session tracking not\
      \ found'\n   assert 'write_session_usage_summary' in source, 'Session summary\
      \ not found'\n   print('Auto-pipeline integration checks passed')\n   \"\n\n\
      If any check fails, report which check failed and what the error was.\nDo not\
      \ attempt to fix - just report findings.\n\nReference: docs/plans/2026-02-13-06-token-usage-tracking-design.md\n"
    depends_on:
    - '6.2'
    - '5.2'
    - '4.1'
    max_attempts: 5
