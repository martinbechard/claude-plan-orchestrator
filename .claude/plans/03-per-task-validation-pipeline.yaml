meta:
  name: Per-Task Validation Pipeline
  description: 'Extend the orchestrator to optionally spawn a validator agent after
    each implementation task completes. The validator independently verifies the result
    and produces a PASS/WARN/FAIL verdict. On FAIL, the task is retried with validation
    findings prepended to the prompt. Validation is opt-in via plan meta.validation
    configuration.

    '
  plan_doc: docs/plans/2026-02-13-03-per-task-validation-pipeline-design.md
  created: '2026-02-13'
  max_attempts_default: 3
sections:
- id: phase-1
  name: Phase 1 - Agent Definitions
  status: completed
  tasks:
  - id: '1.1'
    name: Create validator agent definition
    agent: coder
    status: completed
    description: "Create the validator.md agent definition file in .claude/agents/.\n\
      \nFile to create: .claude/agents/validator.md\n\nThe file uses markdown with\
      \ YAML frontmatter (--- delimited). Structure:\n\nFrontmatter fields:\n\n  name:\
      \ validator\n\n  description: \"Post-task verification coordinator. Runs build\
      \ and tests,\n    checks task requirements are met, produces PASS/WARN/FAIL\
      \ verdict.\n    Does not modify code.\"\n\n  tools: [Read, Grep, Glob, Bash]\n\
      \n  model: sonnet\n\n\nBody content should include these sections:\n\n1. Role:\
      \ You are a post-task validator. Your job is to independently verify\n   that\
      \ a completed task meets its requirements. You do NOT fix issues - you\n   only\
      \ observe, test, and report findings.\n\n2. Before Validating checklist:\n \
      \  - Read the original task description provided in the prompt\n   - Read the\
      \ task result message\n   - Identify files that were expected to be created\
      \ or modified\n   - Read the design document if one is referenced\n\n3. Validation\
      \ Steps:\n   - Run the build command provided in the prompt to check compilation\n\
      \   - Run the test command provided in the prompt to check tests pass\n   -\
      \ Verify each requirement from the task description is satisfied\n   - Check\
      \ for regressions in related code (imports, exports, integrations)\n\n4. Output\
      \ Format (use this exact format):\n\n   **Verdict: PASS** or **Verdict: WARN**\
      \ or **Verdict: FAIL**\n\n   **Findings:**\n\n   - [PASS] Description with file:line\
      \ references\n\n   - [WARN] Description with file:line references\n\n   - [FAIL]\
      \ Description with file:line references\n\n   **Evidence:**\n\n   - Finding\
      \ N: Command output or code reference supporting each finding\n\n5. Verdict\
      \ Rules:\n   - PASS: All requirements met, build and tests pass, no regressions\n\
      \   - WARN: Requirements met but minor issues found (style, naming, missing\n\
      \     comments). Build and tests pass.\n   - FAIL: Requirements not met, build\
      \ fails, tests fail, or regressions\n     found. Provide specific evidence for\
      \ each failure.\n\n6. Output Protocol:\n   Write task-status.json when done\
      \ with task_id, status, message, timestamp,\n   plan_modified fields. The message\
      \ should include the verdict and\n   finding count.\n\n\nIMPORTANT: The Bash\
      \ tool is included so the validator can run build and test commands. The validator\
      \ must NOT use Bash to modify files - only to run verification commands.\n\n\
      Reference: docs/plans/2026-02-13-03-per-task-validation-pipeline-design.md\n"
    attempts: 1
    last_attempt: '2026-02-13T23:21:48.542387'
    completed_at: '2026-02-13T23:22:50.677044'
    result_message: 'Created .claude/agents/validator.md with YAML frontmatter (name,
      description, tools, model) and structured body sections: Role, Before Validating
      checklist, Validation Steps, Output Format, Verdict Rules, Constraints, and
      Output Protocol. Follows established agent definition pattern from coder.md
      and code-reviewer.md.'
  - id: '1.2'
    name: Create issue-verifier agent definition
    agent: coder
    status: completed
    description: "Create the issue-verifier.md agent definition file in .claude/agents/.\n\
      \nFile to create: .claude/agents/issue-verifier.md\n\nThe file uses markdown\
      \ with YAML frontmatter (--- delimited). Structure:\n\nFrontmatter fields:\n\
      \n  name: issue-verifier\n\n  description: \"Defect fix verification specialist.\
      \ Reads original defect\n    file, checks reported symptoms are resolved, runs\
      \ targeted tests.\n    Produces PASS/FAIL with specific evidence.\"\n\n  tools:\
      \ [Read, Grep, Glob, Bash]\n\n  model: sonnet\n\n\nBody content should include\
      \ these sections:\n\n1. Role: You are a defect fix verifier. Your job is to\
      \ independently\n   verify that a defect fix has resolved the originally reported\
      \ symptoms.\n   You do NOT fix issues - you only observe, test, and report.\n\
      \n2. Before Verifying checklist:\n   - Read the original defect file (path provided\
      \ in the prompt)\n   - Read the Expected Behavior and Actual Behavior sections\n\
      \   - Read the Fix Required section for testable conditions\n   - Check if there\
      \ is a Verification Log section with prior attempts\n\n3. Verification Steps:\n\
      \   - Run the build command to verify code compiles\n   - Run the test command\
      \ to verify tests pass\n   - For each item in \"Fix Required\" that describes\
      \ a testable condition,\n     run the command or check the condition\n   - Verify\
      \ the reported symptom is actually gone (the MOST IMPORTANT check)\n   - Check\
      \ for regressions in related functionality\n\n4. Output Format (use the same\
      \ format as validator.md):\n\n   **Verdict: PASS** or **Verdict: FAIL**\n\n\
      \   **Checks performed:**\n\n   - [x] or [ ] Build passes\n\n   - [x] or [ ]\
      \ Unit tests pass\n\n   - [x] or [ ] (each specific symptom check from the defect)\n\
      \n   **Findings:**\n\n   (describe what you observed for each check with specific\
      \ command outputs)\n\n5. Constraints:\n   - Do NOT modify any code files\n \
      \  - Do NOT fix anything\n   - ONLY read, run verification commands, and report\
      \ findings\n   - Be specific about command outputs and observations\n\n6. Output\
      \ Protocol:\n   Write task-status.json when done with task_id, status, message,\
      \ timestamp,\n   plan_modified fields.\n\n\nReference: docs/plans/2026-02-13-03-per-task-validation-pipeline-design.md\n"
    attempts: 1
    last_attempt: '2026-02-13T23:22:52.735092'
    completed_at: '2026-02-13T23:23:52.021683'
    result_message: 'Created .claude/agents/issue-verifier.md with YAML frontmatter
      (name, description, tools, model) and structured body sections: Role, Before
      Verifying checklist, Verification Steps, Output Format (PASS/FAIL with checkbox
      format), Constraints, and Output Protocol. Follows established agent definition
      pattern from validator.md and coder.md.'
- id: phase-2
  name: Phase 2 - Orchestrator Data Structures
  status: completed
  tasks:
  - id: '2.1'
    name: Add ValidationConfig and ValidationVerdict dataclasses
    agent: coder
    status: completed
    depends_on:
    - '1.2'
    description: "In scripts/plan-orchestrator.py, add two new dataclasses after the\
      \ TaskResult dataclass (currently at line 270).\n\nDataclass 1: ValidationConfig\n\
      \n  @dataclass\n\n  class ValidationConfig:\n\n      \"\"\"Configuration for\
      \ per-task validation parsed from plan meta.\"\"\"\n\n      enabled: bool =\
      \ False\n\n      run_after: list[str] = field(default_factory=lambda: [\"coder\"\
      ])\n\n      validators: list[str] = field(default_factory=lambda: [\"validator\"\
      ])\n\n      max_validation_attempts: int = 1\n\n\nDataclass 2: ValidationVerdict\n\
      \n  @dataclass\n\n  class ValidationVerdict:\n\n      \"\"\"Result of a validation\
      \ pass on a completed task.\"\"\"\n\n      verdict: str       # \"PASS\", \"\
      WARN\", or \"FAIL\"\n\n      findings: list[str] = field(default_factory=list)\n\
      \n      raw_output: str = \"\"\n\n\nAlso add the required import at the top\
      \ of the file. The 'field' function from dataclasses is needed (check if it\
      \ is already imported - the existing TaskResult uses @dataclass so the import\
      \ may be there, but 'field' may not be imported).\n\nAdd a module-level constant\
      \ for verdict parsing:\n\n  VERDICT_PATTERN = re.compile(\n      r\"\\*\\*Verdict:\\\
      s*(PASS|WARN|FAIL)\\*\\*\", re.IGNORECASE\n  )\n\n  FINDING_PATTERN = re.compile(\n\
      \      r\"- \\[(PASS|WARN|FAIL)\\]\\s+(.+)\", re.IGNORECASE\n  )\n\n\nPlace\
      \ VERDICT_PATTERN and FINDING_PATTERN near the other module-level constants\
      \ (after REVIEWER_KEYWORDS around line 130).\n\nReference: docs/plans/2026-02-13-03-per-task-validation-pipeline-design.md\n"
    attempts: 1
    last_attempt: '2026-02-13T23:23:54.079643'
    completed_at: '2026-02-13T23:26:11.591602'
    result_message: Added ValidationConfig and ValidationVerdict dataclasses after
      TaskResult in plan-orchestrator.py. Added field import from dataclasses. Added
      VERDICT_PATTERN and FINDING_PATTERN regex constants after REVIEWER_KEYWORDS.
      All syntax checks pass.
  - id: '2.2'
    name: Add parse_validation_config helper function
    agent: coder
    status: completed
    depends_on:
    - '2.1'
    description: "In scripts/plan-orchestrator.py, add a function to parse validation\
      \ config from plan meta. Place it after the ValidationVerdict dataclass.\n\n\
      Function: parse_validation_config(plan: dict) -> ValidationConfig\n\n  - Extracts\
      \ plan.get(\"meta\", {}).get(\"validation\", {})\n\n  - If the validation dict\
      \ is empty or not a dict, returns\n    ValidationConfig() (defaults: enabled=False)\n\
      \n  - Otherwise creates ValidationConfig from the dict fields:\n\n    enabled\
      \ = val_dict.get(\"enabled\", False)\n\n    run_after = val_dict.get(\"run_after\"\
      , [\"coder\"])\n\n    validators = val_dict.get(\"validators\", [\"validator\"\
      ])\n\n    max_validation_attempts = val_dict.get(\"max_validation_attempts\"\
      , 1)\n\n  - Returns the populated ValidationConfig\n\n\nAdd a docstring explaining\
      \ that this parses the optional meta.validation block from the plan YAML.\n\n\
      Reference: docs/plans/2026-02-13-03-per-task-validation-pipeline-design.md\n"
    attempts: 1
    last_attempt: '2026-02-13T23:26:13.670086'
    completed_at: '2026-02-13T23:27:28.976141'
    result_message: Added parse_validation_config(plan) -> ValidationConfig helper
      function after the ValidationVerdict dataclass in plan-orchestrator.py. The
      function extracts meta.validation from the plan dict, handles missing/empty/non-dict
      values with safe defaults (enabled=False), and returns a populated ValidationConfig.
      Includes docstring. Syntax verified clean.
- id: phase-3
  name: Phase 3 - Validation Execution Logic
  status: completed
  tasks:
  - id: '3.1'
    name: Add build_validation_prompt function
    agent: coder
    status: completed
    depends_on:
    - '2.2'
    description: "In scripts/plan-orchestrator.py, add a function to build the validation\
      \ prompt. Place it after parse_validation_config().\n\nFunction: build_validation_prompt(\n\
      \    task: dict,\n    section: dict,\n    task_result: TaskResult,\n    validator_name:\
      \ str\n) -> str\n\nImplementation:\n1. Load the validator agent definition using\
      \ load_agent_definition(validator_name)\n2. Get the agent body content (or empty\
      \ string if loading fails)\n3. Build the prompt string using an f-string:\n\n\
      \n   {agent_body}\n\n   ---\n\n   You are validating the results of task {task['id']}:\
      \ {task['name']}\n\n\n   ## Original Task Description\n\n   {task.get('description',\
      \ 'No description')}\n\n\n   ## Task Result\n\n   Status: completed\n\n   Message:\
      \ {task_result.message}\n\n   Duration: {task_result.duration_seconds:.1f}s\n\
      \n\n   ## Validation Checks\n\n   1. Run: {BUILD_COMMAND}\n\n   2. Run: {TEST_COMMAND}\n\
      \n   3. Verify the task description requirements are met\n\n   4. Check for\
      \ regressions in related code\n\n\n   ## Output Format\n\n   Produce your verdict\
      \ in this exact format:\n\n\n   **Verdict: PASS** or **Verdict: WARN** or **Verdict:\
      \ FAIL**\n\n\n   **Findings:**\n\n   - [PASS|WARN|FAIL] Description with file:line\
      \ references\n\n\n   IMPORTANT: You MUST write .claude/plans/task-status.json\
      \ when done.\n\n\n4. Return the prompt string\n\nReference: docs/plans/2026-02-13-03-per-task-validation-pipeline-design.md\n"
    attempts: 1
    last_attempt: '2026-02-13T23:27:31.034003'
    completed_at: '2026-02-13T23:28:56.005288'
    result_message: Added build_validation_prompt() function after parse_validation_config()
      in plan-orchestrator.py. The function loads a validator agent definition via
      load_agent_definition(), extracts the body content, and constructs a structured
      validation prompt with task context, result details, build/test commands, and
      PASS/WARN/FAIL output format instructions. Syntax verified clean.
  - id: '3.2'
    name: Add parse_validation_verdict function
    agent: coder
    status: completed
    depends_on:
    - '3.1'
    description: "In scripts/plan-orchestrator.py, add a function to parse the validation\
      \ verdict from validator output. Place it after build_validation_prompt().\n\
      \nFunction: parse_validation_verdict(output: str) -> ValidationVerdict\n\n \
      \ - Uses VERDICT_PATTERN to find the verdict string in the output\n\n  - If\
      \ no verdict found, defaults to \"FAIL\" (conservative: if the\n    validator\
      \ could not produce a verdict, treat as failure)\n\n  - Uses FINDING_PATTERN\
      \ to extract all findings lines\n\n  - Each finding is stored as a string: \"\
      [SEVERITY] description\"\n\n  - Returns ValidationVerdict(verdict=verdict, findings=findings,\n\
      \    raw_output=output)\n\n\nAdd a docstring explaining the parsing logic and\
      \ that it defaults to FAIL when no verdict is found.\n\nReference: docs/plans/2026-02-13-03-per-task-validation-pipeline-design.md\n"
    attempts: 1
    last_attempt: '2026-02-13T23:28:58.060967'
    completed_at: '2026-02-13T23:30:09.762514'
    result_message: Added parse_validation_verdict(output) -> ValidationVerdict function
      after build_validation_prompt() in plan-orchestrator.py. Uses VERDICT_PATTERN
      regex to extract PASS/WARN/FAIL verdict (defaults to FAIL if not found), FINDING_PATTERN
      to extract individual findings as '[SEVERITY] description' strings, and returns
      a ValidationVerdict with verdict, findings list, and raw output. Added DEFAULT_VERDICT
      manifest constant. Includes docstring explaining conservative default behavior.
      Syntax verified clean.
  - id: '3.3'
    name: Add run_validation function
    agent: coder
    status: completed
    depends_on:
    - '3.2'
    description: "In scripts/plan-orchestrator.py, add the main validation execution\
      \ function. Place it after parse_validation_verdict().\n\nFunction: run_validation(\n\
      \    task: dict,\n    section: dict,\n    task_result: TaskResult,\n    validation_config:\
      \ ValidationConfig,\n    dry_run: bool = False\n) -> ValidationVerdict\n\nImplementation:\n\
      1. Determine the agent that executed the task:\n   agent_name = task.get(\"\
      agent\") or infer_agent_for_task(task) or \"coder\"\n\n2. Check if validation\
      \ should run for this agent type:\n   if agent_name not in validation_config.run_after:\n\
      \       return ValidationVerdict(verdict=\"PASS\")  # Skip validation\n\n3.\
      \ For each validator in validation_config.validators:\n   a. Print: \"[VALIDATION]\
      \ Running validator '{validator}' on task {task_id}\"\n   b. Build the validation\
      \ prompt using build_validation_prompt()\n   c. If dry_run: print the prompt\
      \ preview and return ValidationVerdict(verdict=\"PASS\")\n   d. Clear the status\
      \ file (clear_status_file())\n   e. Call run_claude_task(prompt) to execute\
      \ the validator\n   f. If the validator task failed (not task_result.success):\n\
      \      return ValidationVerdict(verdict=\"FAIL\", findings=[\"Validator failed\
      \ to execute\"])\n   g. Read the validator output from the task log\n   h. Parse\
      \ the verdict using parse_validation_verdict()\n   i. If verdict is FAIL, return\
      \ immediately (short-circuit on first failure)\n\n4. Return the final ValidationVerdict\n\
      \nNote: For now, use a simple approach - read the status file after validation\
      \ to get the result message, and parse the log file for the full output. The\
      \ validator's stdout is captured in the log file at .claude/plans/logs/task-*.log\
      \ (the most recent one after the call).\n\nActually, a simpler approach: run_claude_task\
      \ returns TaskResult with message field. The validator writes its verdict into\
      \ the status file message. We can parse the message for the verdict, and for\
      \ full findings, read the most recent log file.\n\nSimplest approach: After\
      \ run_claude_task completes, read the most recent log file from .claude/plans/logs/\
      \ and parse its STDOUT section for the verdict and findings using parse_validation_verdict().\n\
      \nReference: docs/plans/2026-02-13-03-per-task-validation-pipeline-design.md\n"
    attempts: 1
    last_attempt: '2026-02-13T23:30:11.841774'
    completed_at: '2026-02-13T23:33:38.672651'
    result_message: Added run_validation() function after parse_validation_verdict()
      in plan-orchestrator.py. The function determines the executing agent, checks
      if validation applies for that agent type, and iterates over configured validators.
      For each validator it builds a prompt via build_validation_prompt(), executes
      via run_claude_task(), reads the log output via get_most_recent_log_file()/read_log_stdout(),
      and parses the verdict with parse_validation_verdict(). Short-circuits on FAIL.
      Also added helper functions get_most_recent_log_file() and read_log_stdout(),
      plus manifest constants (TASK_LOG_DIR, LOG_STDOUT_SECTION, LOG_STDERR_SECTION,
      FALLBACK_AGENT_NAME, VALIDATION_LOG_PREFIX, DRY_RUN_PROMPT_PREVIEW_LENGTH).
      Refactored run_claude_task() to use shared TASK_LOG_DIR constant. Syntax verified
      clean.
- id: phase-4
  name: Phase 4 - Orchestrator Integration
  status: completed
  tasks:
  - id: '4.1'
    name: Add validation_findings support to build_claude_prompt
    agent: coder
    status: completed
    depends_on:
    - '3.3'
    description: "In scripts/plan-orchestrator.py, modify build_claude_prompt() to\
      \ prepend validation findings when a task is being retried after validation\
      \ failure.\n\nIn build_claude_prompt() (starts at line 1179), after the agent_content\
      \ block is built (around line 1213), add:\n\n# Prepend validation findings from\
      \ previous failed validation\nvalidation_findings = task.get(\"validation_findings\"\
      , \"\")\nvalidation_header = \"\"\nif validation_findings:\n    validation_header\
      \ = f\"\"\"## PREVIOUS VALIDATION FAILED\n\nThe previous attempt at this task\
      \ was completed but failed validation.\nYou must address these findings:\n\n\
      {validation_findings}\n\n---\n\n\"\"\"\n\nThen in the return f-string (line\
      \ 1270), insert validation_header between agent_content and subagent_header:\n\
      \nreturn f\"\"\"{agent_content}{validation_header}{subagent_header}Run task\
      \ ...\n\nIMPORTANT: Do NOT change any other part of the prompt template. Only\
      \ add the validation_header injection.\n\nReference: docs/plans/2026-02-13-03-per-task-validation-pipeline-design.md\n"
    attempts: 1
    last_attempt: '2026-02-13T23:33:40.737512'
    completed_at: '2026-02-13T23:34:58.826996'
    result_message: Added validation_findings support to build_claude_prompt(). When
      task['validation_findings'] is present (set by the orchestrator after a validation
      FAIL), a header block is prepended between agent_content and subagent_header
      instructing the coder to address the findings. No other parts of the prompt
      template were changed.
  - id: '4.2'
    name: Integrate validation into sequential task execution
    agent: coder
    status: completed
    depends_on:
    - '4.1'
    description: "In scripts/plan-orchestrator.py, modify run_orchestrator() to run\
      \ validation after successful sequential task execution.\n\nIn run_orchestrator(),\
      \ parse the validation config once at startup. After the line that initializes\
      \ circuit_breaker (around line 1738), add:\n\n\n    validation_config = parse_validation_config(plan)\n\
      \n    if validation_config.enabled:\n        print(f\"Validation: enabled (run_after={validation_config.run_after},\
      \ \"\n              f\"validators={validation_config.validators})\")\n\n\nThen\
      \ in the sequential execution block, after the task_result.success check (around\
      \ line 2082), wrap the existing success handling in a validation check:\n\n\
      BEFORE (existing code):\n\n    if task_result.success:\n        task[\"status\"\
      ] = \"completed\"\n        task[\"completed_at\"] = datetime.now().isoformat()\n\
      \        task[\"result_message\"] = task_result.message\n        tasks_completed\
      \ += 1\n        circuit_breaker.record_success()\n        ...\n\n\nAFTER (with\
      \ validation):\n\n    if task_result.success:\n        # Run validation if enabled\n\
      \        if validation_config.enabled:\n            validation_attempts = task.get(\"\
      validation_attempts\", 0)\n            if validation_attempts < validation_config.max_validation_attempts:\n\
      \                verdict = run_validation(\n                    task, section,\
      \ task_result, validation_config, dry_run\n                )\n\n           \
      \     if verdict.verdict == \"FAIL\":\n                    print(f\"[VALIDATION]\
      \ FAIL - {len(verdict.findings)} findings\")\n                    for finding\
      \ in verdict.findings:\n                        print(f\"  {finding}\")\n  \
      \                  task[\"status\"] = \"pending\"\n                    task[\"\
      validation_findings\"] = \"\\n\".join(verdict.findings)\n                  \
      \  task[\"validation_attempts\"] = validation_attempts + 1\n               \
      \     if not dry_run:\n                        save_plan(plan_path, plan)\n\
      \                    continue  # Retry the task\n\n                elif verdict.verdict\
      \ == \"WARN\":\n                    print(f\"[VALIDATION] WARN - {len(verdict.findings)}\
      \ findings (proceeding)\")\n                    for finding in verdict.findings:\n\
      \                        print(f\"  {finding}\")\n                    # Fall\
      \ through to normal completion\n\n                else:\n                  \
      \  print(f\"[VALIDATION] PASS\")\n\n        # Original completion logic (unchanged)\n\
      \        task[\"status\"] = \"completed\"\n        task[\"completed_at\"] =\
      \ datetime.now().isoformat()\n        task[\"result_message\"] = task_result.message\n\
      \        tasks_completed += 1\n        circuit_breaker.record_success()\n  \
      \      ...\n\n\nIMPORTANT: The existing success/failure handling code must remain\
      \ intact. The validation check wraps around the success path only. The FAIL\
      \ branch uses 'continue' to skip marking the task as completed and instead retry\
      \ it on the next loop iteration.\n\nReference: docs/plans/2026-02-13-03-per-task-validation-pipeline-design.md\n"
    attempts: 1
    last_attempt: '2026-02-13T23:35:00.895104'
    completed_at: '2026-02-13T23:37:43.743531'
    result_message: 'Integrated validation into sequential task execution in run_orchestrator().
      Added validation_config parsing at startup (after circuit_breaker init) with
      status print when enabled. Added validation check wrapping the success path:
      on FAIL, resets task to pending with validation_findings and continues loop
      for retry; on WARN, logs findings and falls through to completion; on PASS,
      proceeds normally. Original completion logic preserved unchanged.'
  - id: '4.3'
    name: Integrate validation into parallel task execution
    agent: coder
    status: completed
    depends_on:
    - '4.2'
    description: "In scripts/plan-orchestrator.py, modify the parallel execution block\
      \ in run_orchestrator() to run validation on successful parallel tasks.\n\n\
      In the parallel execution block, after results are collected and before task\
      \ statuses are updated (around line 1933), add sequential validation for each\
      \ successful task:\n\nAfter the rate limit check block and before the \"Update\
      \ task statuses in plan\" comment, add:\n\n\n    # Validate successful parallel\
      \ tasks sequentially\n    if validation_config.enabled:\n        for section,\
      \ task, _ in parallel_tasks:\n            task_id = task.get(\"id\")\n     \
      \       task_result = results.get(task_id)\n            if task_result and task_result.success:\n\
      \                validation_attempts = task.get(\"validation_attempts\", 0)\n\
      \                if validation_attempts < validation_config.max_validation_attempts:\n\
      \                    verdict = run_validation(\n                        task,\
      \ section, task_result, validation_config, dry_run\n                    )\n\
      \                    if verdict.verdict == \"FAIL\":\n                     \
      \   print(f\"  [{task_id}] VALIDATION FAIL\")\n                        task_result\
      \ = TaskResult(\n                            success=False,\n              \
      \              message=f\"Validation failed: {', '.join(verdict.findings[:3])}\"\
      ,\n                            duration_seconds=task_result.duration_seconds\n\
      \                        )\n                        results[task_id] = task_result\n\
      \                        task[\"validation_findings\"] = \"\\n\".join(verdict.findings)\n\
      \                        task[\"validation_attempts\"] = validation_attempts\
      \ + 1\n                    elif verdict.verdict == \"WARN\":\n             \
      \           print(f\"  [{task_id}] VALIDATION WARN\")\n                    else:\n\
      \                        print(f\"  [{task_id}] VALIDATION PASS\")\n\n\nThis\
      \ replaces successful TaskResults with failed ones when validation fails, which\
      \ causes the existing retry logic to handle them correctly.\n\nIMPORTANT: Validators\
      \ run sequentially, not in parallel, to avoid conflicting build/test state.\n\
      \nReference: docs/plans/2026-02-13-03-per-task-validation-pipeline-design.md\n"
    attempts: 1
    last_attempt: '2026-02-13T23:37:45.811229'
    completed_at: '2026-02-13T23:39:50.969928'
    result_message: Integrated validation into parallel task execution in run_orchestrator().
      Added sequential validation block after rate limit checks and before task status
      updates. On FAIL, replaces TaskResult with failed version (success=False) so
      existing retry logic handles it. On WARN, logs findings. On PASS, proceeds normally.
      Validators run sequentially to avoid conflicting build/test state.
- id: phase-5
  name: Phase 5 - Configuration and Documentation
  status: in_progress
  tasks:
  - id: '5.1'
    name: Update orchestrator-config.yaml with validation fields
    agent: coder
    status: completed
    depends_on:
    - '4.3'
    description: "Update .claude/orchestrator-config.yaml to document the new validation\
      \ configuration fields.\n\nAdd a commented-out section after the agents_dir\
      \ entry:\n\n\n    # Per-task validation configuration.\n\n    # When enabled,\
      \ the orchestrator spawns a validator agent after each\n\n    # implementation\
      \ task to independently verify the result.\n\n    # validation:\n\n    #   enabled:\
      \ false\n\n    #   run_after:\n\n    #     - coder           # Agent types that\
      \ trigger validation\n\n    #   validators:\n\n    #     - validator       #\
      \ Validator agent names to run\n\n    #   max_validation_attempts: 1  # Max\
      \ validation retries per task\n\n\nReference: docs/plans/2026-02-13-03-per-task-validation-pipeline-design.md\n"
    attempts: 1
    last_attempt: '2026-02-13T23:39:53.041416'
    completed_at: '2026-02-13T23:40:48.098427'
    result_message: Added commented-out validation configuration section to .claude/orchestrator-config.yaml
      after agents_dir. Documents enabled, run_after, validators, and max_validation_attempts
      fields with inline comments explaining each option.
  - id: '5.2'
    name: Update sample-plan.yaml with validation example
    agent: coder
    status: pending
    depends_on:
    - '5.1'
    description: "Update .claude/plans/sample-plan.yaml to include an example of the\
      \ validation configuration in the meta section.\n\nAdd these lines after the\
      \ notification_email field in the meta section:\n\n\n  # Per-task validation\
      \ (optional, disabled by default)\n\n  # validation:\n\n  #   enabled: true\n\
      \n  #   run_after:\n\n  #     - coder\n\n  #   validators:\n\n  #     - validator\n\
      \n  #   max_validation_attempts: 1\n\n\nReference: docs/plans/2026-02-13-03-per-task-validation-pipeline-design.md\n"
  - id: '5.3'
    name: Update PLAN_CREATION_PROMPT_TEMPLATE with validation guidance
    agent: coder
    status: pending
    depends_on:
    - '5.2'
    description: "In scripts/auto-pipeline.py, update PLAN_CREATION_PROMPT_TEMPLATE\
      \ to inform the plan creator about validation configuration.\n\nAdd a new section\
      \ after the \"## Agent Selection\" section (which was added by Feature 02).\
      \ Insert this text block before the \"## Important\" section:\n\n## Validation\
      \ (Optional)\n\nPlans can enable per-task validation by adding a validation\
      \ block to the meta section. When enabled, a validator agent runs after each\
      \ coder task to independently verify the result.\n\nExample meta configuration:\n\
      \n  meta:\n\n    validation:\n\n      enabled: true\n\n      run_after:\n\n\
      \        - coder\n\n      validators:\n\n        - validator\n\n      max_validation_attempts:\
      \ 1\n\n\nThe validator produces PASS/WARN/FAIL verdicts:\n- PASS: task proceeds\
      \ normally\n- WARN: task completes but warnings are logged\n- FAIL: task is\
      \ retried with validation findings prepended to the prompt\n\nFor defect fixes,\
      \ use the issue-verifier validator instead of or in addition to the default\
      \ validator. This validator reads the original defect file and checks whether\
      \ reported symptoms are resolved.\n\nReference: docs/plans/2026-02-13-03-per-task-validation-pipeline-design.md\n"
- id: phase-6
  name: Phase 6 - Verification
  status: pending
  tasks:
  - id: '6.1'
    name: Verify agent files and validation logic
    agent: code-reviewer
    status: pending
    depends_on:
    - '5.3'
    max_attempts: 5
    description: "Run verification checks to confirm the per-task validation pipeline\
      \ works correctly.\n\n1. Verify agent files exist and have valid frontmatter:\n\
      \n   python3 -c \"\n   import yaml, os\n   for agent in ['validator', 'issue-verifier']:\n\
      \       path = f'.claude/agents/{agent}.md'\n       assert os.path.exists(path),\
      \ f'Missing: {path}'\n       content = open(path).read()\n       parts = content.split('---',\
      \ 2)\n       assert len(parts) >= 3, f'Invalid frontmatter in {path}'\n    \
      \   meta = yaml.safe_load(parts[1])\n       assert 'name' in meta, f'Missing\
      \ name in {path}'\n       assert 'description' in meta, f'Missing description\
      \ in {path}'\n       print(f'OK: {path} - name={meta[\\\"name\\\"]}')\n   print('All\
      \ agent files valid')\n   \"\n\n\n2. Verify no Python syntax errors:\n\n   python3\
      \ -c \"import py_compile; py_compile.compile('scripts/plan-orchestrator.py',\
      \ doraise=True); print('plan-orchestrator.py: syntax OK')\"\n\n   python3 -c\
      \ \"import py_compile; py_compile.compile('scripts/auto-pipeline.py', doraise=True);\
      \ print('auto-pipeline.py: syntax OK')\"\n\n\n3. Verify orchestrator dry-run\
      \ still works:\n\n   python scripts/plan-orchestrator.py --plan .claude/plans/sample-plan.yaml\
      \ --dry-run\n\n\n4. Verify ValidationConfig parsing:\n\n   python3 -c \"\n \
      \  import sys, os\n   # Test parsing logic manually\n   plan_with_validation\
      \ = {\n       'meta': {\n           'validation': {\n               'enabled':\
      \ True,\n               'run_after': ['coder'],\n               'validators':\
      \ ['validator'],\n               'max_validation_attempts': 2\n           }\n\
      \       }\n   }\n   plan_without_validation = {'meta': {}}\n\n   # Simulate\
      \ parse_validation_config\n   def parse_vc(plan):\n       val = plan.get('meta',\
      \ {}).get('validation', {})\n       if not isinstance(val, dict) or not val:\n\
      \           return {'enabled': False}\n       return {\n           'enabled':\
      \ val.get('enabled', False),\n           'run_after': val.get('run_after', ['coder']),\n\
      \           'validators': val.get('validators', ['validator']),\n          \
      \ 'max_validation_attempts': val.get('max_validation_attempts', 1),\n      \
      \ }\n\n   c1 = parse_vc(plan_with_validation)\n   assert c1['enabled'] == True\n\
      \   assert c1['run_after'] == ['coder']\n   assert c1['max_validation_attempts']\
      \ == 2\n\n   c2 = parse_vc(plan_without_validation)\n   assert c2['enabled']\
      \ == False\n\n   print('All ValidationConfig parsing tests passed')\n   \"\n\
      \n\n5. Verify verdict parsing regex:\n\n   python3 -c \"\n   import re\n   VERDICT_PATTERN\
      \ = re.compile(r'\\*\\*Verdict:\\s*(PASS|WARN|FAIL)\\*\\*', re.IGNORECASE)\n\
      \   FINDING_PATTERN = re.compile(r'- \\[(PASS|WARN|FAIL)\\]\\s+(.+)', re.IGNORECASE)\n\
      \n   test_output = '''\n   **Verdict: FAIL**\n\n   **Findings:**\n   - [FAIL]\
      \ Build command failed with exit code 1\n   - [WARN] Missing docstring in new\
      \ function\n   - [PASS] File headers present\n   '''\n\n   verdict_match = VERDICT_PATTERN.search(test_output)\n\
      \   assert verdict_match, 'No verdict found'\n   assert verdict_match.group(1)\
      \ == 'FAIL'\n\n   findings = FINDING_PATTERN.findall(test_output)\n   assert\
      \ len(findings) == 3\n   assert findings[0] == ('FAIL', 'Build command failed\
      \ with exit code 1')\n   assert findings[1] == ('WARN', 'Missing docstring in\
      \ new function')\n   assert findings[2] == ('PASS', 'File headers present')\n\
      \n   print('All verdict parsing tests passed')\n   \"\n\n\n6. Verify the build\
      \ passes:\n\n   pnpm run build\n\n\n7. Verify tests pass:\n\n   pnpm test\n\n\
      \nIf any check fails, report the failure with specific details.\n"
