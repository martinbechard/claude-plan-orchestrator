meta:
  name: Per-Task Validation Pipeline
  description: 'Extend the item-level verify-then-fix cycle to work at the individual
    task level within the orchestrator. After each implementation task completes,
    optionally spawn a validator agent to independently verify the result. If
    validation fails, retry the task with findings appended to the prompt.

    '
  plan_doc: docs/plans/2026-02-13-03-per-task-validation-pipeline-design.md
  created: '2026-02-13'
  max_attempts_default: 3
sections:
- id: phase-1
  name: Phase 1 - Agent Definitions
  status: pending
  tasks:
  - id: '1.1'
    name: Create validator agent definition
    agent: coder
    status: pending
    description: "Create the file .claude/agents/validator.md with YAML frontmatter\
      \ and markdown body.\n\nThis agent is a post-task verification coordinator.\
      \ It receives context about a\ncompleted task and independently verifies the\
      \ result by running build/test commands\nand checking that task requirements\
      \ are met.\n\nFrontmatter fields:\n  name: validator\n  description: \"Post-task\
      \ verification coordinator. Runs build, tests, and\n    requirement checks after\
      \ implementation tasks. Produces structured\n    PASS/WARN/FAIL verdicts. Read-only\
      \ - does not modify code files.\"\n  tools: [Read, Grep, Glob, Bash]\n  model:\
      \ sonnet\n\nBody content should include these sections:\n\n1. Role: You are\
      \ a post-task validator. Your job is to independently verify\n   that a completed\
      \ task's output is correct. You are READ-ONLY and must NOT\n   modify any code\
      \ files. You only observe, run verification commands, and report.\n\n2. Validation\
      \ Checklist:\n   - Build passes: Run the build command and check exit code\n\
      \   - Tests pass: Run the test command and check exit code\n   - Requirements\
      \ met: Check each requirement from the task description\n   - No regressions:\
      \ Verify no existing functionality was broken\n\n3. Output Format:\n   Produce\
      \ your verdict in this EXACT format (the orchestrator parses this):\n\n   **Verdict:\
      \ PASS** or **Verdict: WARN** or **Verdict: FAIL**\n\n   **Findings:**\n   -\
      \ [PASS] Build passes\n   - [PASS] Unit tests pass\n   - [WARN|FAIL] Description\
      \ with file:line references\n\n   Rules for verdict:\n   - PASS: All checks\
      \ passed\n   - WARN: Minor issues that don't affect functionality (style, docs)\n\
      \   - FAIL: Build fails, tests fail, or core requirements not met\n\n4. Constraints:\n\
      \   - Do NOT modify any code files\n   - Do NOT attempt to fix issues - only\
      \ report them\n   - Be specific about failures: include file paths, line numbers,\
      \ error messages\n   - If you cannot determine whether a requirement is met,\
      \ mark it as WARN\n\nReference: docs/plans/2026-02-13-03-per-task-validation-pipeline-design.md\n"
  - id: '1.2'
    name: Create issue-verifier agent definition
    agent: coder
    status: pending
    description: "Create the file .claude/agents/issue-verifier.md with YAML frontmatter\
      \ and markdown body.\n\nThis agent specializes in verifying defect fixes. It\
      \ reads the original defect\nfile, checks whether the reported symptoms are resolved,\
      \ and runs targeted tests.\n\nFrontmatter fields:\n  name: issue-verifier\n\
      \  description: \"Defect fix verification specialist. Reads the original defect\n\
      \    file, checks whether reported symptoms are resolved, runs targeted tests,\n\
      \    and produces PASS/FAIL verdict with specific evidence. Read-only.\"\n  tools:\
      \ [Read, Grep, Glob, Bash]\n  model: sonnet\n\nBody content should include these\
      \ sections:\n\n1. Role: You are a defect fix verifier. Your job is to check\
      \ whether a specific\n   defect has been resolved by the implementation task.\
      \ You are READ-ONLY and\n   must NOT modify any code files.\n\n2. Verification\
      \ Process:\n   - Read the defect file specified in the task context\n   - Identify\
      \ the \"Expected Behavior\" and \"Actual Behavior\" sections\n   - Read the \"\
      Fix Required\" section for specific commands or checks\n   - Run each verification\
      \ command\n   - Check that the reported symptom is actually gone\n\n3. Output\
      \ Format (same as validator.md):\n   **Verdict: PASS** or **Verdict: FAIL**\n\
      \n   **Findings:**\n   - [PASS|FAIL] Build passes\n   - [PASS|FAIL] Unit tests\
      \ pass\n   - [PASS|FAIL] (each specific symptom check from the defect)\n\n \
      \  **Evidence:**\n   - Command output showing the symptom is resolved (or still\
      \ present)\n\n4. Constraints:\n   - Do NOT modify any code files\n   - Do NOT\
      \ fix anything\n   - Do NOT change the defect's Status line\n   - ONLY read,\
      \ run verification commands, and report\n\nReference: docs/plans/2026-02-13-03-per-task-validation-pipeline-design.md\n"
    depends_on:
    - '1.1'
- id: phase-2
  name: Phase 2 - Dataclasses and Parsing
  status: pending
  tasks:
  - id: '2.1'
    name: Add ValidationConfig dataclass
    agent: coder
    status: pending
    description: "In scripts/plan-orchestrator.py, add a ValidationConfig dataclass\
      \ after the\nCircuitBreaker class (around line 330).\n\nFirst, read the design\
      \ document: docs/plans/2026-02-13-03-per-task-validation-pipeline-design.md\n\
      \nAdd the following dataclass:\n\n    @dataclass\n    class ValidationConfig:\n\
      \        \"\"\"Configuration for per-task validation pipeline.\"\"\"\n       \
      \ enabled: bool = False\n        run_after: list[str] = field(default_factory=lambda:\
      \ [\"coder\"])\n        validators: list[str] = field(default_factory=lambda:\
      \ [\"validator\"])\n        max_validation_attempts: int = 1\n\nNote: You will\
      \ need to import 'field' from dataclasses. Check the existing\nimport line:\n\
      \    from dataclasses import dataclass\nChange it to:\n    from dataclasses import\
      \ dataclass, field\n\nVerify: python3 -c \"import py_compile; py_compile.compile('scripts/plan-orchestrator.py',\
      \ doraise=True); print('OK')\"\n\nReference: docs/plans/2026-02-13-03-per-task-validation-pipeline-design.md\n"
    depends_on:
    - '1.2'
  - id: '2.2'
    name: Add ValidationVerdict dataclass
    agent: coder
    status: pending
    description: "In scripts/plan-orchestrator.py, add a ValidationVerdict dataclass\
      \ after\nthe ValidationConfig class.\n\n    @dataclass\n    class ValidationVerdict:\n\
      \        \"\"\"Result of a validation pass on a completed task.\"\"\"\n      \
      \  verdict: str  # \"PASS\", \"WARN\", or \"FAIL\"\n        findings: list[str]\n\
      \        raw_output: str\n\nAlso add a module-level constant for parsing the\
      \ verdict from validator output:\n\n    # Regex to extract validation verdict\
      \ from validator agent output\n    VERDICT_PATTERN = re.compile(\n        r\"\\\
      *\\*Verdict:\\s*(PASS|WARN|FAIL)\\*\\*\",\n        re.IGNORECASE\n    )\n   \
      \ FINDING_PATTERN = re.compile(\n        r\"^\\s*-\\s*\\[(PASS|WARN|FAIL)\\]\\\
      s+(.+)$\",\n        re.MULTILINE | re.IGNORECASE\n    )\n\nPlace the constants\
      \ near the other regex patterns (RATE_LIMIT_PATTERN area,\naround line 168).\n\
      \nVerify: python3 -c \"import py_compile; py_compile.compile('scripts/plan-orchestrator.py',\
      \ doraise=True); print('OK')\"\n\nReference: docs/plans/2026-02-13-03-per-task-validation-pipeline-design.md\n"
    depends_on:
    - '2.1'
  - id: '2.3'
    name: Add parse_validation_config helper function
    agent: coder
    status: pending
    description: "In scripts/plan-orchestrator.py, add a function after the ValidationConfig\
      \ class:\n\n    def parse_validation_config(plan: dict) -> ValidationConfig:\n\
      \        \"\"\"Parse validation configuration from plan YAML meta.validation\
      \ block.\n\n        Returns ValidationConfig with defaults if no validation section\
      \ exists.\n        \"\"\"\n        validation_meta = plan.get(\"meta\", {}).get(\"\
      validation\", {})\n        if not validation_meta:\n            return ValidationConfig()\n\
      \n        return ValidationConfig(\n            enabled=validation_meta.get(\"\
      enabled\", False),\n            run_after=validation_meta.get(\"run_after\",\
      \ [\"coder\"]),\n            validators=validation_meta.get(\"validators\", [\"\
      validator\"]),\n            max_validation_attempts=validation_meta.get(\"max_validation_attempts\"\
      , 1),\n        )\n\nVerify: python3 -c \"import py_compile; py_compile.compile('scripts/plan-orchestrator.py',\
      \ doraise=True); print('OK')\"\n\nReference: docs/plans/2026-02-13-03-per-task-validation-pipeline-design.md\n"
    depends_on:
    - '2.2'
  - id: '2.4'
    name: Add parse_validation_verdict helper function
    agent: coder
    status: pending
    description: "In scripts/plan-orchestrator.py, add a function after parse_validation_config:\n\
      \n    def parse_validation_verdict(output: str) -> ValidationVerdict:\n     \
      \   \"\"\"Parse a validation verdict from validator agent output.\n\n        Extracts\
      \ the PASS/WARN/FAIL verdict and individual findings from\n        the validator's\
      \ structured output.\n        \"\"\"\n        # Extract verdict\n        verdict_match\
      \ = VERDICT_PATTERN.search(output)\n        verdict = verdict_match.group(1).upper()\
      \ if verdict_match else \"FAIL\"\n\n        # Extract findings\n        findings\
      \ = []\n        for match in FINDING_PATTERN.finditer(output):\n            severity\
      \ = match.group(1).upper()\n            description = match.group(2).strip()\n\
      \            findings.append(f\"[{severity}] {description}\")\n\n        # If\
      \ no structured findings found but verdict exists, use raw output\n        if\
      \ not findings and verdict == \"FAIL\":\n            findings.append(\"[FAIL]\
      \ Validation failed (no structured findings extracted)\")\n\n        return ValidationVerdict(\n\
      \            verdict=verdict,\n            findings=findings,\n            raw_output=output,\n\
      \        )\n\nVerify: python3 -c \"import py_compile; py_compile.compile('scripts/plan-orchestrator.py',\
      \ doraise=True); print('OK')\"\n\nReference: docs/plans/2026-02-13-03-per-task-validation-pipeline-design.md\n"
    depends_on:
    - '2.3'
- id: phase-3
  name: Phase 3 - Validation Prompt and Execution
  status: pending
  tasks:
  - id: '3.1'
    name: Add VALIDATION_PROMPT_TEMPLATE constant
    agent: coder
    status: pending
    description: "In scripts/plan-orchestrator.py, add a module-level prompt template\
      \ constant\nnear the other template strings (or near the VERDICT_PATTERN constants).\n\
      \n    VALIDATION_PROMPT_TEMPLATE = \"\"\"You are validating the results of task\
      \ {task_id}: {task_name}\n\n    ## Original Task Description\n    {task_description}\n\
      \n    ## Task Result\n    Status: {result_status}\n    Message: {result_message}\n\
      \n    ## Validation Checks\n    1. Run: {build_command}\n    2. Run: {test_command}\n\
      \    3. Verify the task description requirements are met\n    4. Check for regressions\
      \ in related code\n\n    ## Output Format\n    Produce your verdict in this EXACT\
      \ format (the orchestrator parses this):\n\n    **Verdict: PASS** or **Verdict:\
      \ WARN** or **Verdict: FAIL**\n\n    **Findings:**\n    - [PASS|WARN|FAIL] Description\
      \ with file:line references\n\n    ## CRITICAL RULES\n    - Do NOT modify any\
      \ code files\n    - Do NOT fix anything - only report findings\n    - Be specific:\
      \ include file paths, line numbers, error messages\n    - Run the actual commands\
      \ and report their real output\n    \"\"\"\n\nIMPORTANT: The template uses leading\
      \ whitespace for readability in the source\ncode. Use textwrap.dedent() when\
      \ formatting the template, or format it without\nleading indentation. Use Python\
      \ string .format() placeholders.\n\nVerify: python3 -c \"import py_compile;\
      \ py_compile.compile('scripts/plan-orchestrator.py', doraise=True); print('OK')\"\
      \n\nReference: docs/plans/2026-02-13-03-per-task-validation-pipeline-design.md\n"
    depends_on:
    - '2.4'
  - id: '3.2'
    name: Add run_validation function
    agent: coder
    status: pending
    description: "In scripts/plan-orchestrator.py, add a function that runs the validation\
      \ pass:\n\n    def run_validation(\n        plan: dict,\n        section: dict,\n\
      \        task: dict,\n        task_result: TaskResult,\n        validation_config:\
      \ ValidationConfig,\n        plan_path: str,\n        dry_run: bool = False\n\
      \    ) -> ValidationVerdict:\n        \"\"\"Run post-task validation using the\
      \ configured validator agent.\n\n        Builds a validation prompt, spawns the\
      \ validator agent via\n        run_claude_task(), and parses the verdict from\
      \ the output.\n        \"\"\"\n        task_id = task.get(\"id\", \"unknown\"\
      )\n        task_name = task.get(\"name\", \"\")\n        task_description = task.get(\"\
      description\", \"\")\n\n        # Use the first configured validator\n       \
      \ validator_name = validation_config.validators[0] if validation_config.validators\
      \ else \"validator\"\n\n        # Load the validator agent definition\n      \
      \  agent_content = \"\"\n        agent_def = load_agent_definition(validator_name)\n\
      \        if agent_def:\n            agent_content = agent_def[\"content\"] + \"\
      \\n\\n---\\n\\n\"\n\n        # Build validation prompt\n        validation_prompt\
      \ = agent_content + VALIDATION_PROMPT_TEMPLATE.format(\n            task_id=task_id,\n\
      \            task_name=task_name,\n            task_description=task_description,\n\
      \            result_status=\"SUCCESS\" if task_result.success else \"FAILED\"\
      ,\n            result_message=task_result.message,\n            build_command=BUILD_COMMAND,\n\
      \            test_command=TEST_COMMAND,\n        )\n\n        print(f\"  [VALIDATION]\
      \ Running {validator_name} for task {task_id}...\")\n        verbose_log(f\"Validation\
      \ prompt length: {len(validation_prompt)} chars\", \"VALIDATE\")\n\n        #\
      \ Run the validator\n        val_result = run_claude_task(validation_prompt, dry_run=dry_run)\n\
      \n        if dry_run:\n            return ValidationVerdict(verdict=\"PASS\",\
      \ findings=[], raw_output=\"dry run\")\n\n        # Parse the verdict from the\
      \ validator output\n        verdict = parse_validation_verdict(val_result.message)\n\
      \n        print(f\"  [VALIDATION] Verdict: {verdict.verdict}\")\n        for\
      \ finding in verdict.findings:\n            print(f\"  [VALIDATION]   {finding}\"\
      )\n\n        return verdict\n\nVerify: python3 -c \"import py_compile; py_compile.compile('scripts/plan-orchestrator.py',\
      \ doraise=True); print('OK')\"\n\nReference: docs/plans/2026-02-13-03-per-task-validation-pipeline-design.md\n"
    depends_on:
    - '3.1'
- id: phase-4
  name: Phase 4 - Orchestrator Integration
  status: pending
  tasks:
  - id: '4.1'
    name: Add validation_findings support to build_claude_prompt
    agent: coder
    status: pending
    description: "In scripts/plan-orchestrator.py, modify build_claude_prompt() (starts\
      \ at\nline 1178) to check for and include validation findings.\n\nAfter the\
      \ agent_content block (around line 1210) and before the return\nstatement, add:\n\
      \n    # Prepend validation findings from a prior failed validation\n    validation_findings_text\
      \ = \"\"\n    if task.get(\"validation_findings\"):\n        validation_findings_text\
      \ = (\n            \"## PREVIOUS VALIDATION FAILED\\n\\n\"\n            \"The\
      \ previous attempt at this task was completed but failed independent\\n\"\n \
      \           \"validation. You MUST address these findings:\\n\\n\"\n          \
      \  f\"{task['validation_findings']}\\n\\n---\\n\\n\"\n        )\n\nThen update\
      \ the return f-string to include it. Find the return statement that\nstarts with:\n\
      \    return f\"\"\"{agent_content}{subagent_header}Run task {task['id']}...\n\
      \nChange it to:\n    return f\"\"\"{agent_content}{subagent_header}{validation_findings_text}Run\
      \ task {task['id']}...\n\nThis prepends the validation findings AFTER the agent\
      \ content and subagent\nheader but BEFORE the task details. The findings appear\
      \ as context for the\ntask executor to address.\n\nVerify: python3 -c \"import\
      \ py_compile; py_compile.compile('scripts/plan-orchestrator.py', doraise=True);\
      \ print('OK')\"\n\nReference: scripts/plan-orchestrator.py build_claude_prompt()\
      \ lines 1178-1260\nReference: docs/plans/2026-02-13-03-per-task-validation-pipeline-design.md\n"
    depends_on:
    - '3.2'
  - id: '4.2'
    name: Initialize ValidationConfig in run_orchestrator
    agent: coder
    status: pending
    description: "In scripts/plan-orchestrator.py, add ValidationConfig initialization\
      \ in\nrun_orchestrator() (starts at line 1699).\n\nAfter the circuit_breaker\
      \ initialization (around line 1728), add:\n\n    # Initialize validation config\n\
      \    validation_config = parse_validation_config(plan)\n    if validation_config.enabled:\n\
      \        print(f\"Validation: enabled (run_after={validation_config.run_after},\
      \ \"\n              f\"validators={validation_config.validators})\")\n    else:\n\
      \        verbose_log(\"Validation: disabled\", \"INIT\")\n\nThis parses the meta.validation\
      \ block from the plan YAML and makes it\navailable for the task execution loop.\n\
      \nVerify: python3 -c \"import py_compile; py_compile.compile('scripts/plan-orchestrator.py',\
      \ doraise=True); print('OK')\"\n\nReference: scripts/plan-orchestrator.py run_orchestrator()\
      \ lines 1699-1760\nReference: docs/plans/2026-02-13-03-per-task-validation-pipeline-design.md\n"
    depends_on:
    - '4.1'
  - id: '4.3'
    name: Add post-task validation in sequential execution
    agent: coder
    status: pending
    description: "In scripts/plan-orchestrator.py, add validation logic in the sequential\n\
      execution block. Find the section after task_result.success check (around\n\
      line 2066) where the task is marked completed.\n\nReplace the simple success\
      \ handling with validation-aware logic. The current\ncode is approximately:\n\
      \n    if task_result.success:\n        task[\"status\"] = \"completed\"\n    \
      \    task[\"completed_at\"] = datetime.now().isoformat()\n        task[\"result_message\"\
      ] = task_result.message\n        tasks_completed += 1\n        circuit_breaker.record_success()\n\
      \nWrap this in a validation check. The new code should be:\n\n    if task_result.success:\n\
      \        # Check if validation is needed for this task\n        agent_name = task.get(\"\
      agent\") or infer_agent_for_task(task)\n        should_validate = (\n       \
      \     validation_config.enabled\n            and agent_name in validation_config.run_after\n\
      \        )\n\n        validation_passed = True\n        if should_validate:\n\
      \            validation_attempts = task.get(\"validation_attempts\", 0)\n   \
      \         if validation_attempts < validation_config.max_validation_attempts:\n\
      \                verdict = run_validation(\n                    plan, section,\
      \ task, task_result,\n                    validation_config, plan_path, dry_run\n\
      \                )\n\n                if verdict.verdict == \"FAIL\":\n       \
      \             validation_passed = False\n                    task[\"status\"]\
      \ = \"pending\"  # Will retry\n                    task[\"validation_attempts\"\
      ] = validation_attempts + 1\n                    # Store findings for next attempt\n\
      \                    findings_text = \"\\n\".join(verdict.findings)\n        \
      \            task[\"validation_findings\"] = findings_text\n                 \
      \   print(f\"  [VALIDATION] Task {task_id} failed validation \"\n           \
      \               f\"(attempt {validation_attempts + 1}/\"\n                   \
      \           f\"{validation_config.max_validation_attempts})\")\n             \
      \       print(f\"  [VALIDATION] Will retry with findings prepended\")\n\n   \
      \             elif verdict.verdict == \"WARN\":\n                    # Warnings\
      \ don't block completion\n                    task[\"validation_warnings\"] =\
      \ \"\\n\".join(verdict.findings)\n                    print(f\"  [VALIDATION]\
      \ Task {task_id} passed with warnings\")\n            else:\n                #\
      \ Max validation attempts reached, complete anyway\n                print(f\"\
      \  [VALIDATION] Max validation attempts reached for {task_id}\")\n\n        if\
      \ validation_passed:\n            task[\"status\"] = \"completed\"\n         \
      \   task[\"completed_at\"] = datetime.now().isoformat()\n            task[\"\
      result_message\"] = task_result.message\n            # Clear validation state\
      \ on completion\n            task.pop(\"validation_findings\", None)\n       \
      \     task.pop(\"validation_attempts\", None)\n            tasks_completed +=\
      \ 1\n            circuit_breaker.record_success()\n\nMake sure the existing section-completion\
      \ check and notification logic\n(around line 2073-2081) still runs only when\
      \ validation_passed is True.\n\nVerify: python3 -c \"import py_compile; py_compile.compile('scripts/plan-orchestrator.py',\
      \ doraise=True); print('OK')\"\n\nReference: scripts/plan-orchestrator.py lines\
      \ 2049-2082\nReference: docs/plans/2026-02-13-03-per-task-validation-pipeline-design.md\n"
    depends_on:
    - '4.2'
  - id: '4.4'
    name: Add post-task validation in parallel execution
    agent: coder
    status: pending
    description: "In scripts/plan-orchestrator.py, add validation logic in the parallel\n\
      execution block. Find the section after parallel results are collected\n(around\
      \ line 1830-1835) where task statuses are updated.\n\nAfter the artifact copy\
      \ section and before the plan save, add sequential\nvalidation for each successful\
      \ parallel task:\n\n    # Validate successful parallel tasks (sequentially to\
      \ avoid conflicts)\n    if validation_config.enabled:\n        for section_p,\
      \ task_p, _ in parallel_tasks:\n            task_id_p = task_p.get(\"id\")\n\
      \            task_result_p = results.get(task_id_p)\n            if not task_result_p\
      \ or not task_result_p.success:\n                continue\n\n            agent_name_p\
      \ = task_p.get(\"agent\") or infer_agent_for_task(task_p)\n            if agent_name_p\
      \ not in validation_config.run_after:\n                continue\n\n          \
      \  validation_attempts = task_p.get(\"validation_attempts\", 0)\n           \
      \ if validation_attempts >= validation_config.max_validation_attempts:\n    \
      \            print(f\"  [{task_id_p}] Max validation attempts reached\")\n  \
      \              continue\n\n            verdict = run_validation(\n           \
      \     plan, section_p, task_p, task_result_p,\n                validation_config,\
      \ plan_path, dry_run\n            )\n\n            if verdict.verdict == \"FAIL\"\
      :\n                task_p[\"status\"] = \"pending\"  # Will retry in next iteration\n\
      \                task_p[\"validation_attempts\"] = validation_attempts + 1\n\
      \                findings_text = \"\\n\".join(verdict.findings)\n            \
      \    task_p[\"validation_findings\"] = findings_text\n                print(f\"\
      \  [{task_id_p}] VALIDATION FAILED - will retry\")\n            elif verdict.verdict\
      \ == \"WARN\":\n                task_p[\"validation_warnings\"] = \"\\n\".join(verdict.findings)\n\
      \                print(f\"  [{task_id_p}] Validation passed with warnings\")\n\
      \n    NOTE: In the parallel block, the task status update logic happens after\
      \ the\n    results are collected (search for where task[\"status\"] = \"completed\"\
      \n    is set based on results). The validation should run BEFORE the status\n\
      \    is finalized. Study the existing parallel result handling to find the\n \
      \   exact insertion point.\n\nVerify: python3 -c \"import py_compile; py_compile.compile('scripts/plan-orchestrator.py',\
      \ doraise=True); print('OK')\"\n\nReference: scripts/plan-orchestrator.py parallel\
      \ execution block lines 1780-1950\nReference: docs/plans/2026-02-13-03-per-task-validation-pipeline-design.md\n"
    depends_on:
    - '4.3'
- id: phase-5
  name: Phase 5 - Unit Tests
  status: pending
  tasks:
  - id: '5.1'
    name: Create unit tests for ValidationConfig and parsing
    agent: coder
    status: pending
    description: "Create tests/test_validation_pipeline.py with unit tests for ValidationConfig\n\
      and parse_validation_config.\n\nSince plan-orchestrator.py is a script with top-level\
      \ code that may be\ndifficult to import, replicate the ValidationConfig class\
      \ and\nparse_validation_config function in the test file (same approach used\
      \ in\ntest_budget_guard.py if it exists). Add a comment noting they are copied\n\
      for test isolation.\n\nTest cases for ValidationConfig:\n\n1. test_default_values:\n\
      \   - ValidationConfig() should have enabled=False, run_after=[\"coder\"],\n\
      \     validators=[\"validator\"], max_validation_attempts=1\n\n2. test_custom_values:\n\
      \   - ValidationConfig(enabled=True, run_after=[\"coder\", \"custom\"],\n   \
      \   validators=[\"validator\", \"issue-verifier\"], max_validation_attempts=3)\n\
      \   - Verify all fields set correctly\n\nTest cases for parse_validation_config:\n\
      \n3. test_parse_empty_plan:\n   - plan = {\"meta\": {}}\n   - Result should have\
      \ all defaults, enabled=False\n\n4. test_parse_enabled_validation:\n   - plan\
      \ = {\"meta\": {\"validation\": {\"enabled\": True}}}\n   - Result: enabled=True,\
      \ run_after=[\"coder\"], validators=[\"validator\"]\n\n5. test_parse_full_config:\n\
      \   - plan = {\"meta\": {\"validation\": {\n       \"enabled\": True,\n     \
      \  \"run_after\": [\"coder\", \"custom\"],\n       \"validators\": [\"validator\"\
      , \"issue-verifier\"],\n       \"max_validation_attempts\": 2\n     }}}\n   -\
      \ Verify all fields match\n\n6. test_parse_no_meta:\n   - plan = {}\n   - Result\
      \ should have all defaults\n\nRun: python -m pytest tests/test_validation_pipeline.py\
      \ -v -k \"Config\"\nFix any failures before marking complete.\n\nReference: docs/plans/2026-02-13-03-per-task-validation-pipeline-design.md\n"
    depends_on:
    - '2.4'
  - id: '5.2'
    name: Create unit tests for ValidationVerdict and parsing
    agent: coder
    status: pending
    description: "Add test cases for ValidationVerdict and parse_validation_verdict\
      \ to\ntests/test_validation_pipeline.py.\n\nReplicate the ValidationVerdict class,\
      \ VERDICT_PATTERN, FINDING_PATTERN,\nand parse_validation_verdict function in\
      \ the test file.\n\nTest cases:\n\n1. test_parse_pass_verdict:\n   - Input: \"\
      **Verdict: PASS**\\n\\n**Findings:**\\n- [PASS] Build passes\\n- [PASS] Tests\
      \ pass\"\n   - Result: verdict=\"PASS\", findings=[\"[PASS] Build passes\", \"\
      [PASS] Tests pass\"]\n\n2. test_parse_fail_verdict:\n   - Input: \"**Verdict:\
      \ FAIL**\\n\\n**Findings:**\\n- [PASS] Build passes\\n- [FAIL] Test test_foo\
      \ fails at file.py:42\"\n   - Result: verdict=\"FAIL\", 2 findings, second contains\
      \ \"FAIL\"\n\n3. test_parse_warn_verdict:\n   - Input: \"**Verdict: WARN**\\\
      n\\n**Findings:**\\n- [PASS] Build passes\\n- [WARN] Missing docstring in module.py:10\"\
      \n   - Result: verdict=\"WARN\", 2 findings\n\n4. test_parse_no_verdict_defaults_to_fail:\n\
      \   - Input: \"Some output without a verdict line\"\n   - Result: verdict=\"\
      FAIL\", findings has one entry about \"no structured findings\"\n\n5. test_parse_case_insensitive:\n\
      \   - Input: \"**Verdict: pass**\" (lowercase)\n   - Result: verdict=\"PASS\"\
      \n\n6. test_parse_verdict_with_surrounding_text:\n   - Input: \"Preamble text...\\\
      n\\n**Verdict: FAIL**\\n\\nMore text\\n\\n**Findings:**\\n- [FAIL] Something\
      \ broke\"\n   - Result: verdict=\"FAIL\", findings=[\"[FAIL] Something broke\"\
      ]\n\n7. test_verdict_raw_output_preserved:\n   - Input: any text\n   - Result:\
      \ raw_output should equal the input string\n\nRun: python -m pytest tests/test_validation_pipeline.py\
      \ -v -k \"Verdict\"\nFix any failures before marking complete.\n\nReference:\
      \ docs/plans/2026-02-13-03-per-task-validation-pipeline-design.md\n"
    depends_on:
    - '5.1'
  - id: '5.3'
    name: Create unit tests for validation flow integration
    agent: coder
    status: pending
    description: "Add integration test cases to tests/test_validation_pipeline.py\
      \ that verify\nthe validation flow logic without actually spawning Claude.\n\n\
      These tests verify the decision logic, not the actual Claude execution.\n\nTest\
      \ cases:\n\n1. test_validation_disabled_skips_validation:\n   - Create ValidationConfig(enabled=False)\n\
      \   - Verify that for any task, the validation_config.enabled check would\n \
      \    be False, meaning validation is skipped\n\n2. test_validation_not_triggered_for_reviewer:\n\
      \   - Create ValidationConfig(enabled=True, run_after=[\"coder\"])\n   - Task\
      \ with agent=\"code-reviewer\"\n   - Verify \"code-reviewer\" not in validation_config.run_after\n\
      \n3. test_validation_triggered_for_coder:\n   - Create ValidationConfig(enabled=True,\
      \ run_after=[\"coder\"])\n   - Task with agent=\"coder\"\n   - Verify \"coder\"\
      \ in validation_config.run_after\n\n4. test_max_validation_attempts_respected:\n\
      \   - Create ValidationConfig(enabled=True, max_validation_attempts=2)\n   -\
      \ Task with validation_attempts=2 (already at max)\n   - Verify validation_attempts\
      \ >= max_validation_attempts\n\n5. test_validation_findings_stored_in_task:\n\
      \   - Simulate a FAIL verdict with findings\n   - Create a task dict\n   - Set\
      \ task[\"validation_findings\"] = \"findings text\"\n   - Verify task[\"validation_findings\"\
      ] contains the findings\n   - Set task[\"validation_attempts\"] = 1\n   - Verify\
      \ task[\"validation_attempts\"] == 1\n\n6. test_validation_findings_cleared_on_success:\n\
      \   - Create a task dict with validation_findings and validation_attempts\n \
      \  - Simulate clearing: task.pop(\"validation_findings\", None)\n   - Verify\
      \ \"validation_findings\" not in task\n\nRun: python -m pytest tests/test_validation_pipeline.py\
      \ -v\nFix any failures before marking complete.\n\nReference: docs/plans/2026-02-13-03-per-task-validation-pipeline-design.md\n"
    depends_on:
    - '5.2'
- id: phase-6
  name: Phase 6 - Verification
  status: pending
  tasks:
  - id: '6.1'
    name: Run full verification suite
    agent: code-reviewer
    status: pending
    description: "Run all verification checks to confirm the per-task validation pipeline\n\
      works correctly.\n\n1. Syntax check:\n   python3 -c \"import py_compile; py_compile.compile('scripts/plan-orchestrator.py',\
      \ doraise=True); print('plan-orchestrator.py: OK')\"\n\n2. Run unit tests:\n\
      \   python -m pytest tests/test_validation_pipeline.py -v\n\n3. Run orchestrator\
      \ dry-run to verify no regressions:\n   python scripts/plan-orchestrator.py --plan\
      \ .claude/plans/sample-plan.yaml --dry-run\n\n4. Verify agent files exist and\
      \ have valid frontmatter:\n   python3 -c \"\n   import yaml, os\n   for agent\
      \ in ['validator', 'issue-verifier']:\n       path = f'.claude/agents/{agent}.md'\n\
      \       assert os.path.exists(path), f'Missing: {path}'\n       content = open(path).read()\n\
      \       parts = content.split('---', 2)\n       assert len(parts) >= 3, f'Invalid\
      \ frontmatter in {path}'\n       meta = yaml.safe_load(parts[1])\n       assert\
      \ 'name' in meta, f'Missing name in {path}'\n       assert 'description' in\
      \ meta, f'Missing description in {path}'\n       print(f'OK: {path} - name={meta[\\\"name\\\
      \"]}')\n   print('All agent files valid')\n   \"\n\n5. Verify structural checks\
      \ in plan-orchestrator.py:\n   python3 -c \"\n   with open('scripts/plan-orchestrator.py')\
      \ as f:\n       source = f.read()\n   assert 'class ValidationConfig' in source,\
      \ 'ValidationConfig class not found'\n   assert 'class ValidationVerdict' in\
      \ source, 'ValidationVerdict class not found'\n   assert 'parse_validation_config'\
      \ in source, 'parse_validation_config not found'\n   assert 'parse_validation_verdict'\
      \ in source, 'parse_validation_verdict not found'\n   assert 'run_validation'\
      \ in source, 'run_validation function not found'\n   assert 'VALIDATION_PROMPT_TEMPLATE'\
      \ in source, 'VALIDATION_PROMPT_TEMPLATE not found'\n   assert 'VERDICT_PATTERN'\
      \ in source, 'VERDICT_PATTERN not found'\n   assert 'FINDING_PATTERN' in source,\
      \ 'FINDING_PATTERN not found'\n   assert 'validation_findings' in source, 'validation_findings\
      \ handling not found'\n   assert 'validation_config' in source, 'validation_config\
      \ usage not found'\n   print('All structural checks passed')\n   \"\n\n6. Run\
      \ build and existing tests to check for regressions:\n   pnpm run build\n   pnpm\
      \ test\n\nIf any check fails, report which check failed and what the error was.\n\
      Do not attempt to fix - just report findings.\n\nReference: docs/plans/2026-02-13-03-per-task-validation-pipeline-design.md\n"
    depends_on:
    - '5.3'
    - '4.4'
    max_attempts: 5
