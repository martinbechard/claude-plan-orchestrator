meta:
  name: Tiered Model Selection with Cost-Aware Escalation
  description: 'Add dynamic model escalation to the orchestrator so agents start with
    cost-efficient models and automatically escalate to more capable ones when tasks
    fail repeatedly. Adds MODEL_TIERS constant, EscalationConfig dataclass, parse_escalation_config
    helper, --model flag injection in all CLI execution paths (sequential, parallel,
    validation), escalation logging, and model_used tracking in usage reports. Builds
    on Feature 02 (Agent Definition Framework) and Feature 06 (Token Usage Tracking)
    which are already implemented.

    '
  plan_doc: docs/plans/2026-02-16-10-tiered-model-escalation-design.md
  created: '2026-02-16'
  max_attempts_default: 3
  validation:
    enabled: true
    run_after:
    - coder
    validators:
    - validator
    max_validation_attempts: 1
sections:
- id: phase-1
  name: Phase 1 - EscalationConfig and Model Tier Logic
  status: completed
  tasks:
  - id: '1.1'
    name: Add MODEL_TIERS constant and EscalationConfig dataclass
    agent: coder
    status: completed
    description: "In scripts/plan-orchestrator.py, add the MODEL_TIERS constant and\
      \ EscalationConfig dataclass.\nReference: docs/plans/2026-02-16-10-tiered-model-escalation-design.md\n\
      Steps: 1. Add manifest constants after the existing DEFAULT_RESERVED_BUDGET_USD\n\
      \   constant (currently at line 53):\n\n   MODEL_TIERS: list[str] = [\"haiku\"\
      , \"sonnet\", \"opus\"]\n   DEFAULT_ESCALATE_AFTER_FAILURES = 2\n   DEFAULT_MAX_MODEL\
      \ = \"opus\"\n   DEFAULT_VALIDATION_MODEL = \"sonnet\"\n   DEFAULT_STARTING_MODEL\
      \ = \"sonnet\"\n\n2. Add the EscalationConfig dataclass after the parse_budget_config\n\
      \   function (after line ~610). Place it before the build_validation_prompt\n\
      \   function:\n\n   @dataclass\n   class EscalationConfig:\n       \"\"\"Model\
      \ escalation configuration for cost-aware tier promotion.\n\n       Controls\
      \ automatic model upgrades when tasks fail repeatedly.\n       When disabled,\
      \ models are unchanged (backwards compatible).\n       \"\"\"\n       enabled:\
      \ bool = False\n       escalate_after: int = DEFAULT_ESCALATE_AFTER_FAILURES\n\
      \       max_model: str = DEFAULT_MAX_MODEL\n       validation_model: str = DEFAULT_VALIDATION_MODEL\n\
      \       starting_model: str = DEFAULT_STARTING_MODEL\n\n       def get_effective_model(self,\
      \ agent_model: str, attempt: int) -> str:\n           \"\"\"Compute the effective\
      \ model for a given agent and attempt number.\n\n           Uses the MODEL_TIERS\
      \ ladder to determine escalation. Attempts 1 through\n           escalate_after\
      \ use the base model. Each subsequent batch of escalate_after\n           attempts\
      \ promotes one tier, capped at max_model.\n\n           Args:\n            \
      \   agent_model: The agent's starting model from frontmatter.\n            \
      \   attempt: The current attempt number (1-based).\n\n           Returns:\n\
      \               The model string to use for this attempt.\n           \"\"\"\
      \n           if not self.enabled:\n               return agent_model\n     \
      \      base = agent_model or self.starting_model\n           if base not in\
      \ MODEL_TIERS:\n               return base\n           base_idx = MODEL_TIERS.index(base)\n\
      \           max_idx = MODEL_TIERS.index(self.max_model) if self.max_model in\
      \ MODEL_TIERS else len(MODEL_TIERS) - 1\n           steps = max(0, (attempt\
      \ - 1) // self.escalate_after)\n           effective_idx = min(base_idx + steps,\
      \ max_idx)\n           return MODEL_TIERS[effective_idx]\n\n3. Verify with:\
      \ python3 -c \"import py_compile; py_compile.compile('scripts/plan-orchestrator.py',\
      \ doraise=True); print('syntax OK')\"\nFiles: scripts/plan-orchestrator.py\n"
    attempts: 1
    last_attempt: '2026-02-16T18:11:07.852520'
    completed_at: '2026-02-16T18:13:45.198919'
    result_message: Added MODEL_TIERS constant (haiku/sonnet/opus), escalation defaults
      (DEFAULT_ESCALATE_AFTER_FAILURES, DEFAULT_MAX_MODEL, DEFAULT_VALIDATION_MODEL,
      DEFAULT_STARTING_MODEL), and EscalationConfig dataclass with get_effective_model()
      method. Placed constants after DEFAULT_RESERVED_BUDGET_USD and dataclass after
      parse_validation_config, before build_validation_prompt. Syntax verified, smoke
      tests pass.
  - id: '1.2'
    name: Add parse_escalation_config helper function
    agent: coder
    status: completed
    depends_on:
    - '1.1'
    description: "In scripts/plan-orchestrator.py, add the parse_escalation_config\
      \ helper function immediately after the EscalationConfig dataclass.\nReference:\
      \ docs/plans/2026-02-16-10-tiered-model-escalation-design.md\nSteps: 1. Add\
      \ this function right after EscalationConfig:\n\n   def parse_escalation_config(plan:\
      \ dict) -> EscalationConfig:\n       \"\"\"Parse model escalation configuration\
      \ from plan YAML meta.\n\n       Reads meta.model_escalation from the plan dict.\
      \ When the block\n       is absent, returns a disabled EscalationConfig (backwards\
      \ compatible).\n\n       Args:\n           plan: The full plan dict loaded from\
      \ YAML.\n\n       Returns:\n           An EscalationConfig populated from plan\
      \ meta or defaults.\n       \"\"\"\n       esc_meta = plan.get(\"meta\", {}).get(\"\
      model_escalation\", {})\n       if not esc_meta:\n           return EscalationConfig()\n\
      \       return EscalationConfig(\n           enabled=esc_meta.get(\"enabled\"\
      , False),\n           escalate_after=esc_meta.get(\"escalate_after\", DEFAULT_ESCALATE_AFTER_FAILURES),\n\
      \           max_model=esc_meta.get(\"max_model\", DEFAULT_MAX_MODEL),\n    \
      \       validation_model=esc_meta.get(\"validation_model\", DEFAULT_VALIDATION_MODEL),\n\
      \           starting_model=esc_meta.get(\"starting_model\", DEFAULT_STARTING_MODEL),\n\
      \       )\n\n2. Verify with: python3 -c \"import py_compile; py_compile.compile('scripts/plan-orchestrator.py',\
      \ doraise=True); print('syntax OK')\"\nFiles: scripts/plan-orchestrator.py\n"
    attempts: 1
    last_attempt: '2026-02-16T18:13:47.273269'
    completed_at: '2026-02-16T18:15:45.859449'
    result_message: Added parse_escalation_config() helper function immediately after
      EscalationConfig dataclass at line 707. The function parses meta.model_escalation
      from plan YAML dict, returning a disabled EscalationConfig with defaults when
      the block is absent (backwards compatible). Syntax verified, function import
      and behavior tested.
- id: phase-2
  name: Phase 2 - CLI Model Flag Injection
  status: completed
  tasks:
  - id: '2.1'
    name: Add model parameter to run_claude_task
    agent: coder
    status: completed
    depends_on:
    - '1.2'
    description: "In scripts/plan-orchestrator.py, modify run_claude_task() to accept\
      \ and use an optional model parameter.\nReference: docs/plans/2026-02-16-10-tiered-model-escalation-design.md\n\
      Steps: 1. Change the function signature (currently at line 2116) from:\n   def\
      \ run_claude_task(prompt: str, dry_run: bool = False) -> TaskResult:\n   To:\n\
      \   def run_claude_task(prompt: str, dry_run: bool = False, model: str = \"\"\
      ) -> TaskResult:\n\n2. In the command construction block (around line 2125-2130),\
      \ after\n   building the base cmd list, add model injection BEFORE the\n   output-format\
      \ flags:\n\n   if model:\n       cmd.extend([\"--model\", model])\n\n   This\
      \ should go right after:\n     cmd = [*CLAUDE_CMD, \"--dangerously-skip-permissions\"\
      , \"--print\", prompt]\n   And before:\n     if VERBOSE:\n         cmd.extend([\"\
      --output-format\", \"stream-json\", \"--verbose\"])\n\n3. Update the verbose\
      \ log to show model selection:\n   After the existing \"Prompt length\" verbose_log,\
      \ add:\n   if model:\n       verbose_log(f\"Model override: {model}\", \"EXEC\"\
      )\n\n4. Verify with: python3 -c \"import py_compile; py_compile.compile('scripts/plan-orchestrator.py',\
      \ doraise=True); print('syntax OK')\"\nFiles: scripts/plan-orchestrator.py\n"
    attempts: 1
    last_attempt: '2026-02-16T18:15:47.943426'
    completed_at: '2026-02-16T18:18:10.740877'
    result_message: 'Added optional model parameter to run_claude_task(). When model
      is non-empty, injects --model flag into Claude CLI command before output-format
      flags. Added verbose log showing model override. Signature: run_claude_task(prompt,
      dry_run=False, model='''') -> TaskResult. Syntax verified, functional test confirms
      parameter wiring.'
  - id: '2.2'
    name: Add model flag to parallel task runner
    agent: coder
    status: completed
    depends_on:
    - '2.1'
    description: "In scripts/plan-orchestrator.py, modify run_parallel_task() to accept\
      \ and pass a model parameter to the Claude CLI subprocess.\nReference: docs/plans/2026-02-16-10-tiered-model-escalation-design.md\n\
      Steps: 1. Find run_parallel_task() (currently at line 1664). Add a model\n \
      \  parameter to its signature. The current signature is:\n   def run_parallel_task(task_id:\
      \ str, task: dict, section: dict,\n                         plan: dict, plan_path:\
      \ str, parallel_group: str,\n                         sibling_task_ids: list[str])\
      \ -> tuple[str, TaskResult]:\n   Change to:\n   def run_parallel_task(task_id:\
      \ str, task: dict, section: dict,\n                         plan: dict, plan_path:\
      \ str, parallel_group: str,\n                         sibling_task_ids: list[str],\n\
      \                         model: str = \"\") -> tuple[str, TaskResult]:\n\n\
      2. In the command construction block (around line 1711), add model\n   injection\
      \ after the base cmd list:\n   cmd = [*CLAUDE_CMD, \"--dangerously-skip-permissions\"\
      , \"--print\", prompt]\n   if model:\n       cmd.extend([\"--model\", model])\n\
      \n3. Add a log line for model selection:\n   verbose_log(f\"[PARALLEL] Task\
      \ {task_id} using model: {model or 'default'}\", \"PARALLEL\")\n\n4. Verify\
      \ with: python3 -c \"import py_compile; py_compile.compile('scripts/plan-orchestrator.py',\
      \ doraise=True); print('syntax OK')\"\nFiles: scripts/plan-orchestrator.py\n"
    attempts: 1
    last_attempt: '2026-02-16T18:18:12.811430'
    completed_at: '2026-02-16T18:20:39.183833'
    result_message: 'Added optional model parameter to run_parallel_task(). When model
      is non-empty, injects --model flag into Claude CLI subprocess command. Added
      verbose log showing model selection for parallel tasks. Signature: run_parallel_task(...,
      dry_run=False, model='''') -> tuple[str, TaskResult]. Syntax verified, parameter
      introspection confirms correct wiring.'
- id: phase-3
  name: Phase 3 - Orchestrator Integration
  status: completed
  tasks:
  - id: '3.1'
    name: Integrate escalation config into run_orchestrator initialization
    agent: coder
    status: completed
    depends_on:
    - '1.2'
    description: "In scripts/plan-orchestrator.py, parse escalation config and display\
      \ it in the startup header.\nReference: docs/plans/2026-02-16-10-tiered-model-escalation-design.md\n\
      Steps: 1. In run_orchestrator() (currently at line 2405), after the existing\n\
      \   validation_config = parse_validation_config(plan) line (around line 2438),\n\
      \   add:\n\n   # Parse model escalation configuration\n   escalation_config\
      \ = parse_escalation_config(plan)\n\n2. In the header print block (after the\
      \ existing validation info print\n   around line 2462-2464), add escalation\
      \ info:\n\n   if escalation_config.enabled:\n       print(f\"Model escalation:\
      \ enabled (escalate_after={escalation_config.escalate_after}, \"\n         \
      \    f\"max_model={escalation_config.max_model}, \"\n             f\"validation_model={escalation_config.validation_model})\"\
      )\n   else:\n       print(\"Model escalation: disabled (using agent default\
      \ models)\")\n\n3. Verify with: python3 -c \"import py_compile; py_compile.compile('scripts/plan-orchestrator.py',\
      \ doraise=True); print('syntax OK')\"\nFiles: scripts/plan-orchestrator.py\n"
    attempts: 1
    last_attempt: '2026-02-16T18:20:41.265130'
    completed_at: '2026-02-16T18:22:34.909533'
    result_message: Integrated escalation config into run_orchestrator initialization.
      Added parse_escalation_config(plan) call after validation_config parsing (line
      2519), and added escalation status display in the startup header showing enabled/disabled
      state with escalate_after, max_model, and validation_model parameters.
  - id: '3.2'
    name: Wire escalation into sequential task execution
    agent: coder
    status: completed
    depends_on:
    - '2.1'
    - '3.1'
    description: "In scripts/plan-orchestrator.py, compute the effective model before\
      \ each sequential task execution and pass it to run_claude_task().\nReference:\
      \ docs/plans/2026-02-16-10-tiered-model-escalation-design.md\nSteps: 1. In the\
      \ sequential execution path of run_orchestrator(), find where\n   run_claude_task\
      \ is called (currently around line 2821):\n     task_result = run_claude_task(prompt,\
      \ dry_run=dry_run)\n\n2. Before that call, add model computation. Place it after\
      \ the prompt\n   is built (after build_claude_prompt) and before run_claude_task:\n\
      \n   # Compute effective model for this task attempt\n   agent_name = task.get(\"\
      agent\") or infer_agent_for_task(task) or FALLBACK_AGENT_NAME\n   agent_def\
      \ = load_agent_definition(agent_name)\n   agent_model = agent_def.get(\"model\"\
      , \"\") if agent_def else \"\"\n   current_attempt = task.get(\"attempts\",\
      \ 1)\n   effective_model = escalation_config.get_effective_model(agent_model,\
      \ current_attempt)\n\n   # Log model selection\n   if escalation_config.enabled:\n\
      \       if effective_model != agent_model:\n           print(f\"Task {task_id}\
      \ attempt {current_attempt}: escalating from {agent_model} to {effective_model}\"\
      )\n       else:\n           print(f\"Task {task_id} attempt {current_attempt}:\
      \ using {effective_model}\")\n\n   # Record model used for observability\n \
      \  task[\"model_used\"] = effective_model\n\n3. Update the run_claude_task call\
      \ to pass the model:\n   task_result = run_claude_task(prompt, dry_run=dry_run,\
      \ model=effective_model)\n\n4. Verify with: python3 -c \"import py_compile;\
      \ py_compile.compile('scripts/plan-orchestrator.py', doraise=True); print('syntax\
      \ OK')\"\nFiles: scripts/plan-orchestrator.py\n"
    attempts: 1
    last_attempt: '2026-02-16T18:22:36.997169'
    completed_at: '2026-02-16T18:25:06.835003'
    result_message: Wired model escalation into sequential task execution. Before
      each run_claude_task() call, computes effective model from agent definition
      + attempt count via escalation_config.get_effective_model(). Logs model selection
      when escalation is enabled (including escalation vs same-model). Records model_used
      on task dict for observability. Passes effective_model to run_claude_task().
  - id: '3.3'
    name: Wire escalation into parallel task execution
    agent: coder
    status: completed
    depends_on:
    - '2.2'
    - '3.1'
    description: "In scripts/plan-orchestrator.py, compute the effective model for\
      \ parallel tasks and pass it to run_parallel_task().\nReference: docs/plans/2026-02-16-10-tiered-model-escalation-design.md\n\
      Steps: 1. Find where run_parallel_task is called in run_orchestrator(). Search\n\
      \   for \"executor.submit(run_parallel_task\" in the parallel execution block.\n\
      \n2. Before submitting each parallel task, compute the effective model:\n\n\
      \   agent_name = t.get(\"agent\") or infer_agent_for_task(t) or FALLBACK_AGENT_NAME\n\
      \   agent_def = load_agent_definition(agent_name)\n   agent_model = agent_def.get(\"\
      model\", \"\") if agent_def else \"\"\n   par_attempt = t.get(\"attempts\",\
      \ 1)\n   par_effective_model = escalation_config.get_effective_model(agent_model,\
      \ par_attempt)\n   t[\"model_used\"] = par_effective_model\n\n3. Pass par_effective_model\
      \ to the run_parallel_task call by adding\n   model=par_effective_model as an\
      \ argument.\n\n4. Verify with: python3 -c \"import py_compile; py_compile.compile('scripts/plan-orchestrator.py',\
      \ doraise=True); print('syntax OK')\"\nFiles: scripts/plan-orchestrator.py\n"
    attempts: 1
    last_attempt: '2026-02-16T18:25:08.916443'
    completed_at: '2026-02-16T18:28:28.877923'
    result_message: Wired model escalation into parallel task execution. Computes
      effective model per parallel task using agent definition + attempt count via
      escalation_config.get_effective_model() before ThreadPoolExecutor submit. Stores
      model_used on task dict for observability and passes it to run_parallel_task
      via model kwarg. Logs model selection with escalation detection.
  - id: '3.4'
    name: Wire validation model override
    agent: coder
    status: completed
    depends_on:
    - '2.1'
    - '3.1'
    description: "In scripts/plan-orchestrator.py, make run_validation() pass the\
      \ validation_model to run_claude_task() when escalation is enabled.\nReference:\
      \ docs/plans/2026-02-16-10-tiered-model-escalation-design.md\nSteps: 1. Modify\
      \ the run_validation() function signature (currently at line 819)\n   to accept\
      \ an escalation_config parameter:\n\n   def run_validation(\n       task: dict,\n\
      \       section: dict,\n       task_result: \"TaskResult\",\n       validation_config:\
      \ ValidationConfig,\n       dry_run: bool = False,\n       escalation_config:\
      \ Optional[EscalationConfig] = None,\n   ) -> ValidationVerdict:\n\n2. In run_validation(),\
      \ find where run_claude_task is called for the\n   validator (currently line\
      \ 877):\n     validator_result = run_claude_task(prompt)\n   Change to:\n  \
      \   validation_model = \"\"\n     if escalation_config and escalation_config.enabled:\n\
      \         validation_model = escalation_config.validation_model\n     validator_result\
      \ = run_claude_task(prompt, model=validation_model)\n\n3. Update all calls to\
      \ run_validation() in run_orchestrator() to pass\n   escalation_config. There\
      \ are two calls:\n   - In the sequential path (around line 2850)\n   - In the\
      \ parallel results handling (around line 2663)\n   Add escalation_config=escalation_config\
      \ to both calls.\n\n4. Verify with: python3 -c \"import py_compile; py_compile.compile('scripts/plan-orchestrator.py',\
      \ doraise=True); print('syntax OK')\"\nFiles: scripts/plan-orchestrator.py\n"
    attempts: 1
    last_attempt: '2026-02-16T18:28:30.970770'
    completed_at: '2026-02-16T18:31:55.058706'
    result_message: Wired validation model override into run_validation(). Added escalation_config
      parameter (Optional[EscalationConfig] = None) to function signature. When escalation
      is enabled, extracts validation_model from escalation_config and passes it to
      run_claude_task(). Updated both call sites in run_orchestrator() (parallel results
      handling and sequential path) to pass escalation_config=escalation_config.
- id: phase-4
  name: Phase 4 - Usage Report Enhancement
  status: completed
  tasks:
  - id: '4.1'
    name: Add model_used to usage report task records
    agent: coder
    status: completed
    depends_on:
    - '3.2'
    description: "In scripts/plan-orchestrator.py, enhance the PlanUsageTracker to\
      \ record and report the model used per task.\nReference: docs/plans/2026-02-16-10-tiered-model-escalation-design.md\n\
      Steps: 1. Modify the PlanUsageTracker.record() method to accept an optional\n\
      \   model parameter:\n\n   def record(self, task_id: str, usage: TaskUsage,\
      \ model: str = \"\") -> None:\n       \"\"\"Record usage for a completed task.\"\
      \"\"\n       self.task_usages[task_id] = usage\n       self.task_models[task_id]\
      \ = model\n\n2. Add the task_models dict to __init__:\n   def __init__(self):\n\
      \       self.task_usages: dict[str, TaskUsage] = {}\n       self.task_models:\
      \ dict[str, str] = {}\n\n3. In format_summary_line(), add model info when available:\n\
      \   Find the existing return statement and enhance it. If the task has\n   a\
      \ model recorded, append it to the summary line:\n   model = self.task_models.get(task_id,\
      \ \"\")\n   model_str = f\" [{model}]\" if model else \"\"\n   Include model_str\
      \ in the returned string.\n\n4. In write_report(), add model to each task entry\
      \ in the tasks list:\n   For each task record in the report[\"tasks\"] loop,\
      \ add:\n   \"model\": self.task_models.get(tid, \"\"),\n\n5. Update all calls\
      \ to usage_tracker.record() in run_orchestrator() to\n   pass the model. There\
      \ are two locations:\n   - Sequential path (around line 2829): pass model=effective_model\n\
      \   - Parallel path: pass model=t.get(\"model_used\", \"\")\n\n6. Verify with:\
      \ python3 -c \"import py_compile; py_compile.compile('scripts/plan-orchestrator.py',\
      \ doraise=True); print('syntax OK')\"\nFiles: scripts/plan-orchestrator.py\n"
    attempts: 1
    last_attempt: '2026-02-16T18:31:57.127847'
    completed_at: '2026-02-16T18:35:00.697128'
    result_message: Enhanced PlanUsageTracker to record and report model per task.
      Added task_models dict to __init__, model parameter to record(), model tag in
      format_summary_line(), model field in write_report() JSON output. Updated both
      call sites (parallel and sequential paths) to pass effective model to the tracker.
- id: phase-5
  name: Phase 5 - Unit Tests
  status: pending
  tasks:
  - id: '5.1'
    name: Write unit tests for EscalationConfig
    agent: coder
    status: pending
    depends_on:
    - '1.2'
    description: "Create tests/test_model_escalation.py with unit tests for the EscalationConfig\
      \ dataclass.\nReference: docs/plans/2026-02-16-10-tiered-model-escalation-design.md\n\
      Steps: 1. Create tests/test_model_escalation.py with these test cases:\n\n \
      \  a. test_escalation_config_defaults: Create EscalationConfig() with\n    \
      \  no args. Verify enabled=False, escalate_after=2, max_model=\"opus\",\n  \
      \    validation_model=\"sonnet\", starting_model=\"sonnet\".\n\n   b. test_escalation_disabled_returns_agent_model:\
      \ Create\n      EscalationConfig(enabled=False). Call get_effective_model(\"\
      haiku\", 5).\n      Verify returns \"haiku\" (no escalation when disabled).\n\
      \n   c. test_escalation_first_attempts_use_base_model: Create\n      EscalationConfig(enabled=True,\
      \ escalate_after=2).\n      Call get_effective_model(\"sonnet\", 1). Verify\
      \ returns \"sonnet\".\n      Call get_effective_model(\"sonnet\", 2). Verify\
      \ returns \"sonnet\".\n\n   d. test_escalation_after_failures: Create\n    \
      \  EscalationConfig(enabled=True, escalate_after=2).\n      Call get_effective_model(\"\
      sonnet\", 3). Verify returns \"opus\".\n      (attempt 3 = beyond 2 failures,\
      \ escalate from sonnet to opus)\n\n   e. test_escalation_from_haiku_to_sonnet:\
      \ Create\n      EscalationConfig(enabled=True, escalate_after=2).\n      Call\
      \ get_effective_model(\"haiku\", 3). Verify returns \"sonnet\".\n      Call\
      \ get_effective_model(\"haiku\", 5). Verify returns \"opus\".\n\n   f. test_escalation_capped_at_max_model:\
      \ Create\n      EscalationConfig(enabled=True, escalate_after=1, max_model=\"\
      sonnet\").\n      Call get_effective_model(\"haiku\", 10). Verify returns \"\
      sonnet\"\n      (capped, not opus).\n\n   g. test_escalation_already_at_max:\
      \ Create\n      EscalationConfig(enabled=True, escalate_after=1, max_model=\"\
      opus\").\n      Call get_effective_model(\"opus\", 5). Verify returns \"opus\"\
      \n      (cannot escalate beyond opus).\n\n   h. test_escalation_unknown_model_passthrough:\
      \ Create\n      EscalationConfig(enabled=True).\n      Call get_effective_model(\"\
      custom-model\", 5). Verify returns\n      \"custom-model\" (unknown models are\
      \ returned unchanged).\n\n   i. test_escalation_empty_model_uses_starting: Create\n\
      \      EscalationConfig(enabled=True, starting_model=\"sonnet\").\n      Call\
      \ get_effective_model(\"\", 1). Verify returns \"sonnet\".\n      Call get_effective_model(\"\
      \", 3). Verify returns \"opus\".\n\n2. Import using importlib (same pattern\
      \ as test_budget_guard.py):\n   import importlib.util\n   spec = importlib.util.spec_from_file_location(\n\
      \       \"plan_orchestrator\", \"scripts/plan-orchestrator.py\")\n   mod = importlib.util.module_from_spec(spec)\n\
      \   spec.loader.exec_module(mod)\n   EscalationConfig = mod.EscalationConfig\n\
      \   MODEL_TIERS = mod.MODEL_TIERS\n\n3. Run: python3 -m pytest tests/test_model_escalation.py\
      \ -v\n   Fix any failures.\n\nFiles: tests/test_model_escalation.py\n"
  - id: '5.2'
    name: Write unit tests for parse_escalation_config
    agent: coder
    status: pending
    depends_on:
    - '1.2'
    - '5.1'
    description: "Add parse_escalation_config tests to tests/test_model_escalation.py.\n\
      Reference: docs/plans/2026-02-16-10-tiered-model-escalation-design.md\nSteps:\
      \ 1. Add parse_escalation_config to the import block:\n   parse_escalation_config\
      \ = mod.parse_escalation_config\n\n2. Add these test cases:\n\n   a. test_parse_escalation_config_defaults:\
      \ Call parse_escalation_config\n      with an empty plan dict {}. Verify returns\
      \ EscalationConfig with\n      enabled=False and all defaults.\n\n   b. test_parse_escalation_config_enabled:\
      \ Call parse_escalation_config\n      with plan = {\"meta\": {\"model_escalation\"\
      : {\"enabled\": True,\n      \"escalate_after\": 3, \"max_model\": \"sonnet\"\
      }}}.\n      Verify enabled=True, escalate_after=3, max_model=\"sonnet\".\n\n\
      \   c. test_parse_escalation_config_partial: Call parse_escalation_config\n\
      \      with plan = {\"meta\": {\"model_escalation\": {\"enabled\": True}}}.\n\
      \      Verify enabled=True but all other fields use defaults.\n\n   d. test_parse_escalation_config_with_validation_model:\
      \ Call\n      parse_escalation_config with plan that includes\n      validation_model=\"\
      haiku\". Verify validation_model=\"haiku\".\n\n   e. test_parse_escalation_config_no_meta:\
      \ Call parse_escalation_config\n      with plan = {\"sections\": []}. Verify\
      \ returns disabled config.\n\n3. Run: python3 -m pytest tests/test_model_escalation.py\
      \ -v\n   Fix any failures.\n\nFiles: tests/test_model_escalation.py\n"
- id: phase-6
  name: Phase 6 - Verification
  status: pending
  tasks:
  - id: '6.1'
    name: Verify syntax, tests, and dry-run
    agent: code-reviewer
    status: pending
    depends_on:
    - '3.2'
    - '3.3'
    - '3.4'
    - '4.1'
    - '5.2'
    description: "Run verification checks to confirm the tiered model escalation feature\
      \ works correctly.\nSteps: 1. Check Python syntax for both scripts:\n   python3\
      \ -c \"import py_compile; py_compile.compile('scripts/auto-pipeline.py', doraise=True);\
      \ py_compile.compile('scripts/plan-orchestrator.py', doraise=True)\"\n\n2. Run\
      \ unit tests:\n   python3 -m pytest tests/ 2>/dev/null || echo 'No test suite\
      \ configured'\n\n3. Verify the EscalationConfig class exists and works:\n  \
      \ python3 -c \"\n   import importlib.util\n   spec = importlib.util.spec_from_file_location('po',\
      \ 'scripts/plan-orchestrator.py')\n   mod = importlib.util.module_from_spec(spec)\n\
      \   spec.loader.exec_module(mod)\n   assert hasattr(mod, 'EscalationConfig'),\
      \ 'EscalationConfig not found'\n   assert hasattr(mod, 'parse_escalation_config'),\
      \ 'parse_escalation_config not found'\n   assert hasattr(mod, 'MODEL_TIERS'),\
      \ 'MODEL_TIERS not found'\n   # Verify tier ladder\n   assert mod.MODEL_TIERS\
      \ == ['haiku', 'sonnet', 'opus'], f'MODEL_TIERS wrong: {mod.MODEL_TIERS}'\n\
      \   # Verify escalation logic\n   cfg = mod.EscalationConfig(enabled=True, escalate_after=2)\n\
      \   assert cfg.get_effective_model('sonnet', 1) == 'sonnet'\n   assert cfg.get_effective_model('sonnet',\
      \ 3) == 'opus'\n   assert cfg.get_effective_model('haiku', 3) == 'sonnet'\n\
      \   # Verify disabled returns unchanged\n   cfg_off = mod.EscalationConfig(enabled=False)\n\
      \   assert cfg_off.get_effective_model('haiku', 10) == 'haiku'\n   print('All\
      \ escalation classes verified')\n   \"\n\n4. Verify run_claude_task accepts\
      \ model parameter:\n   python3 -c \"\n   import inspect, importlib.util\n  \
      \ spec = importlib.util.spec_from_file_location('po', 'scripts/plan-orchestrator.py')\n\
      \   mod = importlib.util.module_from_spec(spec)\n   spec.loader.exec_module(mod)\n\
      \   sig = inspect.signature(mod.run_claude_task)\n   assert 'model' in sig.parameters,\
      \ f'model param missing from run_claude_task: {sig}'\n   print('run_claude_task\
      \ model parameter verified')\n   \"\n\n5. Verify run_validation accepts escalation_config\
      \ parameter:\n   python3 -c \"\n   import inspect, importlib.util\n   spec =\
      \ importlib.util.spec_from_file_location('po', 'scripts/plan-orchestrator.py')\n\
      \   mod = importlib.util.module_from_spec(spec)\n   spec.loader.exec_module(mod)\n\
      \   sig = inspect.signature(mod.run_validation)\n   assert 'escalation_config'\
      \ in sig.parameters, f'escalation_config missing: {sig}'\n   print('run_validation\
      \ escalation_config parameter verified')\n   \"\n\n6. Run orchestrator dry-run\
      \ to verify startup prints escalation info:\n   python3 scripts/plan-orchestrator.py\
      \ --plan .claude/plans/sample-plan.yaml --dry-run\n\n7. Verify PlanUsageTracker.record\
      \ accepts model parameter:\n   python3 -c \"\n   import inspect, importlib.util\n\
      \   spec = importlib.util.spec_from_file_location('po', 'scripts/plan-orchestrator.py')\n\
      \   mod = importlib.util.module_from_spec(spec)\n   spec.loader.exec_module(mod)\n\
      \   sig = inspect.signature(mod.PlanUsageTracker.record)\n   assert 'model'\
      \ in sig.parameters, f'model param missing from record: {sig}'\n   print('PlanUsageTracker.record\
      \ model parameter verified')\n   \"\n\nIf any check fails, report the failure\
      \ with specific details.\nFiles: scripts/plan-orchestrator.py, tests/test_model_escalation.py\n"
