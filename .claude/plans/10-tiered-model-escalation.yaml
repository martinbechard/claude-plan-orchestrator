meta:
  name: Tiered Model Selection with Cost-Aware Escalation
  description: >
    Add dynamic model escalation to the orchestrator so agents start with
    cost-efficient models and automatically escalate to more capable ones when
    tasks fail repeatedly. Adds MODEL_TIERS constant, EscalationConfig dataclass,
    parse_escalation_config helper, --model flag injection in all CLI execution
    paths (sequential, parallel, validation), escalation logging, and model_used
    tracking in usage reports. Builds on Feature 02 (Agent Definition Framework)
    and Feature 06 (Token Usage Tracking) which are already implemented.
  plan_doc: docs/plans/2026-02-16-10-tiered-model-escalation-design.md
  created: '2026-02-16'
  max_attempts_default: 3
  validation:
    enabled: true
    run_after:
      - coder
    validators:
      - validator
    max_validation_attempts: 1
sections:
  - id: phase-1
    name: Phase 1 - EscalationConfig and Model Tier Logic
    status: pending
    tasks:
      - id: '1.1'
        name: Add MODEL_TIERS constant and EscalationConfig dataclass
        agent: coder
        status: pending
        description: >
          In scripts/plan-orchestrator.py, add the MODEL_TIERS constant and
          EscalationConfig dataclass.

          Reference: docs/plans/2026-02-16-10-tiered-model-escalation-design.md

          Steps:
          1. Add manifest constants after the existing DEFAULT_RESERVED_BUDGET_USD
             constant (currently at line 53):

             MODEL_TIERS: list[str] = ["haiku", "sonnet", "opus"]
             DEFAULT_ESCALATE_AFTER_FAILURES = 2
             DEFAULT_MAX_MODEL = "opus"
             DEFAULT_VALIDATION_MODEL = "sonnet"
             DEFAULT_STARTING_MODEL = "sonnet"

          2. Add the EscalationConfig dataclass after the parse_budget_config
             function (after line ~610). Place it before the build_validation_prompt
             function:

             @dataclass
             class EscalationConfig:
                 """Model escalation configuration for cost-aware tier promotion.

                 Controls automatic model upgrades when tasks fail repeatedly.
                 When disabled, models are unchanged (backwards compatible).
                 """
                 enabled: bool = False
                 escalate_after: int = DEFAULT_ESCALATE_AFTER_FAILURES
                 max_model: str = DEFAULT_MAX_MODEL
                 validation_model: str = DEFAULT_VALIDATION_MODEL
                 starting_model: str = DEFAULT_STARTING_MODEL

                 def get_effective_model(self, agent_model: str, attempt: int) -> str:
                     """Compute the effective model for a given agent and attempt number.

                     Uses the MODEL_TIERS ladder to determine escalation. Attempts 1 through
                     escalate_after use the base model. Each subsequent batch of escalate_after
                     attempts promotes one tier, capped at max_model.

                     Args:
                         agent_model: The agent's starting model from frontmatter.
                         attempt: The current attempt number (1-based).

                     Returns:
                         The model string to use for this attempt.
                     """
                     if not self.enabled:
                         return agent_model
                     base = agent_model or self.starting_model
                     if base not in MODEL_TIERS:
                         return base
                     base_idx = MODEL_TIERS.index(base)
                     max_idx = MODEL_TIERS.index(self.max_model) if self.max_model in MODEL_TIERS else len(MODEL_TIERS) - 1
                     steps = max(0, (attempt - 1) // self.escalate_after)
                     effective_idx = min(base_idx + steps, max_idx)
                     return MODEL_TIERS[effective_idx]

          3. Verify with: python3 -c "import py_compile; py_compile.compile('scripts/plan-orchestrator.py', doraise=True); print('syntax OK')"

          Files: scripts/plan-orchestrator.py

      - id: '1.2'
        name: Add parse_escalation_config helper function
        agent: coder
        status: pending
        depends_on:
          - '1.1'
        description: >
          In scripts/plan-orchestrator.py, add the parse_escalation_config helper
          function immediately after the EscalationConfig dataclass.

          Reference: docs/plans/2026-02-16-10-tiered-model-escalation-design.md

          Steps:
          1. Add this function right after EscalationConfig:

             def parse_escalation_config(plan: dict) -> EscalationConfig:
                 """Parse model escalation configuration from plan YAML meta.

                 Reads meta.model_escalation from the plan dict. When the block
                 is absent, returns a disabled EscalationConfig (backwards compatible).

                 Args:
                     plan: The full plan dict loaded from YAML.

                 Returns:
                     An EscalationConfig populated from plan meta or defaults.
                 """
                 esc_meta = plan.get("meta", {}).get("model_escalation", {})
                 if not esc_meta:
                     return EscalationConfig()
                 return EscalationConfig(
                     enabled=esc_meta.get("enabled", False),
                     escalate_after=esc_meta.get("escalate_after", DEFAULT_ESCALATE_AFTER_FAILURES),
                     max_model=esc_meta.get("max_model", DEFAULT_MAX_MODEL),
                     validation_model=esc_meta.get("validation_model", DEFAULT_VALIDATION_MODEL),
                     starting_model=esc_meta.get("starting_model", DEFAULT_STARTING_MODEL),
                 )

          2. Verify with: python3 -c "import py_compile; py_compile.compile('scripts/plan-orchestrator.py', doraise=True); print('syntax OK')"

          Files: scripts/plan-orchestrator.py

  - id: phase-2
    name: Phase 2 - CLI Model Flag Injection
    status: pending
    tasks:
      - id: '2.1'
        name: Add model parameter to run_claude_task
        agent: coder
        status: pending
        depends_on:
          - '1.2'
        description: >
          In scripts/plan-orchestrator.py, modify run_claude_task() to accept and
          use an optional model parameter.

          Reference: docs/plans/2026-02-16-10-tiered-model-escalation-design.md

          Steps:
          1. Change the function signature (currently at line 2116) from:
             def run_claude_task(prompt: str, dry_run: bool = False) -> TaskResult:
             To:
             def run_claude_task(prompt: str, dry_run: bool = False, model: str = "") -> TaskResult:

          2. In the command construction block (around line 2125-2130), after
             building the base cmd list, add model injection BEFORE the
             output-format flags:

             if model:
                 cmd.extend(["--model", model])

             This should go right after:
               cmd = [*CLAUDE_CMD, "--dangerously-skip-permissions", "--print", prompt]
             And before:
               if VERBOSE:
                   cmd.extend(["--output-format", "stream-json", "--verbose"])

          3. Update the verbose log to show model selection:
             After the existing "Prompt length" verbose_log, add:
             if model:
                 verbose_log(f"Model override: {model}", "EXEC")

          4. Verify with: python3 -c "import py_compile; py_compile.compile('scripts/plan-orchestrator.py', doraise=True); print('syntax OK')"

          Files: scripts/plan-orchestrator.py

      - id: '2.2'
        name: Add model flag to parallel task runner
        agent: coder
        status: pending
        depends_on:
          - '2.1'
        description: >
          In scripts/plan-orchestrator.py, modify run_parallel_task() to accept
          and pass a model parameter to the Claude CLI subprocess.

          Reference: docs/plans/2026-02-16-10-tiered-model-escalation-design.md

          Steps:
          1. Find run_parallel_task() (currently at line 1664). Add a model
             parameter to its signature. The current signature is:
             def run_parallel_task(task_id: str, task: dict, section: dict,
                                   plan: dict, plan_path: str, parallel_group: str,
                                   sibling_task_ids: list[str]) -> tuple[str, TaskResult]:
             Change to:
             def run_parallel_task(task_id: str, task: dict, section: dict,
                                   plan: dict, plan_path: str, parallel_group: str,
                                   sibling_task_ids: list[str],
                                   model: str = "") -> tuple[str, TaskResult]:

          2. In the command construction block (around line 1711), add model
             injection after the base cmd list:
             cmd = [*CLAUDE_CMD, "--dangerously-skip-permissions", "--print", prompt]
             if model:
                 cmd.extend(["--model", model])

          3. Add a log line for model selection:
             verbose_log(f"[PARALLEL] Task {task_id} using model: {model or 'default'}", "PARALLEL")

          4. Verify with: python3 -c "import py_compile; py_compile.compile('scripts/plan-orchestrator.py', doraise=True); print('syntax OK')"

          Files: scripts/plan-orchestrator.py

  - id: phase-3
    name: Phase 3 - Orchestrator Integration
    status: pending
    tasks:
      - id: '3.1'
        name: Integrate escalation config into run_orchestrator initialization
        agent: coder
        status: pending
        depends_on:
          - '1.2'
        description: >
          In scripts/plan-orchestrator.py, parse escalation config and display it
          in the startup header.

          Reference: docs/plans/2026-02-16-10-tiered-model-escalation-design.md

          Steps:
          1. In run_orchestrator() (currently at line 2405), after the existing
             validation_config = parse_validation_config(plan) line (around line 2438),
             add:

             # Parse model escalation configuration
             escalation_config = parse_escalation_config(plan)

          2. In the header print block (after the existing validation info print
             around line 2462-2464), add escalation info:

             if escalation_config.enabled:
                 print(f"Model escalation: enabled (escalate_after={escalation_config.escalate_after}, "
                       f"max_model={escalation_config.max_model}, "
                       f"validation_model={escalation_config.validation_model})")
             else:
                 print("Model escalation: disabled (using agent default models)")

          3. Verify with: python3 -c "import py_compile; py_compile.compile('scripts/plan-orchestrator.py', doraise=True); print('syntax OK')"

          Files: scripts/plan-orchestrator.py

      - id: '3.2'
        name: Wire escalation into sequential task execution
        agent: coder
        status: pending
        depends_on:
          - '2.1'
          - '3.1'
        description: >
          In scripts/plan-orchestrator.py, compute the effective model before each
          sequential task execution and pass it to run_claude_task().

          Reference: docs/plans/2026-02-16-10-tiered-model-escalation-design.md

          Steps:
          1. In the sequential execution path of run_orchestrator(), find where
             run_claude_task is called (currently around line 2821):
               task_result = run_claude_task(prompt, dry_run=dry_run)

          2. Before that call, add model computation. Place it after the prompt
             is built (after build_claude_prompt) and before run_claude_task:

             # Compute effective model for this task attempt
             agent_name = task.get("agent") or infer_agent_for_task(task) or FALLBACK_AGENT_NAME
             agent_def = load_agent_definition(agent_name)
             agent_model = agent_def.get("model", "") if agent_def else ""
             current_attempt = task.get("attempts", 1)
             effective_model = escalation_config.get_effective_model(agent_model, current_attempt)

             # Log model selection
             if escalation_config.enabled:
                 if effective_model != agent_model:
                     print(f"Task {task_id} attempt {current_attempt}: escalating from {agent_model} to {effective_model}")
                 else:
                     print(f"Task {task_id} attempt {current_attempt}: using {effective_model}")

             # Record model used for observability
             task["model_used"] = effective_model

          3. Update the run_claude_task call to pass the model:
             task_result = run_claude_task(prompt, dry_run=dry_run, model=effective_model)

          4. Verify with: python3 -c "import py_compile; py_compile.compile('scripts/plan-orchestrator.py', doraise=True); print('syntax OK')"

          Files: scripts/plan-orchestrator.py

      - id: '3.3'
        name: Wire escalation into parallel task execution
        agent: coder
        status: pending
        depends_on:
          - '2.2'
          - '3.1'
        description: >
          In scripts/plan-orchestrator.py, compute the effective model for parallel
          tasks and pass it to run_parallel_task().

          Reference: docs/plans/2026-02-16-10-tiered-model-escalation-design.md

          Steps:
          1. Find where run_parallel_task is called in run_orchestrator(). Search
             for "executor.submit(run_parallel_task" in the parallel execution block.

          2. Before submitting each parallel task, compute the effective model:

             agent_name = t.get("agent") or infer_agent_for_task(t) or FALLBACK_AGENT_NAME
             agent_def = load_agent_definition(agent_name)
             agent_model = agent_def.get("model", "") if agent_def else ""
             par_attempt = t.get("attempts", 1)
             par_effective_model = escalation_config.get_effective_model(agent_model, par_attempt)
             t["model_used"] = par_effective_model

          3. Pass par_effective_model to the run_parallel_task call by adding
             model=par_effective_model as an argument.

          4. Verify with: python3 -c "import py_compile; py_compile.compile('scripts/plan-orchestrator.py', doraise=True); print('syntax OK')"

          Files: scripts/plan-orchestrator.py

      - id: '3.4'
        name: Wire validation model override
        agent: coder
        status: pending
        depends_on:
          - '2.1'
          - '3.1'
        description: >
          In scripts/plan-orchestrator.py, make run_validation() pass the
          validation_model to run_claude_task() when escalation is enabled.

          Reference: docs/plans/2026-02-16-10-tiered-model-escalation-design.md

          Steps:
          1. Modify the run_validation() function signature (currently at line 819)
             to accept an escalation_config parameter:

             def run_validation(
                 task: dict,
                 section: dict,
                 task_result: "TaskResult",
                 validation_config: ValidationConfig,
                 dry_run: bool = False,
                 escalation_config: Optional[EscalationConfig] = None,
             ) -> ValidationVerdict:

          2. In run_validation(), find where run_claude_task is called for the
             validator (currently line 877):
               validator_result = run_claude_task(prompt)
             Change to:
               validation_model = ""
               if escalation_config and escalation_config.enabled:
                   validation_model = escalation_config.validation_model
               validator_result = run_claude_task(prompt, model=validation_model)

          3. Update all calls to run_validation() in run_orchestrator() to pass
             escalation_config. There are two calls:
             - In the sequential path (around line 2850)
             - In the parallel results handling (around line 2663)
             Add escalation_config=escalation_config to both calls.

          4. Verify with: python3 -c "import py_compile; py_compile.compile('scripts/plan-orchestrator.py', doraise=True); print('syntax OK')"

          Files: scripts/plan-orchestrator.py

  - id: phase-4
    name: Phase 4 - Usage Report Enhancement
    status: pending
    tasks:
      - id: '4.1'
        name: Add model_used to usage report task records
        agent: coder
        status: pending
        depends_on:
          - '3.2'
        description: >
          In scripts/plan-orchestrator.py, enhance the PlanUsageTracker to record
          and report the model used per task.

          Reference: docs/plans/2026-02-16-10-tiered-model-escalation-design.md

          Steps:
          1. Modify the PlanUsageTracker.record() method to accept an optional
             model parameter:

             def record(self, task_id: str, usage: TaskUsage, model: str = "") -> None:
                 """Record usage for a completed task."""
                 self.task_usages[task_id] = usage
                 self.task_models[task_id] = model

          2. Add the task_models dict to __init__:
             def __init__(self):
                 self.task_usages: dict[str, TaskUsage] = {}
                 self.task_models: dict[str, str] = {}

          3. In format_summary_line(), add model info when available:
             Find the existing return statement and enhance it. If the task has
             a model recorded, append it to the summary line:
             model = self.task_models.get(task_id, "")
             model_str = f" [{model}]" if model else ""
             Include model_str in the returned string.

          4. In write_report(), add model to each task entry in the tasks list:
             For each task record in the report["tasks"] loop, add:
             "model": self.task_models.get(tid, ""),

          5. Update all calls to usage_tracker.record() in run_orchestrator() to
             pass the model. There are two locations:
             - Sequential path (around line 2829): pass model=effective_model
             - Parallel path: pass model=t.get("model_used", "")

          6. Verify with: python3 -c "import py_compile; py_compile.compile('scripts/plan-orchestrator.py', doraise=True); print('syntax OK')"

          Files: scripts/plan-orchestrator.py

  - id: phase-5
    name: Phase 5 - Unit Tests
    status: pending
    tasks:
      - id: '5.1'
        name: Write unit tests for EscalationConfig
        agent: coder
        status: pending
        depends_on:
          - '1.2'
        description: >
          Create tests/test_model_escalation.py with unit tests for the
          EscalationConfig dataclass.

          Reference: docs/plans/2026-02-16-10-tiered-model-escalation-design.md

          Steps:
          1. Create tests/test_model_escalation.py with these test cases:

             a. test_escalation_config_defaults: Create EscalationConfig() with
                no args. Verify enabled=False, escalate_after=2, max_model="opus",
                validation_model="sonnet", starting_model="sonnet".

             b. test_escalation_disabled_returns_agent_model: Create
                EscalationConfig(enabled=False). Call get_effective_model("haiku", 5).
                Verify returns "haiku" (no escalation when disabled).

             c. test_escalation_first_attempts_use_base_model: Create
                EscalationConfig(enabled=True, escalate_after=2).
                Call get_effective_model("sonnet", 1). Verify returns "sonnet".
                Call get_effective_model("sonnet", 2). Verify returns "sonnet".

             d. test_escalation_after_failures: Create
                EscalationConfig(enabled=True, escalate_after=2).
                Call get_effective_model("sonnet", 3). Verify returns "opus".
                (attempt 3 = beyond 2 failures, escalate from sonnet to opus)

             e. test_escalation_from_haiku_to_sonnet: Create
                EscalationConfig(enabled=True, escalate_after=2).
                Call get_effective_model("haiku", 3). Verify returns "sonnet".
                Call get_effective_model("haiku", 5). Verify returns "opus".

             f. test_escalation_capped_at_max_model: Create
                EscalationConfig(enabled=True, escalate_after=1, max_model="sonnet").
                Call get_effective_model("haiku", 10). Verify returns "sonnet"
                (capped, not opus).

             g. test_escalation_already_at_max: Create
                EscalationConfig(enabled=True, escalate_after=1, max_model="opus").
                Call get_effective_model("opus", 5). Verify returns "opus"
                (cannot escalate beyond opus).

             h. test_escalation_unknown_model_passthrough: Create
                EscalationConfig(enabled=True).
                Call get_effective_model("custom-model", 5). Verify returns
                "custom-model" (unknown models are returned unchanged).

             i. test_escalation_empty_model_uses_starting: Create
                EscalationConfig(enabled=True, starting_model="sonnet").
                Call get_effective_model("", 1). Verify returns "sonnet".
                Call get_effective_model("", 3). Verify returns "opus".

          2. Import using importlib (same pattern as test_budget_guard.py):
             import importlib.util
             spec = importlib.util.spec_from_file_location(
                 "plan_orchestrator", "scripts/plan-orchestrator.py")
             mod = importlib.util.module_from_spec(spec)
             spec.loader.exec_module(mod)
             EscalationConfig = mod.EscalationConfig
             MODEL_TIERS = mod.MODEL_TIERS

          3. Run: python3 -m pytest tests/test_model_escalation.py -v
             Fix any failures.

          Files: tests/test_model_escalation.py

      - id: '5.2'
        name: Write unit tests for parse_escalation_config
        agent: coder
        status: pending
        depends_on:
          - '1.2'
          - '5.1'
        description: >
          Add parse_escalation_config tests to tests/test_model_escalation.py.

          Reference: docs/plans/2026-02-16-10-tiered-model-escalation-design.md

          Steps:
          1. Add parse_escalation_config to the import block:
             parse_escalation_config = mod.parse_escalation_config

          2. Add these test cases:

             a. test_parse_escalation_config_defaults: Call parse_escalation_config
                with an empty plan dict {}. Verify returns EscalationConfig with
                enabled=False and all defaults.

             b. test_parse_escalation_config_enabled: Call parse_escalation_config
                with plan = {"meta": {"model_escalation": {"enabled": True,
                "escalate_after": 3, "max_model": "sonnet"}}}.
                Verify enabled=True, escalate_after=3, max_model="sonnet".

             c. test_parse_escalation_config_partial: Call parse_escalation_config
                with plan = {"meta": {"model_escalation": {"enabled": True}}}.
                Verify enabled=True but all other fields use defaults.

             d. test_parse_escalation_config_with_validation_model: Call
                parse_escalation_config with plan that includes
                validation_model="haiku". Verify validation_model="haiku".

             e. test_parse_escalation_config_no_meta: Call parse_escalation_config
                with plan = {"sections": []}. Verify returns disabled config.

          3. Run: python3 -m pytest tests/test_model_escalation.py -v
             Fix any failures.

          Files: tests/test_model_escalation.py

  - id: phase-6
    name: Phase 6 - Verification
    status: pending
    tasks:
      - id: '6.1'
        name: Verify syntax, tests, and dry-run
        agent: code-reviewer
        status: pending
        depends_on:
          - '3.2'
          - '3.3'
          - '3.4'
          - '4.1'
          - '5.2'
        description: >
          Run verification checks to confirm the tiered model escalation feature
          works correctly.

          Steps:
          1. Check Python syntax for both scripts:
             python3 -c "import py_compile; py_compile.compile('scripts/auto-pipeline.py', doraise=True); py_compile.compile('scripts/plan-orchestrator.py', doraise=True)"

          2. Run unit tests:
             python3 -m pytest tests/ 2>/dev/null || echo 'No test suite configured'

          3. Verify the EscalationConfig class exists and works:
             python3 -c "
             import importlib.util
             spec = importlib.util.spec_from_file_location('po', 'scripts/plan-orchestrator.py')
             mod = importlib.util.module_from_spec(spec)
             spec.loader.exec_module(mod)
             assert hasattr(mod, 'EscalationConfig'), 'EscalationConfig not found'
             assert hasattr(mod, 'parse_escalation_config'), 'parse_escalation_config not found'
             assert hasattr(mod, 'MODEL_TIERS'), 'MODEL_TIERS not found'
             # Verify tier ladder
             assert mod.MODEL_TIERS == ['haiku', 'sonnet', 'opus'], f'MODEL_TIERS wrong: {mod.MODEL_TIERS}'
             # Verify escalation logic
             cfg = mod.EscalationConfig(enabled=True, escalate_after=2)
             assert cfg.get_effective_model('sonnet', 1) == 'sonnet'
             assert cfg.get_effective_model('sonnet', 3) == 'opus'
             assert cfg.get_effective_model('haiku', 3) == 'sonnet'
             # Verify disabled returns unchanged
             cfg_off = mod.EscalationConfig(enabled=False)
             assert cfg_off.get_effective_model('haiku', 10) == 'haiku'
             print('All escalation classes verified')
             "

          4. Verify run_claude_task accepts model parameter:
             python3 -c "
             import inspect, importlib.util
             spec = importlib.util.spec_from_file_location('po', 'scripts/plan-orchestrator.py')
             mod = importlib.util.module_from_spec(spec)
             spec.loader.exec_module(mod)
             sig = inspect.signature(mod.run_claude_task)
             assert 'model' in sig.parameters, f'model param missing from run_claude_task: {sig}'
             print('run_claude_task model parameter verified')
             "

          5. Verify run_validation accepts escalation_config parameter:
             python3 -c "
             import inspect, importlib.util
             spec = importlib.util.spec_from_file_location('po', 'scripts/plan-orchestrator.py')
             mod = importlib.util.module_from_spec(spec)
             spec.loader.exec_module(mod)
             sig = inspect.signature(mod.run_validation)
             assert 'escalation_config' in sig.parameters, f'escalation_config missing: {sig}'
             print('run_validation escalation_config parameter verified')
             "

          6. Run orchestrator dry-run to verify startup prints escalation info:
             python3 scripts/plan-orchestrator.py --plan .claude/plans/sample-plan.yaml --dry-run

          7. Verify PlanUsageTracker.record accepts model parameter:
             python3 -c "
             import inspect, importlib.util
             spec = importlib.util.spec_from_file_location('po', 'scripts/plan-orchestrator.py')
             mod = importlib.util.module_from_spec(spec)
             spec.loader.exec_module(mod)
             sig = inspect.signature(mod.PlanUsageTracker.record)
             assert 'model' in sig.parameters, f'model param missing from record: {sig}'
             print('PlanUsageTracker.record model parameter verified')
             "

          If any check fails, report the failure with specific details.

          Files: scripts/plan-orchestrator.py, tests/test_model_escalation.py
